{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-butterfly/source/css/index.styl","path":"css/index.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/css/var.styl","path":"css/var.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/404.jpg","path":"img/404.jpg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/favicon.png","path":"img/favicon.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":1,"renderable":1},{"_id":"source/robots.txt","path":"robots.txt","modified":1,"renderable":0},{"_id":"source/assets/404-background.png","path":"assets/404-background.png","modified":1,"renderable":0},{"_id":"source/assets/CC-BY-NC-SA-4.0.png","path":"assets/CC-BY-NC-SA-4.0.png","modified":1,"renderable":0},{"_id":"source/assets/alipay.jpg","path":"assets/alipay.jpg","modified":1,"renderable":0},{"_id":"source/assets/gongan.png","path":"assets/gongan.png","modified":1,"renderable":0},{"_id":"source/assets/wechat.jpg","path":"assets/wechat.jpg","modified":1,"renderable":0},{"_id":"source/assets/006/20220327-c69003dc.png","path":"assets/006/20220327-c69003dc.png","modified":1,"renderable":0},{"_id":"source/assets/011/20220630-4a3e172d.png","path":"assets/011/20220630-4a3e172d.png","modified":1,"renderable":0},{"_id":"source/assets/011/20220630-56618fdb.png","path":"assets/011/20220630-56618fdb.png","modified":1,"renderable":0},{"_id":"source/assets/favicon/about.txt","path":"assets/favicon/about.txt","modified":1,"renderable":0},{"_id":"source/assets/favicon/android-chrome-192x192.png","path":"assets/favicon/android-chrome-192x192.png","modified":1,"renderable":0},{"_id":"source/assets/favicon/android-chrome-512x512.png","path":"assets/favicon/android-chrome-512x512.png","modified":1,"renderable":0},{"_id":"source/assets/favicon/apple-touch-icon.png","path":"assets/favicon/apple-touch-icon.png","modified":1,"renderable":0},{"_id":"source/assets/favicon/favicon-16x16.png","path":"assets/favicon/favicon-16x16.png","modified":1,"renderable":0},{"_id":"source/assets/favicon/favicon-32x32.png","path":"assets/favicon/favicon-32x32.png","modified":1,"renderable":0},{"_id":"source/assets/favicon/favicon.ico","path":"assets/favicon/favicon.ico","modified":1,"renderable":0},{"_id":"source/assets/favicon/site.webmanifest","path":"assets/favicon/site.webmanifest","modified":1,"renderable":0},{"_id":"source/assets/wallpaper/1098350.jpg","path":"assets/wallpaper/1098350.jpg","modified":1,"renderable":0},{"_id":"source/assets/wallpaper/1238279.jpg","path":"assets/wallpaper/1238279.jpg","modified":1,"renderable":0},{"_id":"source/assets/wallpaper/209420.jpg","path":"assets/wallpaper/209420.jpg","modified":1,"renderable":0},{"_id":"source/assets/wallpaper/328761.jpg","path":"assets/wallpaper/328761.jpg","modified":1,"renderable":0},{"_id":"source/assets/wallpaper/592246.jpg","path":"assets/wallpaper/592246.jpg","modified":1,"renderable":0},{"_id":"source/assets/wallpaper/625709.png","path":"assets/wallpaper/625709.png","modified":1,"renderable":0},{"_id":"source/assets/wallpaper/935992.jpg","path":"assets/wallpaper/935992.jpg","modified":1,"renderable":0},{"_id":"source/resume/index/academy.svg","path":"resume/index/academy.svg","modified":1,"renderable":0},{"_id":"source/resume/index/activity.svg","path":"resume/index/activity.svg","modified":1,"renderable":0},{"_id":"source/resume/index/briefcase-solid.svg","path":"resume/index/briefcase-solid.svg","modified":1,"renderable":0},{"_id":"source/resume/index/envelope-solid.svg","path":"resume/index/envelope-solid.svg","modified":1,"renderable":0},{"_id":"source/resume/index/github-brands.svg","path":"resume/index/github-brands.svg","modified":1,"renderable":0},{"_id":"source/resume/index/graduation-cap-solid.svg","path":"resume/index/graduation-cap-solid.svg","modified":1,"renderable":0},{"_id":"source/resume/index/info-circle-solid.svg","path":"resume/index/info-circle-solid.svg","modified":1,"renderable":0},{"_id":"source/resume/index/info-solid.svg","path":"resume/index/info-solid.svg","modified":1,"renderable":0},{"_id":"source/resume/index/personality.svg","path":"resume/index/personality.svg","modified":1,"renderable":0},{"_id":"source/resume/index/phone-solid.svg","path":"resume/index/phone-solid.svg","modified":1,"renderable":0},{"_id":"source/resume/index/project-diagram-solid.svg","path":"resume/index/project-diagram-solid.svg","modified":1,"renderable":0},{"_id":"source/resume/index/rss-solid.svg","path":"resume/index/rss-solid.svg","modified":1,"renderable":0},{"_id":"source/resume/index/social-activity.svg","path":"resume/index/social-activity.svg","modified":1,"renderable":0},{"_id":"source/resume/index/tools-solid.svg","path":"resume/index/tools-solid.svg","modified":1,"renderable":0}],"Cache":[{"_id":"source/_drafts/Gimbal-Camera-Trachking.md","hash":"84d3c4985e1b4930b3a1efd08bb738647af030aa","modified":1661280094000},{"_id":"source/robots.txt","hash":"ce93e7c3a12473e0fb6881b32ed0fd355e66cfa5","modified":1682785294667},{"_id":"source/baidu_verify_codeva-qB4Yi0SUoj.html","hash":"1a081eec9b474f16f3de93b189aab24a2469c868","modified":1682759687620},{"_id":"source/manifest.json","hash":"614bff4a330b99a141b134807d77e329222e1604","modified":1665008410000},{"_id":"source/_data/link.yml","hash":"20696230c25f1685130a1d174ea215126c1b648d","modified":1661322618000},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1661050652000},{"_id":"source/assets/CC-BY-NC-SA-4.0.png","hash":"3af0f83fb09ddbffba0a0e476af204ef53fc3ae2","modified":1682784381455},{"_id":"source/assets/gongan.png","hash":"a99df13e8eb11db86edebf6e5ac246eb59f4b3c4","modified":1682525972344},{"_id":"source/categories/index.md","hash":"fee3f7bfbb81cba078f2c8309fc855c17b980339","modified":1661321724000},{"_id":"source/link/index.md","hash":"2e9af0aabb22c06e04b93131a6a8cd287da4fd26","modified":1661322130000},{"_id":"source/assets/404-background.png","hash":"d2519710498a871ca3e913c57e2ba20a805b6430","modified":1682526876660},{"_id":"source/tags/index.md","hash":"c6caccbf3635125851a2ff174825ba0eb91872a4","modified":1682705431901},{"_id":"source/resume/index.md","hash":"807b6094f86dbbb136a24a7c719c4a5ebc3045b1","modified":1682833107959},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne.md","hash":"390e5c1cd147e51b0e457e293d77954d490f7072","modified":1686048708817},{"_id":"source/_posts/001-MachineLearning/MachineLearning-Normalization.md","hash":"975779219f0d7f76e03aa3849daadc97bec023d1","modified":1686048679889},{"_id":"source/_posts/005-LifeMemories/人生感悟-开篇.md","hash":"f4746b34acbb6f90782e3b8c69cdadf8217a3a21","modified":1686046361749},{"_id":"source/_posts/006-WorkExperience/来上海的第三年.md","hash":"a69704513e009d48329ecc68c85ece3c5d06191a","modified":1686046273245},{"_id":"source/_posts/013-DeepLearning/Pytorch介绍.md","hash":"e776b1813307ec1dcd00e80a50a57fc56fe8e137","modified":1693536585169},{"_id":"source/assets/006/20220327-c69003dc.png","hash":"4b34f8174d3c048482800434c2b06f491c8153e5","modified":1648361360000},{"_id":"source/assets/favicon/about.txt","hash":"c345ed046ad7dbb52e12742708397868112e9bfa","modified":1664951960000},{"_id":"source/assets/favicon/android-chrome-192x192.png","hash":"858a2267c0124fbd58b04898a667745f750615b0","modified":1664951960000},{"_id":"source/assets/favicon/apple-touch-icon.png","hash":"1586abbe20d7bef7fe35e4209234052df16d891d","modified":1664951960000},{"_id":"source/assets/favicon/android-chrome-512x512.png","hash":"49d75d2fd037c84b69cfcfec976cd8f2a553cecb","modified":1664951960000},{"_id":"source/assets/favicon/favicon-16x16.png","hash":"1f8ff7963758d4adc21ab308a125bb9920d0a569","modified":1664951960000},{"_id":"source/assets/favicon/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1664951960000},{"_id":"source/assets/favicon/favicon-32x32.png","hash":"5f554b50ec50cab73eb6262e53f7194ee2cdfa3a","modified":1664951960000},{"_id":"source/assets/favicon/favicon.ico","hash":"2267b835171b4a357116e8d81b79f323534d5928","modified":1664951960000},{"_id":"source/resume/index/academy.svg","hash":"acb9c47d6f3ca92f5adfe44d733f58a2583d5ad6","modified":1592544015000},{"_id":"source/resume/index/activity.svg","hash":"a717c139be291d83db6b037014b515170109cf77","modified":1592544005000},{"_id":"source/resume/index/briefcase-solid.svg","hash":"4c5c84b170667642d3ff823c12b9cacc5ca74083","modified":1682575939100},{"_id":"source/resume/index/envelope-solid.svg","hash":"d128f35d828de147db528c964ab10c65623a7b93","modified":1682575939100},{"_id":"source/resume/index/github-brands.svg","hash":"af31efcec159e03578c986b1d87a856780aacf54","modified":1682575939100},{"_id":"source/resume/index/graduation-cap-solid.svg","hash":"6b167cbd58626d2a737d8b704af9bfc999276246","modified":1682575939100},{"_id":"source/resume/index/info-circle-solid.svg","hash":"0652d15403e6a8ffb809a589510cde3e37d36a46","modified":1682575939108},{"_id":"source/resume/index/info-solid.svg","hash":"2010b6b77211929d0af03e31cae146c8c1072d18","modified":1682575939108},{"_id":"source/resume/index/personality.svg","hash":"8d0a8a6ed983ab906252fe5c0e5fcd4e9f2a8f3d","modified":1592544018000},{"_id":"source/resume/index/phone-solid.svg","hash":"96a3656642fd47ce08725c3f413c95992702f3e7","modified":1682575939108},{"_id":"source/resume/index/project-diagram-solid.svg","hash":"7dcc3ea77d803837f491612c12d78b1e8be7086f","modified":1682575939108},{"_id":"source/resume/index/rss-solid.svg","hash":"a9205be93d44f70af9bf70047b83efcf892dc7ca","modified":1682575939108},{"_id":"source/resume/index/tools-solid.svg","hash":"3b64fedb46a48daf90b2f0208cf88e7f8efc0bf4","modified":1682575939108},{"_id":"source/resume/index/social-activity.svg","hash":"f108e9d23d1070b75c5e795008ea90e23ebfa576","modified":1592544008000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220404-c90356a3.png","hash":"3e856db120fd5e2b575612caa8fd1ab46c09d476","modified":1649057962000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-09163d0e.png","hash":"52a09e3cff20c91917555807068cfa60846ced5a","modified":1649114548000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220404-06906ed3.png","hash":"b124f3ea80dc219f11262782fcbc5dc679b4e9d0","modified":1649058718000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-040bd4c6.png","hash":"4c2533145380840fe7b3558154420f61caed319f","modified":1649132768000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-34745253.png","hash":"6ecf842abf004fb0e4da45f21dc593bc83c929da","modified":1649071704000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-410a67fb.png","hash":"71442f5708eac31a931f25cee592f2d230c6e24c","modified":1649117402000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-40ed843a.png","hash":"02fea750466031992c87f9425bfad0a891553131","modified":1649117564000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-0abae8b4.png","hash":"90bf0e2fabc19088b6b745de3a0c20952818c897","modified":1649119370000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-41245f4f.png","hash":"5a094a0e5e953c0cff1f3f75370d8245784a067e","modified":1649061836000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4225951f.png","hash":"c324b36c6a1eeca2db8c6e831b3c7a0086ba7367","modified":1649109832000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-457455e9.png","hash":"81fdd3eadb671074974cd14afdf7c12dbdfd69a9","modified":1649116702000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-48750e2d.png","hash":"acd7e58ed6f9b93717146a511a08935eecb23888","modified":1649115636000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4b653a5e.png","hash":"ca5a978c5e7baa28e10657b25e93687cebf582c0","modified":1649060262000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-7d662ac4.png","hash":"3d7c28ce4c4686a4298f52cec103f180166f4f13","modified":1649078618000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-7e9d0b45.png","hash":"41facbf9a6efe1e76c24853a1301dc838f4fa663","modified":1649067184000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-83dab511.png","hash":"f16f769adcb6e5662aee4247411c17795e179192","modified":1649116646000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-998602da.png","hash":"ed38b88ee51a9a26100886ddbaeff6542ac68e24","modified":1649059952000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-9b2895c7.png","hash":"b296681d80b95b825e73c32751c5268256583b33","modified":1649067092000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-af372d35.png","hash":"539c68786c8a7f2034d7a39ce35c29b0ad8fb904","modified":1649117228000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-c075dee1.png","hash":"0786df225015065cc9f91c07897760f434fe5814","modified":1649067010000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-cd1519fb.png","hash":"454d5b4c8bc01d7fbf2b0d0de845b10304692fc3","modified":1649114772000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-e582528a.png","hash":"eeb06af41bb05d817e28465bd8fa781d81a7d08a","modified":1649130424000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-eab99959.png","hash":"6e08d2fdcc0291f571260e40b14e644db36d7658","modified":1649060620000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-ed4f3265.png","hash":"bdedfdd8a0213a1b785f7624aff19a7c4c3683b9","modified":1649115714000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f1a06055.png","hash":"74d0e875a0028e721afdec312c24f36e8b8c2671","modified":1649078684000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f843c893.png","hash":"b2b71aad9cf68750d204132a20433856f4d01cd8","modified":1649116734000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f9890acf.png","hash":"aba66a94e3d1a4b5fa26737b79ab3b2d1211a366","modified":1649115560000},{"_id":"source/_posts/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit.md","hash":"de2ee86d39c724627eea27b3190693ef2e50520a","modified":1686047346757},{"_id":"source/_posts/002-NVIDIA/02-ONNX/ONNX概述.md","hash":"3fce9a7d25d409d9c84d8ee8118ae8add9c90e5f","modified":1686047191853},{"_id":"source/_posts/002-NVIDIA/03-TensorRT/TensorRT安装指南.md","hash":"46105779e7fedfc00279d176276733728be42e5a","modified":1686047303989},{"_id":"source/_posts/002-NVIDIA/03-TensorRT/TensorRT概述.md","hash":"3ee9fece8687ef75f7b25318fdfab4efe98681c0","modified":1686046869317},{"_id":"source/_posts/002-NVIDIA/04-cuBLAS/cuBLAS概述.md","hash":"9640bb639bcff743a5dc43b4d68788fc6fc19d75","modified":1686047181785},{"_id":"source/_posts/003-Projects/01-视觉跟踪/Gimbal-Camera-Tracking.md","hash":"32e8172aaede2b4395ba9015631edd8c0a5110dc","modified":1686046647429},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书.md","hash":"5bfbfacf4fefdb0aed8e55051944e7267e426e5b","modified":1686046548869},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu.md","hash":"2c13087a2fac1154955d10836306dff42f1a4cc9","modified":1686046435733},{"_id":"source/_posts/007-ComputerVision/01-OpenCV/【第一章】OpenCV入门.md","hash":"0dc3eb99bd7c08ad40aa9bc1fdceaacde257a128","modified":1686046153665},{"_id":"source/_posts/007-ComputerVision/01-OpenCV/【第三章】图像运算.md","hash":"62b9a082a21e05bee16036f5f00e6906cca6ced1","modified":1686046181037},{"_id":"source/_posts/007-ComputerVision/01-OpenCV/【第二章】图像处理基础.md","hash":"e7928815b71bdbd806bd968b20a2f34891b1b346","modified":1686046173157},{"_id":"source/_posts/008-AutonomousDriving/01-感知/CLOCS后融合.md","hash":"250087591c7c6b81f438a3723f138ffdccac8f0c","modified":1686046126693},{"_id":"source/_posts/008-AutonomousDriving/02-Autoware.universe/3步实现在Jetson-Orin上联调Autoware-Universe和Carla-0-9-15.md","hash":"bb83422a717e63299b39183466a7b322d4640efe","modified":1710692608866},{"_id":"source/_posts/008-AutonomousDriving/02-Autoware.universe/Autoware-Universe环境搭建.md","hash":"0833b34fc353cb07991833bfd6f7754dec869c27","modified":1686226833087},{"_id":"source/_posts/008-AutonomousDriving/02-Autoware.universe/How to Joint testing between Autoware-Universe and Carla-0-9-15 on Jetson Orin?.md","hash":"55666573b5cd7c36c60a190b9e52621081c84511","modified":1710692430474},{"_id":"source/_posts/008-AutonomousDriving/02-Autoware.universe/Ubuntu18安装AWSIM运行Autoware-Universe.md","hash":"6dcb591f27291c5edf628f9779424a0e81b5f019","modified":1686159050685},{"_id":"source/_posts/010-EdgeComputing/01-RaspberryPi/ARM平台安装Clash.md","hash":"dbabf89587ef955b51557af147b0b096c27b0dd5","modified":1686239153507},{"_id":"source/_posts/009-Devops/01-Docker/Docker网络代理配置.md","hash":"01d1ebe11427825b65919bd7005aaabe2f02ac8e","modified":1686046087109},{"_id":"source/_posts/011-ClusterSuperComputing/01-TuringPi/TuringPi2安装Jetson-Xavier-NX模组.md","hash":"4c8416833ebd7cfd74169797a326d33dd3142b5e","modified":1686045478689},{"_id":"source/_posts/012-IntelligentAIRobots/01-VectorRobots/Vector机器人运行SDK-Examples.md","hash":"e90c38e0dfa0315aabd7da2c610362d14aa3b54c","modified":1686154061145},{"_id":"source/_posts/012-IntelligentAIRobots/01-VectorRobots/如何在Ubuntu18-04系统上操作Vector机器人.md","hash":"a149366bbd38ccec009acb095404d62e37dab197","modified":1686106146606},{"_id":"source/_posts/012-IntelligentAIRobots/02-MiRoRobots/MiRo-E入门.md","hash":"42615fea572b0a5adc35220367e5dd6ca9efcdb0","modified":1697016278913},{"_id":"source/_posts/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit/5b208976-b632-11e5-8406-38d379ec46aa.png","hash":"f19837fa9dfe106206ca60c0d00b4af3e178f46b","modified":1661383904000},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu/Cmake.png","hash":"0888a1401029f285f67030adce1111cd72b851a1","modified":1664950642000},{"_id":"node_modules/hexo-theme-butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":1682770079942},{"_id":"node_modules/hexo-theme-butterfly/README.md","hash":"52967a864c244af4db8c63902586cb617ee5b8aa","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/README_CN.md","hash":"e19021371184361261ddef1d98eb308d78922714","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/_config.yml","hash":"adc8f25ce9d499c18731b55735e5bc37262383ca","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/package.json","hash":"12931142de278c77d4f3af08568036782efd4c15","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/plugins.yml","hash":"c7a060713f72ec8b4a45244b3aa8d51d772f5ce8","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/languages/default.yml","hash":"4025c0ba440eb24705dd0293ca9ca84efb3105cc","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/languages/en.yml","hash":"4e9cdb7a3570929bcf082de7a4eac49140dddc73","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/languages/zh-TW.yml","hash":"ee01e068f12dc33adfae5733824ea1255deb5ca6","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/languages/zh-CN.yml","hash":"7dd849c3ba34986c57c764d9e36150b4bfffd2e9","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/layout/archive.pug","hash":"a0c034c2d319320a54046805e80b58dc48b7e233","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/category.pug","hash":"710708cfdb436bc875602abf096c919ccdf544db","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/index.pug","hash":"e1c3146834c16e6077406180858add0a8183875a","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/page.pug","hash":"baf469784aef227e4cc840550888554588e87a13","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/post.pug","hash":"fc9f45252d78fcd15e4a82bfd144401cba5b169a","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/tag.pug","hash":"0440f42569df2676273c026a92384fa7729bc4e9","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/bug_report.yml","hash":"fc468a93a64c7a3e408fbac921c9f5a5a8f32027","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/config.yml","hash":"7dfe7189ffeaebb6db13842237f8e124649bea3d","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/feature_request.yml","hash":"996640605ed1e8e35182f0fd9a60a88783b24b03","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/.github/workflows/publish.yml","hash":"05857c2f265246d8de00e31037f2720709540c09","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/.github/workflows/stale.yml","hash":"ac62b989b5550c756e1986fcc68f243170705383","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/404.pug","hash":"cb49f737aca272ccfeb62880bd651eccee72a129","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/additional-js.pug","hash":"aca0ec7ef69b21d1f242c62fed389468a0f0e1a2","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/footer.pug","hash":"02390a5b6ae1f57497b22ba2e6be9f13cfb7acac","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head.pug","hash":"dd9fde431add984330e3178e06a8d74705e7340e","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/layout.pug","hash":"7fa9ae4b70b87fc97e992dde5944681f92b59bea","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/pagination.pug","hash":"4c85de4dea4dca4e5088097a79bd6d7009cbf8ef","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/rightside.pug","hash":"83a1f2d31792206d432e8e2041e284d88327c02e","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/sidebar.pug","hash":"8d39473ed112d113674a0f689f63fae06c72abd2","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/404.js","hash":"83cd7f73225ccad123afbd526ce1834eb1eb6a6d","modified":1682770079950},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/cdn.js","hash":"21fb5aabe043486d095c4c8cce361ed85ba88a26","modified":1682770079950},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/comment.js","hash":"5351e0bc09e6b5b3f6d30f333a2520626a28ca3a","modified":1682770079950},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/init.js","hash":"3ace1139182d3d367149db138990891427f3356e","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/stylus.js","hash":"9819f0996234fbd80d6c50a9e526c56ebf22588d","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/welcome.js","hash":"8ad9911b755cba13dde2cc055c3f857a6b0dd20e","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/scripts/filters/post_lazyload.js","hash":"932df912976261929f809b7dbd4eb473e7787345","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/filters/random_cover.js","hash":"8d25f47434deae870bbffd07efe528a40363ab4d","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/aside_archives.js","hash":"2ec66513d5322f185d2071acc052978ba9415a8e","modified":1682770079950},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/aside_categories.js","hash":"e00efdb5d02bc5c6eb4159e498af69fa61a7dbb9","modified":1682770079950},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/findArchiveLength.js","hash":"9ea86bd7a3c3fca3324f70b1cd4d9e42f9efb08d","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/inject_head_js.js","hash":"4238e06ff448ff2ee717cd4c874f37f04d35da06","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/page.js","hash":"ce5d5a3d07b0d76ac5e96e5f9e5783f4b601b6be","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/related_post.js","hash":"4677be4175da6800c0b3b8c1614e593f73df8831","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/button.js","hash":"91d954f6e9fe6e571eb8ec9f8996294b2dc3688e","modified":1682770079950},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/flink.js","hash":"ab62919fa567b95fbe14889517abda649991b1ee","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/gallery.js","hash":"950b3dbac0b21717458a8d1769cbfc454d0eff54","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/hide.js","hash":"396c3ab1bcf1c7693ad7e506eadd13016c6769b6","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/inlineImg.js","hash":"a43ee2c7871bdd93cb6beb804429e404570f7929","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/label.js","hash":"03b2afef41d02bd1045c89578a02402c28356006","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/mermaid.js","hash":"531808a290b8bdd66bac2faab211ada8e9646a37","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/note.js","hash":"d51812b43924f1bbf413c67499510dd125022005","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/score.js","hash":"ea679dfe12d0e2290113b4a9d00663ce7a5ee5ad","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/tabs.js","hash":"6c6e415623d0fd39da016d9e353bb4f5cca444f5","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/timeline.js","hash":"300eb779588bf35a1b687d9f829d866074b707e3","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/source/css/index.styl","hash":"861998e4ac67a59529a8245a9130d68f826c9c12","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/var.styl","hash":"30abbb8eed880d51f61f336064d93abd709e0115","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1682770079946},{"_id":"node_modules/hexo-theme-butterfly/source/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1682770079946},{"_id":"node_modules/hexo-theme-butterfly/source/js/main.js","hash":"05c825962e365af62096d3f1b4d7c9ee1b5fc2f5","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/source/js/tw_cn.js","hash":"76d0c5c172cae44b34b0bd3125fd068b2c3cbd4a","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/Open_Graph.pug","hash":"8aa8d799aedbfd811195b84a451bc4b6e2647c12","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/source/js/utils.js","hash":"2e74fe8ae5ac20067668a18df5985459faf419f9","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/analytics.pug","hash":"67e1c3b48e4ca7ee0b2c76d3ca7476b9883cf105","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/config.pug","hash":"021ccdca211cce0438a378ada37897a8ffcb9574","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/config_site.pug","hash":"7df90c8e432e33716517ab918b0a125bc284041b","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/noscript.pug","hash":"d16ad2ee0ff5751fd7f8a5ce1b83935518674977","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/preconnect.pug","hash":"a03b3ddc06e7aa9fd07eea0d5f97c8d5addd2315","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/pwa.pug","hash":"3d492cfe645d37c94d30512e0b230b0a09913148","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/index.pug","hash":"0c1551ef80bbece550fe520d91e21f083cbc14fe","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/menu_item.pug","hash":"31346a210f4f9912c5b29f51d8f659913492f388","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/nav.pug","hash":"f61659aa457d1a2d1baa3a13157996cfac4d6609","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/post-info.pug","hash":"9698f22751778dde063cbfbd01c59ca4462ccd85","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/social.pug","hash":"5de9a82032cdad1db3b868b797460921cd775fc2","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/loading/fullpage-loading.pug","hash":"68cda524337dfe2e1467318a4a6c124b4c3845a7","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/loading/index.pug","hash":"131f344d68b4c241d6e03849b243ee792fcd3cea","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/loading/pace.pug","hash":"6ab4e301c92586505d6cddce1b3ad23b7c79010d","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/mixins/article-sort.pug","hash":"90554c2ca5ba946f4c02e1bc5fe2859cef1b1594","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/mixins/post-ui.pug","hash":"294df7a74cf36af3a7030274d8b745979c1c8c70","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/categories.pug","hash":"5276a8d2835e05bd535fedc9f593a0ce8c3e8437","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/default-page.pug","hash":"12c65c174d26a41821df9bad26cdf1087ec5b0ca","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/flink.pug","hash":"a59bcfbb609a099c1bf5be40b7a94e7e2b06fc4a","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/tags.pug","hash":"9621991359e22b14049346f1cf87bdedc94edf5a","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/post/post-copyright.pug","hash":"ebecba46a5f4efe1c98a386df06c56e26fbd07b9","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/post/reward.pug","hash":"864869c43fe5b5bb6f4ac6b13dd4bfb16ea47550","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/aplayer.pug","hash":"c7cfade2b160380432c47eef4cd62273b6508c58","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/effect.pug","hash":"6528e86656906117a1af6b90e0349c2c4651d5e1","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/pangu.pug","hash":"0f024e36b8116118233e10118714bde304e01e12","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/pjax.pug","hash":"fc0b09068009edd4026d90a669608cbe211aeecf","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/prismjs.pug","hash":"ffb9ea15a2b54423cd4cd441e2d061b8233e9b58","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/subtitle.pug","hash":"35223531f8e086d57caec2d17d45ddbcb39deb74","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_announcement.pug","hash":"ae392459ad401a083ca51ee0b27526b3c1e1faed","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_author.pug","hash":"e37468e63db2a0ac09b65d21b7de3e62425bb455","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_newest_comment.pug","hash":"6d93564a8bd13cb9b52ee5e178db3bcbf18b1bc6","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_post_toc.pug","hash":"3057a2f6f051355e35d3b205121af8735100eacf","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_recent_post.pug","hash":"e5aac7b28ed4123d75797263c64e74ac547945bc","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_tags.pug","hash":"eceb4420a64c720f0d2741e89d6229bbb3d87353","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_webinfo.pug","hash":"35ce167c5a275211bfc1fa3d49adfde5b404d98f","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/index.pug","hash":"010e3d548ababca2280c4fc4168d9a4a1ee4f536","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/source/css/_global/function.styl","hash":"7fdfbe8f97b41588bbd5c6f27e7e85a881b28954","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_global/index.styl","hash":"4f5636c326f794417296bdb6bcfd6a8b207d69d8","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/chat.styl","hash":"f9a5d3f1fc5ed0ed2ee4c1eaa58ed650d11ddebd","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/aside.styl","hash":"57adf29a3e36e4ea84384e36c034eb294dffb208","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/comments.styl","hash":"c61dccca690d486c3d9c29cf028d87b777385141","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/footer.styl","hash":"83553445fbc92cad4ad220fbd87b4c3db958c32a","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/theme.styl","hash":"bcd384c8b2aa0390c9eb69ac1abbfd1240ce1da4","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight.styl","hash":"2f95e99b8351fbecd9037a1bbdc3fee9d6ea8a77","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/loading.styl","hash":"ac2aeee9926f75b2a0098efe1c114126987430f2","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/head.styl","hash":"45d71dbb2a61e30989851ba29bb8be7094574d14","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/pagination.styl","hash":"fb9f78bfbb79579f1d752cb73fb6d25c8418e0fd","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/post.styl","hash":"e24046fad288a13897195038cb7a63d1014cd7b8","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/relatedposts.styl","hash":"d53de408cb27a2e704aba7f7402b7caebe0410d8","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/reward.styl","hash":"c5cfed620708807a48076b5ee59b0ba84e29aa80","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/rightside.styl","hash":"bbc884d6b2158a833b77a1bbc07248e17874b22e","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/sidebar.styl","hash":"631ca35a38bc4ac052e9caf47508ff1f99842fc7","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/third-party.styl","hash":"ca39e634668ed4fbb43267ec4782c2b55c44e698","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_mode/darkmode.styl","hash":"7ff0c456fae2717ddbbb9f8fae2734d449a5448b","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_mode/readmode.styl","hash":"ffea9e7c1543edcf080381e7b99828954c2f2cef","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/404.styl","hash":"50dbb9e6d98c71ffe16741b8c1b0c1b9771efd2b","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/archives.styl","hash":"c9e98027f2dd730ce389c2047f62ebb748955fcf","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/categories.styl","hash":"f01ee74948cedb44e53cd3bb1ef36b7d2778ede7","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/flink.styl","hash":"98d755b686ee833e9da10afaa40c4ec2bd66c19a","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/homepage.styl","hash":"bb470da1d2ba292cae0a30a252f82f37c4130d2d","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/common.styl","hash":"e4b9d6164e97b30c84e1218c7543c60f6b29edcc","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/tags.styl","hash":"580feb7e8b0822a1be48ac380f8c5c53b1523321","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/algolia.styl","hash":"649a054e73278b6724bd4dd9b94724791ec5c928","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/local-search.styl","hash":"a86e4e9198b225b4b73a7a45f04b86cbbed0d231","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/button.styl","hash":"45f0c32bdea117540f6b14ebac6450d7142bd710","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/index.styl","hash":"678e56ad2e46b630364540fc6a881d6801192dcd","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/gallery.styl","hash":"81ad85acf0e0fe7f9ee23c16a700e7154574d5dd","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/hexo.styl","hash":"d76c38adf1d9c1279ef4241835667789f5b736e0","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/hide.styl","hash":"ce489ca2e249e2a3cf71584e20d84bdb022e3475","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/inlineImg.styl","hash":"df9d405c33a9a68946b530410f64096bcb72560c","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/label.styl","hash":"66c59e193d794cdb02cca7bd1dc4aea5a19d7e84","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/note.styl","hash":"85ae91c83691ea4511f4277da1194a185251cc78","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/tabs.styl","hash":"bf9568444dd54e39dc59b461323dcd38942f27d9","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/timeline.styl","hash":"f071156d439556e7463ed4bc61ceee87170d5d08","modified":1682770079982},{"_id":"node_modules/hexo-theme-butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":1682770079942},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/local-search.js","hash":"8a0547ecb33ad2939450152adf54fca58e22a424","modified":1682770079954},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/algolia.js","hash":"fd86281d4f0f99ce173e49c1a0df3507fe268d37","modified":1682770079950},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/abcjs/index.pug","hash":"cabb3a06f8ef297a1ea3d91ced8abeaa0831aa14","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/abcjs/abcjs.pug","hash":"ed6906b7c6aa7046bbad95dfdda9211997be7099","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/artalk.pug","hash":"99d9b17668260b242749c16851d9ec1024d31899","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"0344477a2cf38698318ead2681c63ac12f01586e","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"b2d274db84ef22fbd6d5ea8f4404821898934209","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/remark42.pug","hash":"001e8be47854b891efe04013c240c38fed4185eb","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"d85c3737b5c9548553a78b757a7698df126a52cf","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"58406a7a3bf45815769f652bf3ef81e57dcd07eb","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"39427e107230a10790972349c9dd4c4f31d55eb7","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"55acc455ca8e13211e3906cf78e487cc92accee5","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/chatra.pug","hash":"481cd5053bafb1a19f623554a27d3aa077ea59c3","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/crisp.pug","hash":"76634112c64023177260d1317ae39cef2a68e35f","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"cfe63e7d26a6665df6aa32ca90868ad48e05ec04","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/index.pug","hash":"618e1b7f9204049b07beb9e1363c844a78a9ace3","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/messenger.pug","hash":"3ce0461534b786cb71d9141dff35fa5cb70e22b9","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/tidio.pug","hash":"24a926756c2300b9c561aaab6bd3a71fdd16e16d","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/artalk.pug","hash":"95738f110598f999d627234e78ff9e9decac1b9b","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/disqus.pug","hash":"8ec24c1939895ac0db2b2e8700bc9307b4ceb53c","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"3ba842bf4801b2f115c2cfe5657f35a732ce589f","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"a833715eec2171ff05664d6d5752a57c954192ce","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"1c3a87393cb49915940c2dd206356c2a16549767","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/giscus.pug","hash":"9c16ff9cdc444ebf47eed33ea35b8366459a18f0","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/index.pug","hash":"351fe25fbf02635b1f9e86e5e244c7d61f69baa7","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/js.pug","hash":"00ed91c52939b9675b316137f854d13684c895a6","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/livere.pug","hash":"52ea8aa26b84d3ad38ae28cdf0f163e9ca8dced7","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/remark42.pug","hash":"e9bdf80d6796afc04eb809dbbe780d97f22c7fcd","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/utterances.pug","hash":"a737046e730eb7264606ba0536218964044492f9","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"e18fbd88d8942e53e771f29b26209ab735c5c567","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/valine.pug","hash":"e55b9c0f8ced231f47eb88bd7f4ec99f29c5c29d","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/waline.pug","hash":"26ba1fc99117993087b1c6e02daa2626627d8eb1","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/katex.pug","hash":"dfcbd9881be569ea420eff1a6b00e4f4dbe2138e","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/index.pug","hash":"b8ae5fd7d74e1edcef21f5004fc96147e064d219","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/mermaid.pug","hash":"8e33aca36a4d3ae9e041ba05ced8eff56ae38f77","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/mathjax.pug","hash":"0ea633b11b357afa50c200290d19c32467d58a1d","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/artalk.pug","hash":"e6ebbe137dd86b6d8750a6843e350fcd16030981","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"04b2a5882e789a988e41d45abe606f0617b08e38","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"4ec0642f2d5444acfab570a6f8c7868e7ff43fde","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"e846ddfe4a63b15d1416f6055f5756af5e3da7c6","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"233907dd7f5b5f33412701d2ccffbc0bbae8707b","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/remark42.pug","hash":"ab167c00da4506f591b96f0591bf5bd214a26d4b","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"d19e1c2c0a50f0e4547d71a17b9be88e8152f17c","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"441d87067d87d9996b53b25c05b8e620bd94b027","modified":1682770079974},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/algolia.pug","hash":"9c3c109a12d2b6916e8b4965cca12f521510ead9","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/docsearch.pug","hash":"1c3e101445c5571ba998ce293d3984319df1b3b0","modified":1682770079962},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/local-search.pug","hash":"5ebd5e8d39c9f77f5b2d983f6cd6802ccaf98746","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/index.pug","hash":"a99a41334387ee9a46c6f8e8212331a29a10d159","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/add-this.pug","hash":"2980f1889226ca981aa23b8eb1853fde26dcf89a","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/addtoany.pug","hash":"85c92f8a7e44d7cd1c86f089a05be438535e5362","modified":1682770079958},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/index.pug","hash":"4c4a9c15215ae8ac5eadb0e086b278f76db9ee92","modified":1682770079966},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/share-js.pug","hash":"c7dd2b2ae9b23aa0a60fffd7df9e9f76ef52033e","modified":1682770079970},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight/diff.styl","hash":"cf1fae641c927621a4df1be5ca4a853b9b526e23","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight/index.styl","hash":"18804c58239d95798fa86d0597f32d7f7dd30051","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/diff.styl","hash":"5972c61f5125068cbe0af279a0c93a54847fdc3b","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/index.styl","hash":"5dc2e0bcae9a54bfb9bdcc82d02ae5a3cf1ca97d","modified":1682770079978},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"8970cc1916c982b64a1478792b2822d1d31e276d","modified":1682770079982},{"_id":"source/assets/alipay.jpg","hash":"07a8db5c160cfcc3c4d9bead2205bdd04baa5934","modified":1682526504208},{"_id":"source/assets/wechat.jpg","hash":"4f09131874e89aa287b7071201ae79889aa5a981","modified":1682526511080},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4a0db26b.png","hash":"5ef0708a274bc6a9f418dfef814af40aaf5bf448","modified":1649132304000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a067c6cf.png","hash":"0a7e350cda9603a43cbb926e0cc07cdad0de1af3","modified":1649116910000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a5b9d88a.png","hash":"53a43f6d5a2aa070d3b3d9d18b138411301650d5","modified":1649114524000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a6a42af7.png","hash":"5cf3b43af661b03a1fc1d5193f69b8a87c76a1b9","modified":1649063776000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-dfeec3f1.png","hash":"548c951654ce0adaf423f6ae5e75ac974667ba8b","modified":1649119336000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-e2ff73cf.png","hash":"8fe6eea8cf60917927d3a3b2c213950216531675","modified":1649067314000},{"_id":"source/_posts/002-NVIDIA/03-TensorRT/TensorRT安装指南/02-TensorRT安装指南-6ff62ded.png","hash":"a322d575aacd206eee19753ec7acfb297f63681c","modified":1661321008000},{"_id":"source/_posts/002-NVIDIA/03-TensorRT/TensorRT安装指南/02-TensorRT安装指南-a2f8136f.png","hash":"19388b55040e768c890b36481afee00b46c531e5","modified":1661321162000},{"_id":"source/_posts/002-NVIDIA/03-TensorRT/TensorRT概述/01-TensorRT概述-b0cc304a.png","hash":"10b06bc525cbe4be40495e505adb8393930e3c06","modified":1661327484000},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220112-4b9a80b5.png","hash":"8ade61b9240c9a1462f86520707294f267ad6de3","modified":1641949832000},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830004817114.png","hash":"c5f56bff481d955cc423d3fecc4646ee104b71c7","modified":1661762896000},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830010646685.png","hash":"ae159d59c6d5e2da4da9606c1d1bf2229226acd5","modified":1661764006000},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830010853110.png","hash":"75ba1379237820f7841a60db9ebac4a64423c9e5","modified":1661764132000},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-dff73fa3.png","hash":"43dfa7327079d924ccb61f52744a4658bed8d29e","modified":1649062652000},{"_id":"source/_posts/005-LifeMemories/人生感悟-开篇/677558eb37581273-166153976382210.jpg","hash":"a35bda288d7b01b6b6f8282004f04b757ccf0eb2","modified":1661510962000},{"_id":"source/_posts/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit/nvidia-docker-arch-new.png","hash":"9db84cfe3411ea5751c3aa5dace336049b31f464","modified":1661390240000},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220112-76ab38e2.png","hash":"37eb792f7520c7ae5269a4f35d52bf6d84f060d4","modified":1641960288000},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-3b5f0f20.png","hash":"dcf267ec6cce9940876af7ea9d73018e89d3fa48","modified":1642291592000},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-67901f3c.png","hash":"7b96ee0610f97a09fb233f8d6f37655ee2c86c91","modified":1642292170000},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-76d60e91.png","hash":"b3c90d7dac0f8983a8f9163505eb519e1506b187","modified":1642291930000},{"_id":"source/assets/011/20220630-4a3e172d.png","hash":"c2f3292978a9472c17e432d9dd1dd094c1246676","modified":1656546534000},{"_id":"source/assets/wallpaper/209420.jpg","hash":"23bb40b10a336e7be8cab2b0bded3f35fe15321f","modified":1682526542040},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-923e402c.png","hash":"ced2987c0b8bd109d00eaed925fd074c59d8b9ce","modified":1642119860000},{"_id":"source/assets/011/20220630-56618fdb.png","hash":"4087fcc621b4484fbb9f6446addcfa603ddf1cc1","modified":1656548134000},{"_id":"source/assets/wallpaper/592246.jpg","hash":"203d2d92a4d04820b5dd8a3373df5b5091f51ba4","modified":1682526547480},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-cddd0cdf.png","hash":"bb20549c1436913cbe7dcf0f41717ce13fee66f9","modified":1642116322000},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-159d3456.png","hash":"1e09fa855f09dd939e678e86872236d205757285","modified":1642120706000},{"_id":"source/assets/wallpaper/935992.jpg","hash":"d13e0cd4c91b485e9ee38e93ce2465d7c390b17a","modified":1682526548508},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-e0917cd4.png","hash":"8c3a698e582242dbc6fea5191544a3d6f2eaf9b3","modified":1642121296000},{"_id":"source/assets/wallpaper/1098350.jpg","hash":"499a5693a65f4b39e82d30f08d2deed666b0e14b","modified":1682526537756},{"_id":"source/assets/wallpaper/328761.jpg","hash":"22a46fae62e739a04e59239e82f7de59d8b866fc","modified":1682526548292},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-8bd94d2e.png","hash":"1a26bafa7d817d2116bbeeac2bfa3db2ade4df00","modified":1642116150000},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-0da8fa36.png","hash":"175ce9e0fe439c86c5dca7b2d3e5a9651ffe1150","modified":1642115720000},{"_id":"source/assets/wallpaper/625709.png","hash":"8949902aefb4ac32a1d8da40e079345c7bbac38d","modified":1682526547128},{"_id":"source/assets/wallpaper/1238279.jpg","hash":"21a3b599f029b5a1491e0ff4038ad4488e7fd2a4","modified":1682526543252},{"_id":"public/manifest.json","hash":"e5653018c633c03092c691caf2b17921c60fe633","modified":1711441737070},{"_id":"public/baidusitemap.xml","hash":"d3a88534ba076bc2edad198b553e97e3d4618c09","modified":1711441737070},{"_id":"public/sitemap.xml","hash":"0cc2c2f986f58520da61c53646b2bdcf52050d4f","modified":1711441737070},{"_id":"public/sitemap.txt","hash":"72ce5882e144f5ea39724132d1b96c8e43f41e6a","modified":1711441737070},{"_id":"public/404.html","hash":"294a6590bfbe866b7f9c251babaec31ce847e774","modified":1711441737070},{"_id":"public/baidu_verify_codeva-qB4Yi0SUoj.html","hash":"f0361805b3e2ef71ae07b77487d724f48a3de656","modified":1711441737070},{"_id":"public/categories/index.html","hash":"6fe12703a09482c29224250f60584ccc2d205ea7","modified":1711441737070},{"_id":"public/link/index.html","hash":"f2296e79676c8484ee6a4e8cf46df79a3e5c598d","modified":1711441737070},{"_id":"public/tags/index.html","hash":"1e5138d9d39609def305e4e734d28c88293362aa","modified":1711441737070},{"_id":"public/resume/index.html","hash":"9a4958ba5f2e85643d7278bda1e1a65b44434789","modified":1711441737070},{"_id":"public/2024/03/15/008-AutonomousDriving/02-Autoware.universe/3步实现在Jetson-Orin上联调Autoware-Universe和Carla-0-9-15/index.html","hash":"d1e7c3c11fa6f6ed38a3a0a3bc16bd173297ad03","modified":1711441737070},{"_id":"public/2024/03/12/008-AutonomousDriving/02-Autoware.universe/How to Joint testing between Autoware-Universe and Carla-0-9-15 on Jetson Orin","hash":"db4d198c962ed565d0c7c49f6beeee8e13dac1e8","modified":1711441737070},{"_id":"public/2023/10/11/012-IntelligentAIRobots/02-MiRoRobots/MiRo-E入门/index.html","hash":"654c632f8838602e4398546635cdebfc1aea99e6","modified":1711441737070},{"_id":"public/2023/09/01/013-DeepLearning/Pytorch介绍/index.html","hash":"a9f99a422d48e25fae90f53c75a1ae2fb4b392e1","modified":1711441737070},{"_id":"public/2023/06/08/010-EdgeComputing/01-RaspberryPi/ARM平台安装Clash/index.html","hash":"04a5af9b3cf0896e5e0928010b8329fccce14b88","modified":1711441737070},{"_id":"public/2023/06/08/008-AutonomousDriving/02-Autoware.universe/Autoware-Universe环境搭建/index.html","hash":"dcd09ac0bf9c927bb2b5ac57735e510f8f0593d1","modified":1711441737070},{"_id":"public/2023/06/08/008-AutonomousDriving/02-Autoware.universe/Ubuntu18安装AWSIM运行Autoware-Universe/index.html","hash":"436f8e1bf56f88d3f5502b9391b1abd48071d38f","modified":1711441737070},{"_id":"public/2023/06/07/012-IntelligentAIRobots/01-VectorRobots/Vector机器人运行SDK-Examples/index.html","hash":"14ba1e99dcfb6d04bd8d7dcb38035f10a1f8f13e","modified":1711441737070},{"_id":"public/2023/06/06/012-IntelligentAIRobots/01-VectorRobots/如何在Ubuntu18-04系统上操作Vector机器人/index.html","hash":"0fa39eb8a4d527cb6e3f278551a1fc51711f992a","modified":1711441737070},{"_id":"public/2023/06/06/011-ClusterSuperComputing/01-TuringPi/TuringPi2安装Jetson-Xavier-NX模组/index.html","hash":"adeebbbbb481641b17bf41536b60d8ff433f25ec","modified":1711441737070},{"_id":"public/2023/06/04/009-Devops/01-Docker/Docker网络代理配置/index.html","hash":"308c12a380fa24a079bcfd541c87db51b40267f8","modified":1711441737070},{"_id":"public/2023/04/11/008-AutonomousDriving/01-感知/CLOCS后融合/index.html","hash":"39e17a65675ecbb6b22cfb9325f5bde165d0faef","modified":1711441737070},{"_id":"public/2023/03/27/007-ComputerVision/01-OpenCV/【第三章】图像运算/index.html","hash":"22eedc22534c8568e439407d766c3114f40be32d","modified":1711441737070},{"_id":"public/2023/03/25/007-ComputerVision/01-OpenCV/【第二章】图像处理基础/index.html","hash":"6b1bbd4afea0e74c76ba9950199a419088e236b1","modified":1711441737070},{"_id":"public/2023/03/23/007-ComputerVision/01-OpenCV/【第一章】OpenCV入门/index.html","hash":"3e8b8995dfe2c344107fc2e3d71f349030b2ec90","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/index.html","hash":"98f0584943e4d12262fb00ae199048a058e0b434","modified":1711441737070},{"_id":"public/2023/03/20/006-WorkExperience/来上海的第三年/index.html","hash":"8fac6cf81f7d6d5b35903591acd2eb3c0a24f787","modified":1711441737070},{"_id":"public/2023/03/20/hello-world/index.html","hash":"ba0649d04c9ca9512ff8cc4bff9684262613a90c","modified":1711441737070},{"_id":"public/2022/08/27/004-Linux/01-Cmake/Cmake升级-Ubuntu/index.html","hash":"32cb795a89d96208eede752be8aed1e43f861901","modified":1711441737070},{"_id":"public/2022/08/27/005-LifeMemories/人生感悟-开篇/index.html","hash":"fb041ce51fa35d8a1fe3ba762f180437eb9cd936","modified":1711441737070},{"_id":"public/2022/08/25/001-MachineLearning/MachineLearning-Normalization/index.html","hash":"e803c8ad6949597e2a0d6417c5a8bdaba0f04eb1","modified":1711441737070},{"_id":"public/2022/08/25/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit/index.html","hash":"f8e3b406bb9b1201b030944835b0409cdef330bd","modified":1711441737070},{"_id":"public/2022/08/25/002-NVIDIA/04-cuBLAS/cuBLAS概述/index.html","hash":"60e0eed262a80ddb3247049a6a2aadd78e299b00","modified":1711441737070},{"_id":"public/2022/08/25/002-NVIDIA/02-ONNX/ONNX概述/index.html","hash":"0c5bf86b0bf111412d98db78043fb19962015063","modified":1711441737070},{"_id":"public/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT概述/index.html","hash":"322693b76ed6eb8957d2b0fe058ba393a274aecd","modified":1711441737070},{"_id":"public/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT安装指南/index.html","hash":"792204ad05f0d2cbbc6311dcee0807194d597bc0","modified":1711441737070},{"_id":"public/2022/08/24/003-Projects/01-视觉跟踪/Gimbal-Camera-Tracking/index.html","hash":"74999cf90b088ce7eee7ff9ec9146f58cde85247","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/index.html","hash":"6e47977a2023f1399bf0f7cb6304cbaee517e866","modified":1711441737070},{"_id":"public/archives/index.html","hash":"b82a837c3e40df680c8ac5a30ac1002fad28c734","modified":1711441737070},{"_id":"public/archives/page/2/index.html","hash":"1c3c2262b0aaff90978362a7ed85a3f333665771","modified":1711441737070},{"_id":"public/archives/page/3/index.html","hash":"7845affa4f74f313b8484934411aea1087638170","modified":1711441737070},{"_id":"public/archives/2022/index.html","hash":"864d31bfcc9dc2f369a8ae8f7eec5d7f74d89922","modified":1711441737070},{"_id":"public/archives/2022/08/index.html","hash":"d0ce84983862e0eb407519b9284a00834ee6ef7b","modified":1711441737070},{"_id":"public/archives/2023/index.html","hash":"0f8c10617ecea4074dd1c93dabd16d20eb8cfb5e","modified":1711441737070},{"_id":"public/archives/2023/page/2/index.html","hash":"9039775fe54badec31b5124b35e29a7dee7640a0","modified":1711441737070},{"_id":"public/archives/2023/03/index.html","hash":"9c87e4afe987c98b950e24fb71e4b323a896e75e","modified":1711441737070},{"_id":"public/archives/2023/04/index.html","hash":"c5b7264681f686af3637443634a925a221b4b2c9","modified":1711441737070},{"_id":"public/archives/2023/06/index.html","hash":"f1c7abeba863c45930fbac8b2f7d3dd3caaa8293","modified":1711441737070},{"_id":"public/archives/2023/09/index.html","hash":"25762ed518fa783583fa921b8049cb9aeea41649","modified":1711441737070},{"_id":"public/archives/2023/10/index.html","hash":"5001ec6ee180f9439a3267844ec5e957334f8b6d","modified":1711441737070},{"_id":"public/archives/2024/index.html","hash":"2a679272a205824dafbb274779e21f05dd83d576","modified":1711441737070},{"_id":"public/archives/2024/03/index.html","hash":"1759d62fd53e6254d2d4ee9adb6b2d0f2305c2f1","modified":1711441737070},{"_id":"public/categories/MachineLearning/index.html","hash":"bfea028ebc70588ce32b90a639141f74a73612d1","modified":1711441737070},{"_id":"public/categories/人生回忆/index.html","hash":"c813acd5297d226645d9c4fc06cc2b1f24ca219e","modified":1711441737070},{"_id":"public/categories/工作经验/index.html","hash":"7abc1a58a59c601f4b7ac4dad19ae79f0b8a7144","modified":1711441737070},{"_id":"public/categories/DeepLearning/index.html","hash":"d697cafcb3f7833c2eb928fa5c66ec175ee19b82","modified":1711441737070},{"_id":"public/categories/NVIDIA/index.html","hash":"bcfbbc16d3bf35b1f902b610c1cc5ad46ec7645d","modified":1711441737070},{"_id":"public/categories/人生回忆/人生感悟/index.html","hash":"2869ad9de88bac9e0a03d61d02dd224776f4db50","modified":1711441737070},{"_id":"public/categories/工作经验/上海工作/index.html","hash":"5febd5b1c3270d7410af7bf64f8bceab0a2202b0","modified":1711441737070},{"_id":"public/categories/Projects/index.html","hash":"bca4821bd879e72cc248c266d598d9b7220f875a","modified":1711441737070},{"_id":"public/categories/Linux/index.html","hash":"8baba0cf78b997f67e7522867d74c98f6d2705a7","modified":1711441737070},{"_id":"public/categories/DeepLearning/Pytorch/index.html","hash":"16f6fa74d47fd238378409eba98137c04d75038f","modified":1711441737070},{"_id":"public/categories/ComputerVision/index.html","hash":"8ff11d4e3749867f3a37c504050f3e84a0382d59","modified":1711441737070},{"_id":"public/categories/NVIDIA/Container-Toolkit/index.html","hash":"3040f1995f2598e9100a189780843ae2f55cba76","modified":1711441737070},{"_id":"public/categories/自动驾驶/index.html","hash":"ea1817134ea3bfeb5528bcc53ab8997d99652b97","modified":1711441737070},{"_id":"public/categories/NVIDIA/ONNX/index.html","hash":"136f20e342312d8254da8a137a426dac8b341ecc","modified":1711441737070},{"_id":"public/categories/NVIDIA/TensorRT/index.html","hash":"755eb83cf0e8f28905f090a971c0b9220fba193d","modified":1711441737070},{"_id":"public/categories/Devops/index.html","hash":"e53e316425cf17d66bb99cc3bc5fe4031a3f4d29","modified":1711441737070},{"_id":"public/categories/EdgeComupting/index.html","hash":"04b5b5bbe0c24559b460922488f3fa9109d6b3ff","modified":1711441737070},{"_id":"public/categories/集群超算/index.html","hash":"1ccead8690eddbeb5349532e4d555b277d1b15f5","modified":1711441737070},{"_id":"public/categories/智能AI机器人/index.html","hash":"f102b2232abf98f6d7715818920f04767ecdf72a","modified":1711441737070},{"_id":"public/categories/NVIDIA/cuBLAS/index.html","hash":"e328cbd0d0ae8226a38c9e2a313abe5897f898af","modified":1711441737070},{"_id":"public/categories/Projects/视觉跟踪/index.html","hash":"97df117de1ca5f5047beabb030c55af7bf0e8841","modified":1711441737070},{"_id":"public/categories/Linux/Cmake/index.html","hash":"aee124cccb2259b065807f410dfe2fa525d0bbdd","modified":1711441737070},{"_id":"public/categories/ComputerVision/OpenCV/index.html","hash":"f56fdb73801a1d5bffbc2d2def3f3d67a5dbd1d5","modified":1711441737070},{"_id":"public/categories/自动驾驶/后融合/index.html","hash":"28958c56c3ae60bca49cf0c5ab2ace4397889144","modified":1711441737070},{"_id":"public/categories/自动驾驶/Autoware-Universe/index.html","hash":"c2f0ca37643b2b6908cd1b4b17d4dff1b6f84805","modified":1711441737070},{"_id":"public/categories/Devops/Docker/index.html","hash":"b5a0a64826b0b7f2952158043afcc67f1931046f","modified":1711441737070},{"_id":"public/categories/EdgeComupting/RaspberryPi/index.html","hash":"53c54f4169e719432b661c582b1bfebf74f89e3f","modified":1711441737070},{"_id":"public/categories/集群超算/TuringPi/index.html","hash":"f3ff96fc84a3eacbbc9262e4adf743d00e75e1f8","modified":1711441737070},{"_id":"public/categories/智能AI机器人/VectorRobots/index.html","hash":"3083a183d78b9bc59a0e2d8f787003114d6f3b8a","modified":1711441737070},{"_id":"public/categories/智能AI机器人/MiRoRobots/index.html","hash":"50ce0b4c83c0ce5ccb47548ffe2661f1c1312b81","modified":1711441737070},{"_id":"public/index.html","hash":"34dd10c96b691c91870a89af64b2ded0245db46c","modified":1711441737070},{"_id":"public/page/2/index.html","hash":"75ae6531c131218d192587f672c1b9fcf9fc85da","modified":1711441737070},{"_id":"public/page/3/index.html","hash":"6e113586e6ccb60c3744bbe522ee1219ec5ada73","modified":1711441737070},{"_id":"public/tags/A-First-Course-in-ML/index.html","hash":"95dd6b049fca9584b3bd57e850c1d276ae931fc2","modified":1711441737070},{"_id":"public/tags/感悟/index.html","hash":"323edc2696d08fe8cc788488b6d039c4ca1bbd68","modified":1711441737070},{"_id":"public/tags/上海工作/index.html","hash":"1c3d7609e23162a85e6e6f81926a72d1b3f7b979","modified":1711441737070},{"_id":"public/tags/Pytorch/index.html","hash":"3ed4d4446729fecfc9e683fe2f609506205cb794","modified":1711441737070},{"_id":"public/tags/Container-Toolkit/index.html","hash":"f2779400a315d3a67d3d43a5c38bf1610d3f3d95","modified":1711441737070},{"_id":"public/tags/Docker/index.html","hash":"db487939006d36e4fb454b2eea25009b78541aaf","modified":1711441737070},{"_id":"public/tags/ONNX/index.html","hash":"b44605192537f041feed05fba05c0f2888d998a8","modified":1711441737070},{"_id":"public/tags/TensorRT/index.html","hash":"c8e34e6cb91347974da2a59ddc60539d335bad08","modified":1711441737070},{"_id":"public/tags/cuBLAS/index.html","hash":"78ffd6d868114a561da03d3a2820ed81aea1fa48","modified":1711441737070},{"_id":"public/tags/Gimbal-Camera-Tracking/index.html","hash":"ac8263bdc8ca3c0bc57c9756d19e84725beb8d8f","modified":1711441737070},{"_id":"public/tags/CMake/index.html","hash":"6ea4604d95679ba189dc94690fd009d6958b6c51","modified":1711441737070},{"_id":"public/tags/OpenCV/index.html","hash":"99196c43b5f38c5872a11987841ecc3ec40a9f1a","modified":1711441737070},{"_id":"public/tags/后融合/index.html","hash":"83f7a27cff98a487e5da7a67dbcba5c2d744aa85","modified":1711441737070},{"_id":"public/tags/Autoware-Universe/index.html","hash":"74984d8bd50d1528435d1fb514eea7138b8a8531","modified":1711441737070},{"_id":"public/tags/Ubuntu22/index.html","hash":"657e3a7c7e4dcdabdcfc8648252b75c0db971618","modified":1711441737070},{"_id":"public/tags/Carla/index.html","hash":"38281d40029aecbbeaff4b1e894508360aba5a91","modified":1711441737070},{"_id":"public/tags/Ubuntu18/index.html","hash":"28c5a0614c4a1f2e639e1cc839aaac1a40830eb4","modified":1711441737070},{"_id":"public/tags/AWSIM/index.html","hash":"1b18c5ee45b2053f9280738bf603c971b7bef85a","modified":1711441737070},{"_id":"public/tags/ARM/index.html","hash":"babe97c25715114fe7b5341365a4611539cacabc","modified":1711441737070},{"_id":"public/tags/RaspberryPi/index.html","hash":"3f6c5a6ae84603ae77f8bceff7611a89f923366a","modified":1711441737070},{"_id":"public/tags/Jetson/index.html","hash":"6b05dac45396f8c7ff5f44eb36732302f4a0c099","modified":1711441737070},{"_id":"public/tags/Clash/index.html","hash":"8bb5ac14337d423ee423d3d6be2a70c7379fc238","modified":1711441737070},{"_id":"public/tags/TuringPi2/index.html","hash":"c700af258416d02e7d55a683dc6553904be4ced3","modified":1711441737070},{"_id":"public/tags/Vector/index.html","hash":"87d17613e7303b5744e9ff95f4b4198964172c69","modified":1711441737070},{"_id":"public/tags/MiRo/index.html","hash":"d926042393ec03ae6039bcc081e0ad85276f298f","modified":1711441737070},{"_id":"public/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1711441737070},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1711441737070},{"_id":"public/robots.txt","hash":"ce93e7c3a12473e0fb6881b32ed0fd355e66cfa5","modified":1711441737070},{"_id":"public/assets/404-background.png","hash":"d2519710498a871ca3e913c57e2ba20a805b6430","modified":1711441737070},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1711441737070},{"_id":"public/assets/CC-BY-NC-SA-4.0.png","hash":"3af0f83fb09ddbffba0a0e476af204ef53fc3ae2","modified":1711441737070},{"_id":"public/assets/gongan.png","hash":"a99df13e8eb11db86edebf6e5ac246eb59f4b3c4","modified":1711441737070},{"_id":"public/assets/006/20220327-c69003dc.png","hash":"4b34f8174d3c048482800434c2b06f491c8153e5","modified":1711441737070},{"_id":"public/assets/favicon/about.txt","hash":"c345ed046ad7dbb52e12742708397868112e9bfa","modified":1711441737070},{"_id":"public/assets/favicon/android-chrome-192x192.png","hash":"858a2267c0124fbd58b04898a667745f750615b0","modified":1711441737070},{"_id":"public/assets/favicon/android-chrome-512x512.png","hash":"49d75d2fd037c84b69cfcfec976cd8f2a553cecb","modified":1711441737070},{"_id":"public/assets/favicon/apple-touch-icon.png","hash":"1586abbe20d7bef7fe35e4209234052df16d891d","modified":1711441737070},{"_id":"public/assets/favicon/favicon-16x16.png","hash":"1f8ff7963758d4adc21ab308a125bb9920d0a569","modified":1711441737070},{"_id":"public/assets/favicon/favicon-32x32.png","hash":"5f554b50ec50cab73eb6262e53f7194ee2cdfa3a","modified":1711441737070},{"_id":"public/assets/favicon/favicon.ico","hash":"2267b835171b4a357116e8d81b79f323534d5928","modified":1711441737070},{"_id":"public/assets/favicon/site.webmanifest","hash":"bf31baf91bdd2fcde24a45e3f2a1be33733c6f69","modified":1711441737070},{"_id":"public/resume/index/academy.svg","hash":"acb9c47d6f3ca92f5adfe44d733f58a2583d5ad6","modified":1711441737070},{"_id":"public/resume/index/activity.svg","hash":"a717c139be291d83db6b037014b515170109cf77","modified":1711441737070},{"_id":"public/resume/index/briefcase-solid.svg","hash":"4c5c84b170667642d3ff823c12b9cacc5ca74083","modified":1711441737070},{"_id":"public/resume/index/envelope-solid.svg","hash":"d128f35d828de147db528c964ab10c65623a7b93","modified":1711441737070},{"_id":"public/resume/index/github-brands.svg","hash":"af31efcec159e03578c986b1d87a856780aacf54","modified":1711441737070},{"_id":"public/resume/index/graduation-cap-solid.svg","hash":"6b167cbd58626d2a737d8b704af9bfc999276246","modified":1711441737070},{"_id":"public/resume/index/info-circle-solid.svg","hash":"0652d15403e6a8ffb809a589510cde3e37d36a46","modified":1711441737070},{"_id":"public/resume/index/info-solid.svg","hash":"2010b6b77211929d0af03e31cae146c8c1072d18","modified":1711441737070},{"_id":"public/resume/index/personality.svg","hash":"8d0a8a6ed983ab906252fe5c0e5fcd4e9f2a8f3d","modified":1711441737070},{"_id":"public/resume/index/phone-solid.svg","hash":"96a3656642fd47ce08725c3f413c95992702f3e7","modified":1711441737070},{"_id":"public/resume/index/project-diagram-solid.svg","hash":"7dcc3ea77d803837f491612c12d78b1e8be7086f","modified":1711441737070},{"_id":"public/resume/index/rss-solid.svg","hash":"a9205be93d44f70af9bf70047b83efcf892dc7ca","modified":1711441737070},{"_id":"public/resume/index/social-activity.svg","hash":"f108e9d23d1070b75c5e795008ea90e23ebfa576","modified":1711441737070},{"_id":"public/resume/index/tools-solid.svg","hash":"3b64fedb46a48daf90b2f0208cf88e7f8efc0bf4","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220404-06906ed3.png","hash":"b124f3ea80dc219f11262782fcbc5dc679b4e9d0","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220404-c90356a3.png","hash":"3e856db120fd5e2b575612caa8fd1ab46c09d476","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-040bd4c6.png","hash":"4c2533145380840fe7b3558154420f61caed319f","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-09163d0e.png","hash":"52a09e3cff20c91917555807068cfa60846ced5a","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-0abae8b4.png","hash":"90bf0e2fabc19088b6b745de3a0c20952818c897","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-34745253.png","hash":"6ecf842abf004fb0e4da45f21dc593bc83c929da","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-40ed843a.png","hash":"02fea750466031992c87f9425bfad0a891553131","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-410a67fb.png","hash":"71442f5708eac31a931f25cee592f2d230c6e24c","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-41245f4f.png","hash":"5a094a0e5e953c0cff1f3f75370d8245784a067e","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-457455e9.png","hash":"81fdd3eadb671074974cd14afdf7c12dbdfd69a9","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4225951f.png","hash":"c324b36c6a1eeca2db8c6e831b3c7a0086ba7367","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-48750e2d.png","hash":"acd7e58ed6f9b93717146a511a08935eecb23888","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4b653a5e.png","hash":"ca5a978c5e7baa28e10657b25e93687cebf582c0","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-7d662ac4.png","hash":"3d7c28ce4c4686a4298f52cec103f180166f4f13","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-7e9d0b45.png","hash":"41facbf9a6efe1e76c24853a1301dc838f4fa663","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-83dab511.png","hash":"f16f769adcb6e5662aee4247411c17795e179192","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-998602da.png","hash":"ed38b88ee51a9a26100886ddbaeff6542ac68e24","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-9b2895c7.png","hash":"b296681d80b95b825e73c32751c5268256583b33","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-af372d35.png","hash":"539c68786c8a7f2034d7a39ce35c29b0ad8fb904","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-c075dee1.png","hash":"0786df225015065cc9f91c07897760f434fe5814","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-cd1519fb.png","hash":"454d5b4c8bc01d7fbf2b0d0de845b10304692fc3","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-e582528a.png","hash":"eeb06af41bb05d817e28465bd8fa781d81a7d08a","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-eab99959.png","hash":"6e08d2fdcc0291f571260e40b14e644db36d7658","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-ed4f3265.png","hash":"bdedfdd8a0213a1b785f7624aff19a7c4c3683b9","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f1a06055.png","hash":"74d0e875a0028e721afdec312c24f36e8b8c2671","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f843c893.png","hash":"b2b71aad9cf68750d204132a20433856f4d01cd8","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f9890acf.png","hash":"aba66a94e3d1a4b5fa26737b79ab3b2d1211a366","modified":1711441737070},{"_id":"public/2022/08/25/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit/5b208976-b632-11e5-8406-38d379ec46aa.png","hash":"f19837fa9dfe106206ca60c0d00b4af3e178f46b","modified":1711441737070},{"_id":"public/2022/08/27/004-Linux/01-Cmake/Cmake升级-Ubuntu/Cmake.png","hash":"0888a1401029f285f67030adce1111cd72b851a1","modified":1711441737070},{"_id":"public/css/hbe.style.css","hash":"b0a0077cb588c0941823905fcc383aa7509ade73","modified":1711441737070},{"_id":"public/lib/hbe.js","hash":"136dba00826bdd086153bf0acb5473aea7183ad1","modified":1711441737070},{"_id":"public/assets/alipay.jpg","hash":"07a8db5c160cfcc3c4d9bead2205bdd04baa5934","modified":1711441737070},{"_id":"public/assets/wechat.jpg","hash":"4f09131874e89aa287b7071201ae79889aa5a981","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4a0db26b.png","hash":"5ef0708a274bc6a9f418dfef814af40aaf5bf448","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a067c6cf.png","hash":"0a7e350cda9603a43cbb926e0cc07cdad0de1af3","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a6a42af7.png","hash":"5cf3b43af661b03a1fc1d5193f69b8a87c76a1b9","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a5b9d88a.png","hash":"53a43f6d5a2aa070d3b3d9d18b138411301650d5","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-dfeec3f1.png","hash":"548c951654ce0adaf423f6ae5e75ac974667ba8b","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-e2ff73cf.png","hash":"8fe6eea8cf60917927d3a3b2c213950216531675","modified":1711441737070},{"_id":"public/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT安装指南/02-TensorRT安装指南-6ff62ded.png","hash":"a322d575aacd206eee19753ec7acfb297f63681c","modified":1711441737070},{"_id":"public/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT安装指南/02-TensorRT安装指南-a2f8136f.png","hash":"19388b55040e768c890b36481afee00b46c531e5","modified":1711441737070},{"_id":"public/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT概述/01-TensorRT概述-b0cc304a.png","hash":"10b06bc525cbe4be40495e505adb8393930e3c06","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220112-4b9a80b5.png","hash":"8ade61b9240c9a1462f86520707294f267ad6de3","modified":1711441737070},{"_id":"public/2022/08/27/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830004817114.png","hash":"c5f56bff481d955cc423d3fecc4646ee104b71c7","modified":1711441737070},{"_id":"public/2022/08/27/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830010646685.png","hash":"ae159d59c6d5e2da4da9606c1d1bf2229226acd5","modified":1711441737070},{"_id":"public/2022/08/27/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830010853110.png","hash":"75ba1379237820f7841a60db9ebac4a64423c9e5","modified":1711441737070},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1711441737070},{"_id":"public/js/utils.js","hash":"2e74fe8ae5ac20067668a18df5985459faf419f9","modified":1711441737070},{"_id":"public/js/search/algolia.js","hash":"fd86281d4f0f99ce173e49c1a0df3507fe268d37","modified":1711441737070},{"_id":"public/js/search/local-search.js","hash":"8a0547ecb33ad2939450152adf54fca58e22a424","modified":1711441737070},{"_id":"public/css/index.css","hash":"a58340af8394d992d772d1157e074f4ea6238946","modified":1711441737070},{"_id":"public/js/main.js","hash":"05c825962e365af62096d3f1b4d7c9ee1b5fc2f5","modified":1711441737070},{"_id":"public/js/tw_cn.js","hash":"76d0c5c172cae44b34b0bd3125fd068b2c3cbd4a","modified":1711441737070},{"_id":"public/2022/08/22/001-MachineLearning/MachineLeaning-ChapterOne/20220405-dff73fa3.png","hash":"43dfa7327079d924ccb61f52744a4658bed8d29e","modified":1711441737070},{"_id":"public/2022/08/27/005-LifeMemories/人生感悟-开篇/677558eb37581273-166153976382210.jpg","hash":"a35bda288d7b01b6b6f8282004f04b757ccf0eb2","modified":1711441737070},{"_id":"public/2022/08/25/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit/nvidia-docker-arch-new.png","hash":"9db84cfe3411ea5751c3aa5dace336049b31f464","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220112-76ab38e2.png","hash":"37eb792f7520c7ae5269a4f35d52bf6d84f060d4","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-3b5f0f20.png","hash":"dcf267ec6cce9940876af7ea9d73018e89d3fa48","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-67901f3c.png","hash":"7b96ee0610f97a09fb233f8d6f37655ee2c86c91","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-76d60e91.png","hash":"b3c90d7dac0f8983a8f9163505eb519e1506b187","modified":1711441737070},{"_id":"public/assets/011/20220630-4a3e172d.png","hash":"c2f3292978a9472c17e432d9dd1dd094c1246676","modified":1711441737070},{"_id":"public/assets/wallpaper/209420.jpg","hash":"23bb40b10a336e7be8cab2b0bded3f35fe15321f","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-923e402c.png","hash":"ced2987c0b8bd109d00eaed925fd074c59d8b9ce","modified":1711441737070},{"_id":"public/assets/011/20220630-56618fdb.png","hash":"4087fcc621b4484fbb9f6446addcfa603ddf1cc1","modified":1711441737070},{"_id":"public/assets/wallpaper/592246.jpg","hash":"203d2d92a4d04820b5dd8a3373df5b5091f51ba4","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-cddd0cdf.png","hash":"bb20549c1436913cbe7dcf0f41717ce13fee66f9","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-159d3456.png","hash":"1e09fa855f09dd939e678e86872236d205757285","modified":1711441737070},{"_id":"public/assets/wallpaper/935992.jpg","hash":"d13e0cd4c91b485e9ee38e93ce2465d7c390b17a","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-e0917cd4.png","hash":"8c3a698e582242dbc6fea5191544a3d6f2eaf9b3","modified":1711441737070},{"_id":"public/assets/wallpaper/1098350.jpg","hash":"499a5693a65f4b39e82d30f08d2deed666b0e14b","modified":1711441737070},{"_id":"public/assets/wallpaper/328761.jpg","hash":"22a46fae62e739a04e59239e82f7de59d8b866fc","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-8bd94d2e.png","hash":"1a26bafa7d817d2116bbeeac2bfa3db2ade4df00","modified":1711441737070},{"_id":"public/2023/03/22/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-0da8fa36.png","hash":"175ce9e0fe439c86c5dca7b2d3e5a9651ffe1150","modified":1711441737070},{"_id":"public/assets/wallpaper/625709.png","hash":"8949902aefb4ac32a1d8da40e079345c7bbac38d","modified":1711441737070},{"_id":"public/assets/wallpaper/1238279.jpg","hash":"21a3b599f029b5a1491e0ff4038ad4488e7fd2a4","modified":1711441737070}],"Category":[{"name":"MachineLearning","_id":"clu8483sk0009u9rl3h1ca1sq"},{"name":"人生回忆","_id":"clu8483st000mu9rlbkrxgxip"},{"name":"工作经验","_id":"clu8483sw000tu9rlfxh8evey"},{"name":"DeepLearning","_id":"clu8483sy000yu9rlfkpbgb6o"},{"name":"NVIDIA","_id":"clu8483t00013u9rlftc2abaw"},{"name":"人生感悟","parent":"clu8483st000mu9rlbkrxgxip","_id":"clu8483t7001iu9rlgjwcbl5p"},{"name":"上海工作","parent":"clu8483sw000tu9rlfxh8evey","_id":"clu8483te0021u9rlhu60gh45"},{"name":"Projects","_id":"clu8483tf0025u9rl6kp152da"},{"name":"Linux","_id":"clu8483tg002du9rlbhzd0mts"},{"name":"Pytorch","parent":"clu8483sy000yu9rlfkpbgb6o","_id":"clu8483th002gu9rler7n35t9"},{"name":"ComputerVision","_id":"clu8483ti002ju9rl4avc6jlk"},{"name":"Container-Toolkit","parent":"clu8483t00013u9rlftc2abaw","_id":"clu8483tj002ru9rlejin9f94"},{"name":"自动驾驶","_id":"clu8483tl002wu9rlf7g78ko9"},{"name":"ONNX","parent":"clu8483t00013u9rlftc2abaw","_id":"clu8483tl0030u9rla3i55x46"},{"name":"TensorRT","parent":"clu8483t00013u9rlftc2abaw","_id":"clu8483tn003du9rla7oxgsgb"},{"name":"Devops","_id":"clu8483tp003ou9rl5yf13yh3"},{"name":"EdgeComupting","_id":"clu8483tq003qu9rl27pg43xk"},{"name":"集群超算","_id":"clu8483tr003xu9rl32ard4bv"},{"name":"智能AI机器人","_id":"clu8483ts0040u9rl28xf9rhi"},{"name":"cuBLAS","parent":"clu8483t00013u9rlftc2abaw","_id":"clu8483ts0043u9rlg95xbc6u"},{"name":"视觉跟踪","parent":"clu8483tf0025u9rl6kp152da","_id":"clu8483tu004gu9rlh6hsfz69"},{"name":"Cmake","parent":"clu8483tg002du9rlbhzd0mts","_id":"clu8483tw004nu9rlhfd2bwxy"},{"name":"OpenCV","parent":"clu8483ti002ju9rl4avc6jlk","_id":"clu8483tw004su9rl42vkeyl0"},{"name":"后融合","parent":"clu8483tl002wu9rlf7g78ko9","_id":"clu8483ty0050u9rl3vh357ue"},{"name":"Autoware.Universe","parent":"clu8483tl002wu9rlf7g78ko9","_id":"clu8483ty0054u9rl70qj0yl8"},{"name":"Docker","parent":"clu8483tp003ou9rl5yf13yh3","_id":"clu8483u0005fu9rl1w1tfctq"},{"name":"RaspberryPi","parent":"clu8483tq003qu9rl27pg43xk","_id":"clu8483u0005ju9rl9wiq47ii"},{"name":"TuringPi","parent":"clu8483tr003xu9rl32ard4bv","_id":"clu8483u1005mu9rl4sbr8syl"},{"name":"VectorRobots","parent":"clu8483ts0040u9rl28xf9rhi","_id":"clu8483u1005pu9rl892u6zny"},{"name":"MiRoRobots","parent":"clu8483ts0040u9rl28xf9rhi","_id":"clu8483u2005uu9rl8rtkdzlv"}],"Data":[{"_id":"link","data":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}],"Page":[{"_content":"a67ac8ae2343209fc880c053b2e7f988","source":"baidu_verify_codeva-qB4Yi0SUoj.html","raw":"a67ac8ae2343209fc880c053b2e7f988","date":"2023-04-29T09:15:02.584Z","updated":"2023-04-29T09:14:47.620Z","path":"baidu_verify_codeva-qB4Yi0SUoj.html","title":"","comments":1,"layout":"page","_id":"clu8483s70000u9rlbe5hgn0h","content":"a67ac8ae2343209fc880c053b2e7f988","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover":"https://www.synotech.top:5523/wallpaper/5f17b43d50c03_270_185.png","cover_type":"img","excerpt":"","more":"a67ac8ae2343209fc880c053b2e7f988"},{"_content":"{\n    \"name\": \"string\",\n    \"short_name\": \"Junzhou\",\n    \"theme_color\": \"#49b1f5\",\n    \"background_color\": \"#49b1f5\",\n    \"display\": \"standalone\",\n    \"scope\": \"/\",\n    \"start_url\": \"/\",\n    \"icons\": [\n        {\n          \"src\": \"images/pwaicons/36.png\",\n          \"sizes\": \"36x36\",\n          \"type\": \"image/png\"\n        },\n        {\n            \"src\": \"images/pwaicons/48.png\",\n          \"sizes\": \"48x48\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/72.png\",\n          \"sizes\": \"72x72\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/96.png\",\n          \"sizes\": \"96x96\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/144.png\",\n          \"sizes\": \"144x144\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/192.png\",\n          \"sizes\": \"192x192\",\n          \"type\": \"image/png\"\n        },\n        {\n            \"src\": \"images/pwaicons/512.png\",\n            \"sizes\": \"512x512\",\n            \"type\": \"image/png\"\n          }\n      ],\n      \"splash_pages\": null\n  }\n","source":"manifest.json","raw":"{\n    \"name\": \"string\",\n    \"short_name\": \"Junzhou\",\n    \"theme_color\": \"#49b1f5\",\n    \"background_color\": \"#49b1f5\",\n    \"display\": \"standalone\",\n    \"scope\": \"/\",\n    \"start_url\": \"/\",\n    \"icons\": [\n        {\n          \"src\": \"images/pwaicons/36.png\",\n          \"sizes\": \"36x36\",\n          \"type\": \"image/png\"\n        },\n        {\n            \"src\": \"images/pwaicons/48.png\",\n          \"sizes\": \"48x48\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/72.png\",\n          \"sizes\": \"72x72\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/96.png\",\n          \"sizes\": \"96x96\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/144.png\",\n          \"sizes\": \"144x144\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/192.png\",\n          \"sizes\": \"192x192\",\n          \"type\": \"image/png\"\n        },\n        {\n            \"src\": \"images/pwaicons/512.png\",\n            \"sizes\": \"512x512\",\n            \"type\": \"image/png\"\n          }\n      ],\n      \"splash_pages\": null\n  }\n","date":"2023-03-20T11:34:03.966Z","updated":"2022-10-05T22:20:10.000Z","path":"manifest.json","layout":"false","title":"","comments":1,"_id":"clu8483sd0002u9rlhkbo4vyq","content":"{\"name\":\"string\",\"short_name\":\"Junzhou\",\"theme_color\":\"#49b1f5\",\"background_color\":\"#49b1f5\",\"display\":\"standalone\",\"scope\":\"/\",\"start_url\":\"/\",\"icons\":[{\"src\":\"images/pwaicons/36.png\",\"sizes\":\"36x36\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/48.png\",\"sizes\":\"48x48\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/72.png\",\"sizes\":\"72x72\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/96.png\",\"sizes\":\"96x96\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/144.png\",\"sizes\":\"144x144\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/192.png\",\"sizes\":\"192x192\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/512.png\",\"sizes\":\"512x512\",\"type\":\"image/png\"}],\"splash_pages\":null}","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover":"https://www.synotech.top:5523/wallpaper/5f19587122afa_270_185.jpg","cover_type":"img","excerpt":"","more":"{\"name\":\"string\",\"short_name\":\"Junzhou\",\"theme_color\":\"#49b1f5\",\"background_color\":\"#49b1f5\",\"display\":\"standalone\",\"scope\":\"/\",\"start_url\":\"/\",\"icons\":[{\"src\":\"images/pwaicons/36.png\",\"sizes\":\"36x36\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/48.png\",\"sizes\":\"48x48\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/72.png\",\"sizes\":\"72x72\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/96.png\",\"sizes\":\"96x96\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/144.png\",\"sizes\":\"144x144\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/192.png\",\"sizes\":\"192x192\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/512.png\",\"sizes\":\"512x512\",\"type\":\"image/png\"}],\"splash_pages\":null}"},{"title":"categories","date":"2022-08-24T14:14:30.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2022-08-24 22:14:30\ntype: \"categories\"\n---\n","updated":"2022-08-24T06:15:24.000Z","path":"categories/index.html","comments":1,"layout":"page","_id":"clu8483sh0005u9rl366w2t5l","content":"","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover":"https://www.synotech.top:5523/wallpaper/gkihqEjXxJ5UZ1C.jpg","cover_type":"img","excerpt":"","more":""},{"title":"link","date":"2022-08-24T14:21:50.000Z","type":"link","_content":"","source":"link/index.md","raw":"---\ntitle: link\ndate: 2022-08-24 22:21:50\ntype: \"link\"\n---\n","updated":"2022-08-24T06:22:10.000Z","path":"link/index.html","comments":1,"layout":"page","_id":"clu8483sj0007u9rlaajl8gqu","content":"","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover":"https://www.synotech.top:5523/wallpaper/5f19587122afa_270_185.jpg","cover_type":"img","excerpt":"","more":""},{"title":"tags","date":"2022-08-24T14:13:49.000Z","type":"tags","orderby":"random","order":1,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2022-08-24 22:13:49\ntype: \"tags\"\norderby: random\norder: 1\n\n---\n","updated":"2023-04-28T18:10:31.901Z","path":"tags/index.html","comments":1,"layout":"page","_id":"clu8483sl000au9rlcwbba6ev","content":"","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover":"https://www.synotech.top:5523/wallpaper/5f17b43d50c03_270_185.png","cover_type":"img","excerpt":"","more":""},{"title":"resume","toc":true,"toc_number":true,"comments":0,"date":"2023-04-26T17:08:05.000Z","password":"Lz-Bluet","_content":"\n <center>\n     <h1>Bluet</h1>\n     <div>\n         <span>\n             {% inlineImg src=\"index/phone-solid.svg\" width=\"18px\" %}\n             17269651686\n         </span>\n         ·\n         <span>\n             {% inlineImg src=\"index/envelope-solid.svg\" width=\"18px\" %}\n             1429053840@qq.com\n         </span>\n         ·\n         <span>\n             {% inlineImg src=\"index/github-brands.svg\" width=\"18px\" %}\n             <a href=\"https://github.com/Bluet1997\">Bluet</a>\n         </span>\n         ·\n         <span>\n             {% inlineImg src=\"index/rss-solid.svg\" width=\"18px\" %}\n             <a href=\"https://bluenote.top\">My Blog</a>\n         </span>\n     </div>\n </center>\n\n\n\n ## {% inlineImg index/info-circle-solid.svg 30px %} 个人信息 \n\n - 男，1996/12，汉族\n - 工作经验：3 年\n\n## {% inlineImg src=\"index/graduation-cap-solid.svg\" width=\"30px\" %}教育经历\n\n- 硕士，谢菲尔德大学，高级计算机科学(一等一学位 73/100)，                            2018.9 ~ 2020.3\n- 学士，江西农业大学，软件工程+英语专业(双学位 83/100)，                              2014.9~2018.7\n- 专业英语四级，四六级，雅思，GRE\n\n## {% inlineImg  src=\"index/briefcase-solid.svg\" width=\"30px\"%} 工作经历\n\n- **韵达东普科技公司，无人车/智能仓研究所，算法工程师，2020.05 - 2021.04**\n\n  负责智能仓机械臂视觉算法，快递分拣及单件分离项目，快递无人车视觉算法。\n\n- **贵州翰凯斯科技(Pixmoving)，自动驾驶RD部门，自动驾驶算法工程师，2021.04 - 2023.02**\n\n  负责半封闭园区L4无人车自动驾驶视觉算法，Robobus/清扫车/定制化项目研发。\n\n## {% inlineImg src=\"index/project-diagram-solid.svg\" width=\"30px\"%} 项目经历\n\n- **PIXMoving-清扫车 垃圾检测/实例分割/路沿检测/多传感器融合**   \t\t\t\t                          2022.03– 2022.09\n\n  **平台**: 自研清扫车 **技术点**: 基于ZED2i相机，使用YOLO-V5和Sparseinst算法，从数据收集/清洗/标注/训练到完成ROS/TensorRT部署，整个项目的链路搭建到部署都是由我个人完成，路沿检测研发基于激光雷达和ZED2i相机落地部署。\n\n- **PIXkit纯视觉跟踪**                                                                                                                   2021.12 – 2022.01\n\n  **平台:** 自研自动驾驶套件+云台相机 **技术点**: 使用YOLOX和SiamFC++算法，完成对行人、车辆、无人机目标的自动跟踪系统的落地。\n\n- **Robobus研发** \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                       2021.07 – 2023.02\n\n  **平台**: 自研Robobus **技术点**: **相机选型**及**布局**设计，**外参/内参**标定，**时间同步**，基于**多相机BEV**感知算法(Transformer)完成3D感知任务的研发，多传感器融合方案的**功能设计**，基础自动驾驶框架基于**Autoware**优化部署的。\n\n- **PIXMoving-KUKA机械臂视觉** \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                           2021.07 - 2023.02\n\n  **平台**: KUKA-KR210 **技术点**: 机械臂**Gazebo**模型构建，**3D结构光**相机选型，实时**RTM-3D**成形件**CAD误差检测**自动化流程设计。\n\n- **PIXkit-Autoware感知板块**                                                                                                      2021.04– 2021.07\n\n  **平台**:自研自动驾驶平台 **技术点**: 完整Autoware技术栈部署，2D/3D感知算法部署优化，所有交付的Pixkit项目均基于此发货。\n\n- **快递无人车视觉研发(韵达研究所-无人车)**                                                                              2020.09 – 2021.03\n\n  **平台**: 自研快递无人车 **技术点**：**联合标定**，**障碍物 2D 检测**，**点云 3D 检测(应用)**，**相机和激光雷达融合**(障碍物检测)。**最终效果**：在 Jetson 开发板上实现了 Mask RCNN/YOLO 实时目标检测，借助 TensorRT 加速/DeepStream，使用docker完成部署。\n\n- **机械臂 3D 视觉应用(韵达研究所-智能仓)**                                                                               2020.07 – 2020.09\n\n  **平台**: 自研 6 轴机械臂 **技术点**：研究前沿的 **6D 姿态估计算法**，借助英伟达 **Isaac 平台**复现算法并实现场景的应用落地。**最终效果**：对比测试了 **PVNet/PoseCNN** 模型，在 **Jetosn Xavier NX** 上实现了 **PoseCNN** 模型的部署，完成码垛任务。[Demo链接](https://v.youku.com/v_show/id_XNDg1NjYzNTA4MA==.html?spm=a2hcb.profile.app.5~5!2~5~5!3~5!2~5~5~A)\n\n- **分拣单件分离 2D 机器视觉研发(韵达研究所-智能仓)**                                                             2020.06 – 2020.07\n\n  **应用场景**: 物流分拨中心包裹分拣 **技术点**：应用**OpenCV**库进行传送带上**单件多件包裹**的识别。**最终效果**：配合六面扫，完成摆臂的单件分离的功能实现，传送带速度 **1.5m/s** 左右。\n\n- **仿生机器人视觉导航控制系统**                                                                                                2019.06 – 2019.09\n\n  **机器人平台**：谢菲尔德机器人实验室，仿生机器人**MiRo**（社交陪伴型机器人）。**技术点**：使用级联分类器（**LBP和HOG**）配合**SVM**支持向量机和神经网络模型（**MobileNet-SSD**等深度学习架构）实现目标检测和识别。使用**OpenCV**库函数，选择**TLD tracker**进行目标跟踪。使用**Braitenberg vehicle**仿生理论进行移动控制。**最终效果**：机器人能够自动的识别对应的分类目标（设计了三种类型目标），进行自主的**拦截行为**，目标是视野内**随机的移动目标**。同时，简化版本是进行**目标的跟随**。[Demo1链接](https://v.youku.com/v_show/id_XNDUyNDI3MTcwNA==.html?spm=a2hzp.8253869.0.0) [Demo2链接](https://v.youku.com/v_show/id_XNDUyNDI3MDc3Mg==.html?spm=a2hzp.8253869.0.0)\n\n## {% inlineImg src=\"index/activity.svg\" width=\"30px\"%} 实习经历\n\n- 谢菲尔德Jaguar Robot实习                                                                                                      2019.12-2020.03\n\n  **内容**：部署一个视觉导航项目在Jaguar robot上，在**Dr Li Sun**的带领下完成了一个**自动化导航**的\t系统部署。[Demo链接](https://v.youku.com/v_show/id_XNDUyMDIyMzU5Ng==.html?spm=a2h3j.8428770.3416059.1)\n\n## {% inlineImg src=\"index/academy.svg\" width=\"30px\"%} 学术经历\n\n- *Application of Computer Vision for A Biomimetic Robot* （**Dissertation Project**)\n- *A Self-supervised, Self-adaptive Model for the Next Generation Vision-based Robot Navigation* (**Research Proposal**)\n\n## {% inlineImg src=\"index/social-activity.svg\" width=\"30px\"%} 社会活动\n\n- **谢菲尔德AMRC工厂“UK-RAS Manufacturing Robotics Challenge”挑战赛**\n\n  负责完成控制**LBR KUKA iiwa**机器臂，进行拾取和放置的任务。\n\n## {% inlineImg src=\"index/personality.svg\" width=\"30px\"%} 兴趣爱好\n\n​\t业余健体爱好者，爱好电子科技产品、撸各类机器人，动手实践能力强。\n","source":"resume/index.md","raw":"---\ntitle: resume\ntoc: true\ntoc_number: true\ncomments: false\ndate: 2023-04-27 01:08:05\npassword: Lz-Bluet\n---\n\n <center>\n     <h1>Bluet</h1>\n     <div>\n         <span>\n             {% inlineImg src=\"index/phone-solid.svg\" width=\"18px\" %}\n             17269651686\n         </span>\n         ·\n         <span>\n             {% inlineImg src=\"index/envelope-solid.svg\" width=\"18px\" %}\n             1429053840@qq.com\n         </span>\n         ·\n         <span>\n             {% inlineImg src=\"index/github-brands.svg\" width=\"18px\" %}\n             <a href=\"https://github.com/Bluet1997\">Bluet</a>\n         </span>\n         ·\n         <span>\n             {% inlineImg src=\"index/rss-solid.svg\" width=\"18px\" %}\n             <a href=\"https://bluenote.top\">My Blog</a>\n         </span>\n     </div>\n </center>\n\n\n\n ## {% inlineImg index/info-circle-solid.svg 30px %} 个人信息 \n\n - 男，1996/12，汉族\n - 工作经验：3 年\n\n## {% inlineImg src=\"index/graduation-cap-solid.svg\" width=\"30px\" %}教育经历\n\n- 硕士，谢菲尔德大学，高级计算机科学(一等一学位 73/100)，                            2018.9 ~ 2020.3\n- 学士，江西农业大学，软件工程+英语专业(双学位 83/100)，                              2014.9~2018.7\n- 专业英语四级，四六级，雅思，GRE\n\n## {% inlineImg  src=\"index/briefcase-solid.svg\" width=\"30px\"%} 工作经历\n\n- **韵达东普科技公司，无人车/智能仓研究所，算法工程师，2020.05 - 2021.04**\n\n  负责智能仓机械臂视觉算法，快递分拣及单件分离项目，快递无人车视觉算法。\n\n- **贵州翰凯斯科技(Pixmoving)，自动驾驶RD部门，自动驾驶算法工程师，2021.04 - 2023.02**\n\n  负责半封闭园区L4无人车自动驾驶视觉算法，Robobus/清扫车/定制化项目研发。\n\n## {% inlineImg src=\"index/project-diagram-solid.svg\" width=\"30px\"%} 项目经历\n\n- **PIXMoving-清扫车 垃圾检测/实例分割/路沿检测/多传感器融合**   \t\t\t\t                          2022.03– 2022.09\n\n  **平台**: 自研清扫车 **技术点**: 基于ZED2i相机，使用YOLO-V5和Sparseinst算法，从数据收集/清洗/标注/训练到完成ROS/TensorRT部署，整个项目的链路搭建到部署都是由我个人完成，路沿检测研发基于激光雷达和ZED2i相机落地部署。\n\n- **PIXkit纯视觉跟踪**                                                                                                                   2021.12 – 2022.01\n\n  **平台:** 自研自动驾驶套件+云台相机 **技术点**: 使用YOLOX和SiamFC++算法，完成对行人、车辆、无人机目标的自动跟踪系统的落地。\n\n- **Robobus研发** \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                       2021.07 – 2023.02\n\n  **平台**: 自研Robobus **技术点**: **相机选型**及**布局**设计，**外参/内参**标定，**时间同步**，基于**多相机BEV**感知算法(Transformer)完成3D感知任务的研发，多传感器融合方案的**功能设计**，基础自动驾驶框架基于**Autoware**优化部署的。\n\n- **PIXMoving-KUKA机械臂视觉** \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                           2021.07 - 2023.02\n\n  **平台**: KUKA-KR210 **技术点**: 机械臂**Gazebo**模型构建，**3D结构光**相机选型，实时**RTM-3D**成形件**CAD误差检测**自动化流程设计。\n\n- **PIXkit-Autoware感知板块**                                                                                                      2021.04– 2021.07\n\n  **平台**:自研自动驾驶平台 **技术点**: 完整Autoware技术栈部署，2D/3D感知算法部署优化，所有交付的Pixkit项目均基于此发货。\n\n- **快递无人车视觉研发(韵达研究所-无人车)**                                                                              2020.09 – 2021.03\n\n  **平台**: 自研快递无人车 **技术点**：**联合标定**，**障碍物 2D 检测**，**点云 3D 检测(应用)**，**相机和激光雷达融合**(障碍物检测)。**最终效果**：在 Jetson 开发板上实现了 Mask RCNN/YOLO 实时目标检测，借助 TensorRT 加速/DeepStream，使用docker完成部署。\n\n- **机械臂 3D 视觉应用(韵达研究所-智能仓)**                                                                               2020.07 – 2020.09\n\n  **平台**: 自研 6 轴机械臂 **技术点**：研究前沿的 **6D 姿态估计算法**，借助英伟达 **Isaac 平台**复现算法并实现场景的应用落地。**最终效果**：对比测试了 **PVNet/PoseCNN** 模型，在 **Jetosn Xavier NX** 上实现了 **PoseCNN** 模型的部署，完成码垛任务。[Demo链接](https://v.youku.com/v_show/id_XNDg1NjYzNTA4MA==.html?spm=a2hcb.profile.app.5~5!2~5~5!3~5!2~5~5~A)\n\n- **分拣单件分离 2D 机器视觉研发(韵达研究所-智能仓)**                                                             2020.06 – 2020.07\n\n  **应用场景**: 物流分拨中心包裹分拣 **技术点**：应用**OpenCV**库进行传送带上**单件多件包裹**的识别。**最终效果**：配合六面扫，完成摆臂的单件分离的功能实现，传送带速度 **1.5m/s** 左右。\n\n- **仿生机器人视觉导航控制系统**                                                                                                2019.06 – 2019.09\n\n  **机器人平台**：谢菲尔德机器人实验室，仿生机器人**MiRo**（社交陪伴型机器人）。**技术点**：使用级联分类器（**LBP和HOG**）配合**SVM**支持向量机和神经网络模型（**MobileNet-SSD**等深度学习架构）实现目标检测和识别。使用**OpenCV**库函数，选择**TLD tracker**进行目标跟踪。使用**Braitenberg vehicle**仿生理论进行移动控制。**最终效果**：机器人能够自动的识别对应的分类目标（设计了三种类型目标），进行自主的**拦截行为**，目标是视野内**随机的移动目标**。同时，简化版本是进行**目标的跟随**。[Demo1链接](https://v.youku.com/v_show/id_XNDUyNDI3MTcwNA==.html?spm=a2hzp.8253869.0.0) [Demo2链接](https://v.youku.com/v_show/id_XNDUyNDI3MDc3Mg==.html?spm=a2hzp.8253869.0.0)\n\n## {% inlineImg src=\"index/activity.svg\" width=\"30px\"%} 实习经历\n\n- 谢菲尔德Jaguar Robot实习                                                                                                      2019.12-2020.03\n\n  **内容**：部署一个视觉导航项目在Jaguar robot上，在**Dr Li Sun**的带领下完成了一个**自动化导航**的\t系统部署。[Demo链接](https://v.youku.com/v_show/id_XNDUyMDIyMzU5Ng==.html?spm=a2h3j.8428770.3416059.1)\n\n## {% inlineImg src=\"index/academy.svg\" width=\"30px\"%} 学术经历\n\n- *Application of Computer Vision for A Biomimetic Robot* （**Dissertation Project**)\n- *A Self-supervised, Self-adaptive Model for the Next Generation Vision-based Robot Navigation* (**Research Proposal**)\n\n## {% inlineImg src=\"index/social-activity.svg\" width=\"30px\"%} 社会活动\n\n- **谢菲尔德AMRC工厂“UK-RAS Manufacturing Robotics Challenge”挑战赛**\n\n  负责完成控制**LBR KUKA iiwa**机器臂，进行拾取和放置的任务。\n\n## {% inlineImg src=\"index/personality.svg\" width=\"30px\"%} 兴趣爱好\n\n​\t业余健体爱好者，爱好电子科技产品、撸各类机器人，动手实践能力强。\n","updated":"2023-04-30T05:38:27.959Z","path":"resume/index.html","layout":"page","_id":"clu8483sn000eu9rlgqzbc8ud","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"8ca51907ba13f4d21c3c4969db5b0eeb108874f612b9624f6467db6281a2b0ed\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover":"https://www.synotech.top:5523/wallpaper/5f363d07a24be_270_185.jpg","cover_type":"img","excerpt":"Here's something encrypted, password is required to continue reading.","more":"Here's something encrypted, password is required to continue reading.","origin":"<center>\n     <h1>Bluet</h1>\n     <div>\n         <span>\n             <img class=\"inline-img\" src=\"/resume/index/phone-solid.svg\" style=\"height:width=18px\">\n             17269651686\n         </span>\n         ·\n         <span>\n             <img class=\"inline-img\" src=\"/resume/index/envelope-solid.svg\" style=\"height:width=18px\">\n             1429053840@qq.com\n         </span>\n         ·\n         <span>\n             <img class=\"inline-img\" src=\"/resume/index/github-brands.svg\" style=\"height:width=18px\">\n             <a href=\"https://github.com/Bluet1997\">Bluet</a>\n         </span>\n         ·\n         <span>\n             <img class=\"inline-img\" src=\"/resume/index/rss-solid.svg\" style=\"height:width=18px\">\n             <a href=\"https://bluenote.top\">My Blog</a>\n         </span>\n     </div>\n </center>\n<h2 id=\"swig￼4-个人信息\"><img class=\"inline-img\" src=\"/resume/index/info-circle-solid.svg\" style=\"height:30px\"> 个人信息</h2>\n<ul>\n<li>男，1996/12，汉族</li>\n<li>工作经验：3 年</li>\n</ul>\n<h2 id=\"swig￼5-教育经历\"><img class=\"inline-img\" src=\"/resume/index/graduation-cap-solid.svg\" style=\"height:width=30px\">教育经历</h2>\n<ul>\n<li>硕士，谢菲尔德大学，高级计算机科学(一等一学位 73/100)，                            2018.9 ~ 2020.3</li>\n<li>学士，江西农业大学，软件工程+英语专业(双学位 83/100)，                              2014.9~2018.7</li>\n<li>专业英语四级，四六级，雅思，GRE</li>\n</ul>\n<h2 id=\"swig￼6-工作经历\"><img class=\"inline-img\" src=\"/resume/index/briefcase-solid.svg\" style=\"height:width=30px\"> 工作经历</h2>\n<ul>\n<li>\n<p><strong>韵达东普科技公司，无人车/智能仓研究所，算法工程师，2020.05 - 2021.04</strong></p>\n<p>负责智能仓机械臂视觉算法，快递分拣及单件分离项目，快递无人车视觉算法。</p>\n</li>\n<li>\n<p><strong>贵州翰凯斯科技(Pixmoving)，自动驾驶RD部门，自动驾驶算法工程师，2021.04 - 2023.02</strong></p>\n<p>负责半封闭园区L4无人车自动驾驶视觉算法，Robobus/清扫车/定制化项目研发。</p>\n</li>\n</ul>\n<h2 id=\"swig￼7-项目经历\"><img class=\"inline-img\" src=\"/resume/index/project-diagram-solid.svg\" style=\"height:width=30px\"> 项目经历</h2>\n<ul>\n<li>\n<p><strong>PIXMoving-清扫车 垃圾检测/实例分割/路沿检测/多传感器融合</strong>   \t\t\t\t                          2022.03– 2022.09</p>\n<p><strong>平台</strong>: 自研清扫车 <strong>技术点</strong>: 基于ZED2i相机，使用YOLO-V5和Sparseinst算法，从数据收集/清洗/标注/训练到完成ROS/TensorRT部署，整个项目的链路搭建到部署都是由我个人完成，路沿检测研发基于激光雷达和ZED2i相机落地部署。</p>\n</li>\n<li>\n<p><strong>PIXkit纯视觉跟踪</strong>                                                                                                                   2021.12 – 2022.01</p>\n<p><strong>平台:</strong> 自研自动驾驶套件+云台相机 <strong>技术点</strong>: 使用YOLOX和SiamFC++算法，完成对行人、车辆、无人机目标的自动跟踪系统的落地。</p>\n</li>\n<li>\n<p><strong>Robobus研发</strong> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                       2021.07 – 2023.02</p>\n<p><strong>平台</strong>: 自研Robobus <strong>技术点</strong>: <strong>相机选型</strong>及<strong>布局</strong>设计，<strong>外参/内参</strong>标定，<strong>时间同步</strong>，基于<strong>多相机BEV</strong>感知算法(Transformer)完成3D感知任务的研发，多传感器融合方案的<strong>功能设计</strong>，基础自动驾驶框架基于<strong>Autoware</strong>优化部署的。</p>\n</li>\n<li>\n<p><strong>PIXMoving-KUKA机械臂视觉</strong> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                           2021.07 - 2023.02</p>\n<p><strong>平台</strong>: KUKA-KR210 <strong>技术点</strong>: 机械臂<strong>Gazebo</strong>模型构建，<strong>3D结构光</strong>相机选型，实时<strong>RTM-3D</strong>成形件<strong>CAD误差检测</strong>自动化流程设计。</p>\n</li>\n<li>\n<p><strong>PIXkit-Autoware感知板块</strong>                                                                                                      2021.04– 2021.07</p>\n<p><strong>平台</strong>:自研自动驾驶平台 <strong>技术点</strong>: 完整Autoware技术栈部署，2D/3D感知算法部署优化，所有交付的Pixkit项目均基于此发货。</p>\n</li>\n<li>\n<p><strong>快递无人车视觉研发(韵达研究所-无人车)</strong>                                                                              2020.09 – 2021.03</p>\n<p><strong>平台</strong>: 自研快递无人车 <strong>技术点</strong>：<strong>联合标定</strong>，<strong>障碍物 2D 检测</strong>，<strong>点云 3D 检测(应用)</strong>，<strong>相机和激光雷达融合</strong>(障碍物检测)。<strong>最终效果</strong>：在 Jetson 开发板上实现了 Mask RCNN/YOLO 实时目标检测，借助 TensorRT 加速/DeepStream，使用docker完成部署。</p>\n</li>\n<li>\n<p><strong>机械臂 3D 视觉应用(韵达研究所-智能仓)</strong>                                                                               2020.07 – 2020.09</p>\n<p><strong>平台</strong>: 自研 6 轴机械臂 <strong>技术点</strong>：研究前沿的 <strong>6D 姿态估计算法</strong>，借助英伟达 <strong>Isaac 平台</strong>复现算法并实现场景的应用落地。<strong>最终效果</strong>：对比测试了 <strong>PVNet/PoseCNN</strong> 模型，在 <strong>Jetosn Xavier NX</strong> 上实现了 <strong>PoseCNN</strong> 模型的部署，完成码垛任务。<a href=\"https://v.youku.com/v_show/id_XNDg1NjYzNTA4MA==.html?spm=a2hcb.profile.app.5~5!2~5~5!3~5!2~5~5~A\">Demo链接</a></p>\n</li>\n<li>\n<p><strong>分拣单件分离 2D 机器视觉研发(韵达研究所-智能仓)</strong>                                                             2020.06 – 2020.07</p>\n<p><strong>应用场景</strong>: 物流分拨中心包裹分拣 <strong>技术点</strong>：应用<strong>OpenCV</strong>库进行传送带上<strong>单件多件包裹</strong>的识别。<strong>最终效果</strong>：配合六面扫，完成摆臂的单件分离的功能实现，传送带速度 <strong>1.5m/s</strong> 左右。</p>\n</li>\n<li>\n<p><strong>仿生机器人视觉导航控制系统</strong>                                                                                                2019.06 – 2019.09</p>\n<p><strong>机器人平台</strong>：谢菲尔德机器人实验室，仿生机器人<strong>MiRo</strong>（社交陪伴型机器人）。<strong>技术点</strong>：使用级联分类器（<strong>LBP和HOG</strong>）配合<strong>SVM</strong>支持向量机和神经网络模型（<strong>MobileNet-SSD</strong>等深度学习架构）实现目标检测和识别。使用<strong>OpenCV</strong>库函数，选择<strong>TLD tracker</strong>进行目标跟踪。使用<strong>Braitenberg vehicle</strong>仿生理论进行移动控制。<strong>最终效果</strong>：机器人能够自动的识别对应的分类目标（设计了三种类型目标），进行自主的<strong>拦截行为</strong>，目标是视野内<strong>随机的移动目标</strong>。同时，简化版本是进行<strong>目标的跟随</strong>。<a href=\"https://v.youku.com/v_show/id_XNDUyNDI3MTcwNA==.html?spm=a2hzp.8253869.0.0\">Demo1链接</a> <a href=\"https://v.youku.com/v_show/id_XNDUyNDI3MDc3Mg==.html?spm=a2hzp.8253869.0.0\">Demo2链接</a></p>\n</li>\n</ul>\n<h2 id=\"swig￼8-实习经历\"><img class=\"inline-img\" src=\"/resume/index/activity.svg\" style=\"height:width=30px\"> 实习经历</h2>\n<ul>\n<li>\n<p>谢菲尔德Jaguar Robot实习                                                                                                      2019.12-2020.03</p>\n<p><strong>内容</strong>：部署一个视觉导航项目在Jaguar robot上，在<strong>Dr Li Sun</strong>的带领下完成了一个<strong>自动化导航</strong>的\t系统部署。<a href=\"https://v.youku.com/v_show/id_XNDUyMDIyMzU5Ng==.html?spm=a2h3j.8428770.3416059.1\">Demo链接</a></p>\n</li>\n</ul>\n<h2 id=\"swig￼9-学术经历\"><img class=\"inline-img\" src=\"/resume/index/academy.svg\" style=\"height:width=30px\"> 学术经历</h2>\n<ul>\n<li><em>Application of Computer Vision for A Biomimetic Robot</em> （<strong>Dissertation Project</strong>)</li>\n<li><em>A Self-supervised, Self-adaptive Model for the Next Generation Vision-based Robot Navigation</em> (<strong>Research Proposal</strong>)</li>\n</ul>\n<h2 id=\"swig￼10-社会活动\"><img class=\"inline-img\" src=\"/resume/index/social-activity.svg\" style=\"height:width=30px\"> 社会活动</h2>\n<ul>\n<li>\n<p><strong>谢菲尔德AMRC工厂“UK-RAS Manufacturing Robotics Challenge”挑战赛</strong></p>\n<p>负责完成控制<strong>LBR KUKA iiwa</strong>机器臂，进行拾取和放置的任务。</p>\n</li>\n</ul>\n<h2 id=\"swig￼11-兴趣爱好\"><img class=\"inline-img\" src=\"/resume/index/personality.svg\" style=\"height:width=30px\"> 兴趣爱好</h2>\n<p>​\t业余健体爱好者，爱好电子科技产品、撸各类机器人，动手实践能力强。</p>\n","encrypt":true}],"Post":[{"title":"Gimbal-Camera-Trachking","mathjax":true,"_content":"## 系统部署方式介绍\n**此项目还在迭代中，部署方式采用本地host环境结合Docker环境进行算法的部署。\n以后将会优化镜像部署，不使用本地环境，这样部署效率更高。**\n\n**拉取源码，源码已经提供在gitlab服务器上，地址为*http://220.197.182.34:9010/bluet/gimbal-camera-tracking*，**  \n**gitlab目前部署在我们公司内部服务器, 源码将要分成两部分分别部署在主从机上。**  \n\n---\n### Host主机部署\n\n**主机上需要部署的源码是：fv_tracking, gimbal_control, serial_ros, bboxes_ex_msgs进行部署。**  \n**创建一个gimbal_camera_ws工作空间, 将刚才从gitlab上拉取的源码中以上提到的四个包放在**  \n**gimbal_camera_ws的src文件夹中。**\n\n1. **编译以上的gimbal_camera_ws的源码，在当前工作空间的根目录下执行命令：**\n`$ catkin_make`\n\n2. **编译完成后，链接好相机的USB和串口的USB转换头到工控机上，使用命令：**  \n`$ ls /dev/video* ` `$ ls /dev/ttyUSB0`**去确认是否已经读取到云台相机和云台串口。**  \n\n***备注：在代码中，默认启动和绑定的设备是\" /dev/video0\" 和 \"/dev/ttyUSB0\"， 工控\n机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException***\n\n---\n\n### Docker从机部署\n**Docker镜像目前存储在公司的Harbor仓库中，当前维护版本镜像名为**  \n**pix/gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1**\n\n1. **实例化一个容器，镜像构建容器的启动命令：**  \n`$docker run -it --rm --gpus all\n--net host --privileged --ipc=host --volume /home/t/gimbal-camera-tracking/\ngimbal_camera_ws/:/workspace/gimbal_camera_ws  xxx镜像名`  \n\n2. **启动之后会进入到容器的内部，然后会将本地的源码映射到docker环境中的/workspace**  \n**文件夹下的/gimbal_camera_ws**\n\n***备注：在镜像构建容器的启动命令时，需要加入自己将源码放置的路径，如上命令中，\"/home/t/gimbal-camera-tracking/gimbal_camera_ws\"就是工控机默认将源码***  \n***放在/home/t/路径下，源码的拉取下来的文件夹名为\"gimbal-camera-tracking\"***   \n\n---\n\n### 系统启动方式\n\n**完成上面的主从机的部署后，确认主机的gimbal_camera_ws和从机docker中映射的**  \n**gimbal_camera_ws中的源码都已经编译好了，然后将\"source /home/t/**  \n**gimbal_camera_ws/devel/setup.bash\"和\"source /workspace/gimbal_camera_ws/devel/setup.bash\"**  \n**分别添加到主机和docker从机的\"~/.bashrc\"环境中, 重新source下环境，以初始化环境变量的配置。**  \n\n**接下来是启动步骤：**  \n\n1. **检查是否可以读取到相机图像和串口信息**  \n使用`$ cheese`或者`$ rosrun  fv_tracking web_cam`,命令来读取相机图像，看是否能够打开web_cam节点并读到云台相机图像。  \n使用`$ rosrun serial_ros serial_node` 和`$ rosrun serial_ros moni`，看是否能够读取到云台的姿态信息输出。  \n确认都能读取到数据之后，将cheese关闭，**serial_node和moni就不用关闭。**\n如图：  \n<center class=\"half\">\n    <img src=\"/assets/003/20220114-159d3456.png\" width=\"100\" height=\"100\"/><img src=\"/assets/003/20220114-cddd0cdf.png\" width=\"100\"  height=\"100\"/><img src=\"/assets/003/20220114-923e402c.png\" width=\"100\"  height=\"100\"/>\n</center>\n\n\n\n2. **启动相机节点**  \n在主机中，使用命令`$ rosrun  fv_tracking web_cam`，启动了相机视频流读取节点。\n\n3. **启动YOLOX的ROS节点**  \n在docker中，使用命令`$ roslaunch yolox_ros yolox_ros.launch`，启动了检测器，检测实时图像中的物体。\n\n4. **启动SiamFC++的ROS节点, 确认你需要跟踪的目标在当前相机的视野范围内**  \n在docker中，使用命令`$ roslaunch siamfc_ros siamfc_ros.launch`，跟踪器成功锁定需要被跟踪的目标。\n\n5. **启动主机中的fv_tracking包中的track_kcf的ROS节点, 确认被跟踪的物体是你想要的目标**  \n在主机中，使用命令`$ rosrun fv_tracking tracker_kcf`，云台开始转动跟随被锁定的目标。\n\n6. **启动底盘控制节点, 确认云台已经成功跟随需要被跟踪的目标**  \n在docker中，使用命令`$ roslaunch chassis_control chassis_control.launch`, 开启底盘控制节点，输出控制指令。\n\n7. **启动底盘驱动ROS节点**  \n在主机中，使用命令`$ roslaunch pix_driver pix_driver_read.launch`  \n`$ roslaunch pix_driver pix_driver_write.launch`\n\n8. **确定车辆状态，在遥控器上选择MOD为 self-driving模式，如图已经切换到自动驾驶模式，使用MOD按键进行切换**  \n<div align=center><img src=\"/assets/003/20220114-8bd94d2e.png\" width=\"400\"></div>\n\n---\n\n## 硬件介绍\n\n### 云台硬件规格\n1. **吊舱云台是根据客户需要进行选型，选择支持变焦（10倍）的云台，具有400万有效像素，**\n**三自由度，最大航向角为+-150度无极旋转，工作电流为240mA(@12V)，重量400g（含相机）。**\n2. **云台内部有两路视频流，一路1080P 30FPS本地H.264压缩，存储在设备内，另一路输**\n**出1080P 60FPS格式的HDMI信号，用于无线图传，支持PWM和串口控制。**\n3. **云台可以自动对焦，对焦时间小于1s，焦距范围为 F = 4.9 ~ 49mm，105dB宽动态范**\n**围，温度工作范围-10 至 55 摄氏度。**\n<center class=\"half\">\n    <img src=\"/assets/003/20220114-e0917cd4.png\" width=\"100\"/><img src=\"/assets/003/20220112-4b9a80b5.png\" width=\"100\">\n</center>\n\n---\n## 算法框架介绍\n**云台跟踪联动算法分为两大部分，第一部分是基于深度学习的目标检测算法和目标跟踪算法，\n第二部分是基于PID调节的云台控制算法和底盘控制算法。**  \n\n**流程图如下：**   \n<div align=center><img src=\"/assets/003/20220112-76ab38e2.png\" width=\"40%\"></div>\n\n\n**淡蓝色部分为检测跟踪算法的流程，深蓝色部分为云台和底盘联动控制算法部分**\n\n---\n\n## 云台ROS驱动介绍\n**云台ROS驱动，根据云台供应商给的串口指令集进行串口编码控制，在源码中serial_ros包主要用来\n将云台的控制指令通过ROS进行转换为串口的指令下发给云台控制板。其中主要需要启动的ROS节点是\nserial_node 和 moni。**\n\n启动方式为：  \n`$ rosrun serial_ros serial_node`  \n`$ rosrun serial_ros moni`\n\n**同时云台驱动中也包含了相机启动和KCF跟踪的fv_tracking包，主要启动的ROS节点为\nweb_cam和tracker_kcf**。\n\n启动方式为：  \n`$ rosrun fv_tracking web_cam`  \n`$rosrun fv_tracking tracker_kcf`\n\n**启动后，相机画面及跟踪窗口效果如下：**  \n\n<center class=\"half\">\n    <img src=\"/assets/003/20220116-67901f3c.png\" width=\"400\"/>\n</center>\n\n---\n\n## 检测算法介绍\n**使用的目标检测算法为YOLOX，YOLOX的github[链接](https://github.com/Megvii-BaseDetection/YOLOX \"YOLOX\")**  \n**将YOLOX的源码整合为ROS代码下，改为yolox_ros的ROS包**\n\n启动方式为：  \n`$roslaunch yolox_ros yolox_ros.launch`  \n**启动之后，yolox_ros节点将会订阅来自云台相机的原始图像数据，生成检测结果并展示在桌面上。**\n\n**yolo_ros**包中发布的话题为：**\"/yolox/bounding_boxes\"** 和 **\" /yolox/image_raw\"**。  \n**yolo_ros**包中订阅的话题为：**\"/camera/rgb/image_raw\"**。\n\n**启动后，检测算法窗口效果如下：**  \n\n<center class=\"half\">\n    <img src=\"/assets/003/20220116-76d60e91.png\" width=\"400\"/>\n</center>\n\n---\n\n## 跟踪算法介绍\n**使用的目标跟踪算法SiamFC++，SiamFC++的github[链接](https://github.com/MegviiDetection/video_analyst \"SiamFC++\")**。\n**基于SiamFC++的源码整合为ROS代码下，开发了siamfc_ros的ROS包**。\n\n启动方式为：  \n`$roslaunch siamfc_ros siamfc_ros.launch`  \n**启动之后，siamfc_ros节点将会订阅来自云台相机的原始图像数据，以及来自yolox_ros节**  \n**点的检测到的物体选框，物体选框当前为自动选则模式，选择被检测到的第一个物体，下一步迭代**  \n**会添加，优化一个ID选择器，让用户可以选择当前画面中被检测到物体的ID，然后输入到跟踪**  \n**器中，会生成跟踪结果并展示在桌面上。**\n\n**siamfc_ros**包中发布的话题为：**\"/siamfc/image_raw\"** 和 **\"/siamfc/bounding_box\"**。  \n**siamfc_ros**包中订阅的话题为：**\"/yolox/bounding_boxes \"** 和 **\"/camera/rgb/image_raw \"**。\n\n**启动后，跟踪算法窗口效果如下：**  \n\n<center class=\"half\">\n    <img src=\"/assets/003/20220116-3b5f0f20.png\" width=\"400\"/>\n</center>\n---\n\n## 联动控制算法介绍\n**联动控制算法主要流程是依赖云台的PID调节控制输出的yaw值和底盘的yaw值进行关联，当**  \n**云台跟随目标进行转动的时候，将云台的yaw值实时发出到ROS话题**\"/gimbal_camera/rpy_data\"**上，然后底盘根据云台的**  \n**yaw值进行处理，转换为底盘的航向角。**\n\n### 云台控制算法\n**云台控制算法采用的PID进行调节，PID控制的error主要是根据图像中心十字靶心像素坐标和被跟踪物体的boundingbox的物体中心的像素坐标的误差。**\n\n\n\n### 底盘控制算法\n**底盘的控制算法，其中一部分基于视觉的大致观测距离判断进行线速度的控制，另一部分基于云台的yaw值\n实时对角速度进行控制。**\n\n---\n","source":"_drafts/Gimbal-Camera-Trachking.md","raw":"---\ntitle: Gimbal-Camera-Trachking\ntags: Projects\nmathjax: true\n---\n## 系统部署方式介绍\n**此项目还在迭代中，部署方式采用本地host环境结合Docker环境进行算法的部署。\n以后将会优化镜像部署，不使用本地环境，这样部署效率更高。**\n\n**拉取源码，源码已经提供在gitlab服务器上，地址为*http://220.197.182.34:9010/bluet/gimbal-camera-tracking*，**  \n**gitlab目前部署在我们公司内部服务器, 源码将要分成两部分分别部署在主从机上。**  \n\n---\n### Host主机部署\n\n**主机上需要部署的源码是：fv_tracking, gimbal_control, serial_ros, bboxes_ex_msgs进行部署。**  \n**创建一个gimbal_camera_ws工作空间, 将刚才从gitlab上拉取的源码中以上提到的四个包放在**  \n**gimbal_camera_ws的src文件夹中。**\n\n1. **编译以上的gimbal_camera_ws的源码，在当前工作空间的根目录下执行命令：**\n`$ catkin_make`\n\n2. **编译完成后，链接好相机的USB和串口的USB转换头到工控机上，使用命令：**  \n`$ ls /dev/video* ` `$ ls /dev/ttyUSB0`**去确认是否已经读取到云台相机和云台串口。**  \n\n***备注：在代码中，默认启动和绑定的设备是\" /dev/video0\" 和 \"/dev/ttyUSB0\"， 工控\n机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException***\n\n---\n\n### Docker从机部署\n**Docker镜像目前存储在公司的Harbor仓库中，当前维护版本镜像名为**  \n**pix/gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1**\n\n1. **实例化一个容器，镜像构建容器的启动命令：**  \n`$docker run -it --rm --gpus all\n--net host --privileged --ipc=host --volume /home/t/gimbal-camera-tracking/\ngimbal_camera_ws/:/workspace/gimbal_camera_ws  xxx镜像名`  \n\n2. **启动之后会进入到容器的内部，然后会将本地的源码映射到docker环境中的/workspace**  \n**文件夹下的/gimbal_camera_ws**\n\n***备注：在镜像构建容器的启动命令时，需要加入自己将源码放置的路径，如上命令中，\"/home/t/gimbal-camera-tracking/gimbal_camera_ws\"就是工控机默认将源码***  \n***放在/home/t/路径下，源码的拉取下来的文件夹名为\"gimbal-camera-tracking\"***   \n\n---\n\n### 系统启动方式\n\n**完成上面的主从机的部署后，确认主机的gimbal_camera_ws和从机docker中映射的**  \n**gimbal_camera_ws中的源码都已经编译好了，然后将\"source /home/t/**  \n**gimbal_camera_ws/devel/setup.bash\"和\"source /workspace/gimbal_camera_ws/devel/setup.bash\"**  \n**分别添加到主机和docker从机的\"~/.bashrc\"环境中, 重新source下环境，以初始化环境变量的配置。**  \n\n**接下来是启动步骤：**  \n\n1. **检查是否可以读取到相机图像和串口信息**  \n使用`$ cheese`或者`$ rosrun  fv_tracking web_cam`,命令来读取相机图像，看是否能够打开web_cam节点并读到云台相机图像。  \n使用`$ rosrun serial_ros serial_node` 和`$ rosrun serial_ros moni`，看是否能够读取到云台的姿态信息输出。  \n确认都能读取到数据之后，将cheese关闭，**serial_node和moni就不用关闭。**\n如图：  \n<center class=\"half\">\n    <img src=\"/assets/003/20220114-159d3456.png\" width=\"100\" height=\"100\"/><img src=\"/assets/003/20220114-cddd0cdf.png\" width=\"100\"  height=\"100\"/><img src=\"/assets/003/20220114-923e402c.png\" width=\"100\"  height=\"100\"/>\n</center>\n\n\n\n2. **启动相机节点**  \n在主机中，使用命令`$ rosrun  fv_tracking web_cam`，启动了相机视频流读取节点。\n\n3. **启动YOLOX的ROS节点**  \n在docker中，使用命令`$ roslaunch yolox_ros yolox_ros.launch`，启动了检测器，检测实时图像中的物体。\n\n4. **启动SiamFC++的ROS节点, 确认你需要跟踪的目标在当前相机的视野范围内**  \n在docker中，使用命令`$ roslaunch siamfc_ros siamfc_ros.launch`，跟踪器成功锁定需要被跟踪的目标。\n\n5. **启动主机中的fv_tracking包中的track_kcf的ROS节点, 确认被跟踪的物体是你想要的目标**  \n在主机中，使用命令`$ rosrun fv_tracking tracker_kcf`，云台开始转动跟随被锁定的目标。\n\n6. **启动底盘控制节点, 确认云台已经成功跟随需要被跟踪的目标**  \n在docker中，使用命令`$ roslaunch chassis_control chassis_control.launch`, 开启底盘控制节点，输出控制指令。\n\n7. **启动底盘驱动ROS节点**  \n在主机中，使用命令`$ roslaunch pix_driver pix_driver_read.launch`  \n`$ roslaunch pix_driver pix_driver_write.launch`\n\n8. **确定车辆状态，在遥控器上选择MOD为 self-driving模式，如图已经切换到自动驾驶模式，使用MOD按键进行切换**  \n<div align=center><img src=\"/assets/003/20220114-8bd94d2e.png\" width=\"400\"></div>\n\n---\n\n## 硬件介绍\n\n### 云台硬件规格\n1. **吊舱云台是根据客户需要进行选型，选择支持变焦（10倍）的云台，具有400万有效像素，**\n**三自由度，最大航向角为+-150度无极旋转，工作电流为240mA(@12V)，重量400g（含相机）。**\n2. **云台内部有两路视频流，一路1080P 30FPS本地H.264压缩，存储在设备内，另一路输**\n**出1080P 60FPS格式的HDMI信号，用于无线图传，支持PWM和串口控制。**\n3. **云台可以自动对焦，对焦时间小于1s，焦距范围为 F = 4.9 ~ 49mm，105dB宽动态范**\n**围，温度工作范围-10 至 55 摄氏度。**\n<center class=\"half\">\n    <img src=\"/assets/003/20220114-e0917cd4.png\" width=\"100\"/><img src=\"/assets/003/20220112-4b9a80b5.png\" width=\"100\">\n</center>\n\n---\n## 算法框架介绍\n**云台跟踪联动算法分为两大部分，第一部分是基于深度学习的目标检测算法和目标跟踪算法，\n第二部分是基于PID调节的云台控制算法和底盘控制算法。**  \n\n**流程图如下：**   \n<div align=center><img src=\"/assets/003/20220112-76ab38e2.png\" width=\"40%\"></div>\n\n\n**淡蓝色部分为检测跟踪算法的流程，深蓝色部分为云台和底盘联动控制算法部分**\n\n---\n\n## 云台ROS驱动介绍\n**云台ROS驱动，根据云台供应商给的串口指令集进行串口编码控制，在源码中serial_ros包主要用来\n将云台的控制指令通过ROS进行转换为串口的指令下发给云台控制板。其中主要需要启动的ROS节点是\nserial_node 和 moni。**\n\n启动方式为：  \n`$ rosrun serial_ros serial_node`  \n`$ rosrun serial_ros moni`\n\n**同时云台驱动中也包含了相机启动和KCF跟踪的fv_tracking包，主要启动的ROS节点为\nweb_cam和tracker_kcf**。\n\n启动方式为：  \n`$ rosrun fv_tracking web_cam`  \n`$rosrun fv_tracking tracker_kcf`\n\n**启动后，相机画面及跟踪窗口效果如下：**  \n\n<center class=\"half\">\n    <img src=\"/assets/003/20220116-67901f3c.png\" width=\"400\"/>\n</center>\n\n---\n\n## 检测算法介绍\n**使用的目标检测算法为YOLOX，YOLOX的github[链接](https://github.com/Megvii-BaseDetection/YOLOX \"YOLOX\")**  \n**将YOLOX的源码整合为ROS代码下，改为yolox_ros的ROS包**\n\n启动方式为：  \n`$roslaunch yolox_ros yolox_ros.launch`  \n**启动之后，yolox_ros节点将会订阅来自云台相机的原始图像数据，生成检测结果并展示在桌面上。**\n\n**yolo_ros**包中发布的话题为：**\"/yolox/bounding_boxes\"** 和 **\" /yolox/image_raw\"**。  \n**yolo_ros**包中订阅的话题为：**\"/camera/rgb/image_raw\"**。\n\n**启动后，检测算法窗口效果如下：**  \n\n<center class=\"half\">\n    <img src=\"/assets/003/20220116-76d60e91.png\" width=\"400\"/>\n</center>\n\n---\n\n## 跟踪算法介绍\n**使用的目标跟踪算法SiamFC++，SiamFC++的github[链接](https://github.com/MegviiDetection/video_analyst \"SiamFC++\")**。\n**基于SiamFC++的源码整合为ROS代码下，开发了siamfc_ros的ROS包**。\n\n启动方式为：  \n`$roslaunch siamfc_ros siamfc_ros.launch`  \n**启动之后，siamfc_ros节点将会订阅来自云台相机的原始图像数据，以及来自yolox_ros节**  \n**点的检测到的物体选框，物体选框当前为自动选则模式，选择被检测到的第一个物体，下一步迭代**  \n**会添加，优化一个ID选择器，让用户可以选择当前画面中被检测到物体的ID，然后输入到跟踪**  \n**器中，会生成跟踪结果并展示在桌面上。**\n\n**siamfc_ros**包中发布的话题为：**\"/siamfc/image_raw\"** 和 **\"/siamfc/bounding_box\"**。  \n**siamfc_ros**包中订阅的话题为：**\"/yolox/bounding_boxes \"** 和 **\"/camera/rgb/image_raw \"**。\n\n**启动后，跟踪算法窗口效果如下：**  \n\n<center class=\"half\">\n    <img src=\"/assets/003/20220116-3b5f0f20.png\" width=\"400\"/>\n</center>\n---\n\n## 联动控制算法介绍\n**联动控制算法主要流程是依赖云台的PID调节控制输出的yaw值和底盘的yaw值进行关联，当**  \n**云台跟随目标进行转动的时候，将云台的yaw值实时发出到ROS话题**\"/gimbal_camera/rpy_data\"**上，然后底盘根据云台的**  \n**yaw值进行处理，转换为底盘的航向角。**\n\n### 云台控制算法\n**云台控制算法采用的PID进行调节，PID控制的error主要是根据图像中心十字靶心像素坐标和被跟踪物体的boundingbox的物体中心的像素坐标的误差。**\n\n\n\n### 底盘控制算法\n**底盘的控制算法，其中一部分基于视觉的大致观测距离判断进行线速度的控制，另一部分基于云台的yaw值\n实时对角速度进行控制。**\n\n---\n","slug":"Gimbal-Camera-Trachking","published":0,"date":"2023-03-20T11:34:04.034Z","updated":"2022-08-23T18:41:34.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clu8483sa0001u9rle4hu33zu","content":"<h2 id=\"系统部署方式介绍\">系统部署方式介绍</h2>\n<p><strong>此项目还在迭代中，部署方式采用本地host环境结合Docker环境进行算法的部署。<br>\n以后将会优化镜像部署，不使用本地环境，这样部署效率更高。</strong></p>\n<p><strong>拉取源码，源码已经提供在gitlab服务器上，地址为*<a href=\"http://220.197.182.34:9010/bluet/gimbal-camera-tracking*%EF%BC%8C\">http://220.197.182.34:9010/bluet/gimbal-camera-tracking*，</a></strong><br>\n<strong>gitlab目前部署在我们公司内部服务器, 源码将要分成两部分分别部署在主从机上。</strong></p>\n<hr>\n<h3 id=\"Host主机部署\">Host主机部署</h3>\n<p><strong>主机上需要部署的源码是：fv_tracking, gimbal_control, serial_ros, bboxes_ex_msgs进行部署。</strong><br>\n<strong>创建一个gimbal_camera_ws工作空间, 将刚才从gitlab上拉取的源码中以上提到的四个包放在</strong><br>\n<strong>gimbal_camera_ws的src文件夹中。</strong></p>\n<ol>\n<li>\n<p><strong>编译以上的gimbal_camera_ws的源码，在当前工作空间的根目录下执行命令：</strong><br>\n<code>$ catkin_make</code></p>\n</li>\n<li>\n<p><strong>编译完成后，链接好相机的USB和串口的USB转换头到工控机上，使用命令：</strong><br>\n<code>$ ls /dev/video* </code> <code>$ ls /dev/ttyUSB0</code><strong>去确认是否已经读取到云台相机和云台串口。</strong></p>\n</li>\n</ol>\n<p><em><strong>备注：在代码中，默认启动和绑定的设备是&quot; /dev/video0&quot; 和 “/dev/ttyUSB0”， 工控<br>\n机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException</strong></em></p>\n<hr>\n<h3 id=\"Docker从机部署\">Docker从机部署</h3>\n<p><strong>Docker镜像目前存储在公司的Harbor仓库中，当前维护版本镜像名为</strong><br>\n<strong>pix/gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1</strong></p>\n<ol>\n<li>\n<p><strong>实例化一个容器，镜像构建容器的启动命令：</strong><br>\n<code>$docker run -it --rm --gpus all --net host --privileged --ipc=host --volume /home/t/gimbal-camera-tracking/ gimbal_camera_ws/:/workspace/gimbal_camera_ws  xxx镜像名</code></p>\n</li>\n<li>\n<p><strong>启动之后会进入到容器的内部，然后会将本地的源码映射到docker环境中的/workspace</strong><br>\n<strong>文件夹下的/gimbal_camera_ws</strong></p>\n</li>\n</ol>\n<p><em><strong>备注：在镜像构建容器的启动命令时，需要加入自己将源码放置的路径，如上命令中，&quot;/home/t/gimbal-camera-tracking/gimbal_camera_ws&quot;就是工控机默认将源码</strong></em><br>\n<em><strong>放在/home/t/路径下，源码的拉取下来的文件夹名为&quot;gimbal-camera-tracking&quot;</strong></em></p>\n<hr>\n<h3 id=\"系统启动方式\">系统启动方式</h3>\n<p><strong>完成上面的主从机的部署后，确认主机的gimbal_camera_ws和从机docker中映射的</strong><br>\n<strong>gimbal_camera_ws中的源码都已经编译好了，然后将&quot;source /home/t/</strong><br>\n<strong>gimbal_camera_ws/devel/setup.bash&quot;和&quot;source /workspace/gimbal_camera_ws/devel/setup.bash&quot;</strong><br>\n<strong>分别添加到主机和docker从机的&quot;~/.bashrc&quot;环境中, 重新source下环境，以初始化环境变量的配置。</strong></p>\n<p><strong>接下来是启动步骤：</strong></p>\n<ol>\n<li><strong>检查是否可以读取到相机图像和串口信息</strong><br>\n使用<code>$ cheese</code>或者<code>$ rosrun  fv_tracking web_cam</code>,命令来读取相机图像，看是否能够打开web_cam节点并读到云台相机图像。<br>\n使用<code>$ rosrun serial_ros serial_node</code> 和<code>$ rosrun serial_ros moni</code>，看是否能够读取到云台的姿态信息输出。<br>\n确认都能读取到数据之后，将cheese关闭，<strong>serial_node和moni就不用关闭。</strong><br>\n如图：</li>\n</ol>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-159d3456.png\" width=\"100\" height=\"100\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-cddd0cdf.png\" width=\"100\" height=\"100\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-923e402c.png\" width=\"100\" height=\"100\">\n</center>\n<ol start=\"2\">\n<li>\n<p><strong>启动相机节点</strong><br>\n在主机中，使用命令<code>$ rosrun  fv_tracking web_cam</code>，启动了相机视频流读取节点。</p>\n</li>\n<li>\n<p><strong>启动YOLOX的ROS节点</strong><br>\n在docker中，使用命令<code>$ roslaunch yolox_ros yolox_ros.launch</code>，启动了检测器，检测实时图像中的物体。</p>\n</li>\n<li>\n<p><strong>启动SiamFC++的ROS节点, 确认你需要跟踪的目标在当前相机的视野范围内</strong><br>\n在docker中，使用命令<code>$ roslaunch siamfc_ros siamfc_ros.launch</code>，跟踪器成功锁定需要被跟踪的目标。</p>\n</li>\n<li>\n<p><strong>启动主机中的fv_tracking包中的track_kcf的ROS节点, 确认被跟踪的物体是你想要的目标</strong><br>\n在主机中，使用命令<code>$ rosrun fv_tracking tracker_kcf</code>，云台开始转动跟随被锁定的目标。</p>\n</li>\n<li>\n<p><strong>启动底盘控制节点, 确认云台已经成功跟随需要被跟踪的目标</strong><br>\n在docker中，使用命令<code>$ roslaunch chassis_control chassis_control.launch</code>, 开启底盘控制节点，输出控制指令。</p>\n</li>\n<li>\n<p><strong>启动底盘驱动ROS节点</strong><br>\n在主机中，使用命令<code>$ roslaunch pix_driver pix_driver_read.launch</code><br>\n<code>$ roslaunch pix_driver pix_driver_write.launch</code></p>\n</li>\n<li>\n<p><strong>确定车辆状态，在遥控器上选择MOD为 self-driving模式，如图已经切换到自动驾驶模式，使用MOD按键进行切换</strong></p>\n</li>\n</ol>\n<div align=\"center\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-8bd94d2e.png\" width=\"400\"></div>\n<hr>\n<h2 id=\"硬件介绍\">硬件介绍</h2>\n<h3 id=\"云台硬件规格\">云台硬件规格</h3>\n<ol>\n<li><strong>吊舱云台是根据客户需要进行选型，选择支持变焦（10倍）的云台，具有400万有效像素，</strong><br>\n<strong>三自由度，最大航向角为±150度无极旋转，工作电流为240mA(@12V)，重量400g（含相机）。</strong></li>\n<li><strong>云台内部有两路视频流，一路1080P 30FPS本地H.264压缩，存储在设备内，另一路输</strong><br>\n<strong>出1080P 60FPS格式的HDMI信号，用于无线图传，支持PWM和串口控制。</strong></li>\n<li><strong>云台可以自动对焦，对焦时间小于1s，焦距范围为 F = 4.9 ~ 49mm，105dB宽动态范</strong><br>\n<strong>围，温度工作范围-10 至 55 摄氏度。</strong></li>\n</ol>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-e0917cd4.png\" width=\"100\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220112-4b9a80b5.png\" width=\"100\">\n</center>\n<hr>\n<h2 id=\"算法框架介绍\">算法框架介绍</h2>\n<p><strong>云台跟踪联动算法分为两大部分，第一部分是基于深度学习的目标检测算法和目标跟踪算法，<br>\n第二部分是基于PID调节的云台控制算法和底盘控制算法。</strong></p>\n<p><strong>流程图如下：</strong></p>\n<div align=\"center\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220112-76ab38e2.png\" width=\"40%\"></div>\n<p><strong>淡蓝色部分为检测跟踪算法的流程，深蓝色部分为云台和底盘联动控制算法部分</strong></p>\n<hr>\n<h2 id=\"云台ROS驱动介绍\">云台ROS驱动介绍</h2>\n<p><strong>云台ROS驱动，根据云台供应商给的串口指令集进行串口编码控制，在源码中serial_ros包主要用来<br>\n将云台的控制指令通过ROS进行转换为串口的指令下发给云台控制板。其中主要需要启动的ROS节点是<br>\nserial_node 和 moni。</strong></p>\n<p>启动方式为：<br>\n<code>$ rosrun serial_ros serial_node</code><br>\n<code>$ rosrun serial_ros moni</code></p>\n<p><strong>同时云台驱动中也包含了相机启动和KCF跟踪的fv_tracking包，主要启动的ROS节点为<br>\nweb_cam和tracker_kcf</strong>。</p>\n<p>启动方式为：<br>\n<code>$ rosrun fv_tracking web_cam</code><br>\n<code>$rosrun fv_tracking tracker_kcf</code></p>\n<p><strong>启动后，相机画面及跟踪窗口效果如下：</strong></p>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220116-67901f3c.png\" width=\"400\">\n</center>\n<hr>\n<h2 id=\"检测算法介绍\">检测算法介绍</h2>\n<p><strong>使用的目标检测算法为YOLOX，YOLOX的github<a href=\"https://github.com/Megvii-BaseDetection/YOLOX\" title=\"YOLOX\">链接</a></strong><br>\n<strong>将YOLOX的源码整合为ROS代码下，改为yolox_ros的ROS包</strong></p>\n<p>启动方式为：<br>\n<code>$roslaunch yolox_ros yolox_ros.launch</code><br>\n<strong>启动之后，yolox_ros节点将会订阅来自云台相机的原始图像数据，生成检测结果并展示在桌面上。</strong></p>\n<p><strong>yolo_ros</strong>包中发布的话题为：<strong>“/yolox/bounding_boxes”</strong> 和 <strong>&quot; /yolox/image_raw&quot;</strong>。<br>\n<strong>yolo_ros</strong>包中订阅的话题为：<strong>“/camera/rgb/image_raw”</strong>。</p>\n<p><strong>启动后，检测算法窗口效果如下：</strong></p>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220116-76d60e91.png\" width=\"400\">\n</center>\n<hr>\n<h2 id=\"跟踪算法介绍\">跟踪算法介绍</h2>\n<p><strong>使用的目标跟踪算法SiamFC++，SiamFC++的github<a href=\"https://github.com/MegviiDetection/video_analyst\" title=\"SiamFC++\">链接</a></strong>。<br>\n<strong>基于SiamFC++的源码整合为ROS代码下，开发了siamfc_ros的ROS包</strong>。</p>\n<p>启动方式为：<br>\n<code>$roslaunch siamfc_ros siamfc_ros.launch</code><br>\n<strong>启动之后，siamfc_ros节点将会订阅来自云台相机的原始图像数据，以及来自yolox_ros节</strong><br>\n<strong>点的检测到的物体选框，物体选框当前为自动选则模式，选择被检测到的第一个物体，下一步迭代</strong><br>\n<strong>会添加，优化一个ID选择器，让用户可以选择当前画面中被检测到物体的ID，然后输入到跟踪</strong><br>\n<strong>器中，会生成跟踪结果并展示在桌面上。</strong></p>\n<p><strong>siamfc_ros</strong>包中发布的话题为：<strong>“/siamfc/image_raw”</strong> 和 <strong>“/siamfc/bounding_box”</strong>。<br>\n<strong>siamfc_ros</strong>包中订阅的话题为：<strong>&quot;/yolox/bounding_boxes &quot;</strong> 和 <strong>&quot;/camera/rgb/image_raw &quot;</strong>。</p>\n<p><strong>启动后，跟踪算法窗口效果如下：</strong></p>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220116-3b5f0f20.png\" width=\"400\">\n</center>\n---\n<h2 id=\"联动控制算法介绍\">联动控制算法介绍</h2>\n<p><strong>联动控制算法主要流程是依赖云台的PID调节控制输出的yaw值和底盘的yaw值进行关联，当</strong><br>\n<strong>云台跟随目标进行转动的时候，将云台的yaw值实时发出到ROS话题</strong>&quot;/gimbal_camera/rpy_data&quot;<strong>上，然后底盘根据云台的</strong><br>\n<strong>yaw值进行处理，转换为底盘的航向角。</strong></p>\n<h3 id=\"云台控制算法\">云台控制算法</h3>\n<p><strong>云台控制算法采用的PID进行调节，PID控制的error主要是根据图像中心十字靶心像素坐标和被跟踪物体的boundingbox的物体中心的像素坐标的误差。</strong></p>\n<h3 id=\"底盘控制算法\">底盘控制算法</h3>\n<p><strong>底盘的控制算法，其中一部分基于视觉的大致观测距离判断进行线速度的控制，另一部分基于云台的yaw值<br>\n实时对角速度进行控制。</strong></p>\n<hr>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover":"https://www.synotech.top:5523/wallpaper/5f39f8e53b026_270_185.png","cover_type":"img","excerpt":"","more":"<h2 id=\"系统部署方式介绍\">系统部署方式介绍</h2>\n<p><strong>此项目还在迭代中，部署方式采用本地host环境结合Docker环境进行算法的部署。<br>\n以后将会优化镜像部署，不使用本地环境，这样部署效率更高。</strong></p>\n<p><strong>拉取源码，源码已经提供在gitlab服务器上，地址为*<a href=\"http://220.197.182.34:9010/bluet/gimbal-camera-tracking*%EF%BC%8C\">http://220.197.182.34:9010/bluet/gimbal-camera-tracking*，</a></strong><br>\n<strong>gitlab目前部署在我们公司内部服务器, 源码将要分成两部分分别部署在主从机上。</strong></p>\n<hr>\n<h3 id=\"Host主机部署\">Host主机部署</h3>\n<p><strong>主机上需要部署的源码是：fv_tracking, gimbal_control, serial_ros, bboxes_ex_msgs进行部署。</strong><br>\n<strong>创建一个gimbal_camera_ws工作空间, 将刚才从gitlab上拉取的源码中以上提到的四个包放在</strong><br>\n<strong>gimbal_camera_ws的src文件夹中。</strong></p>\n<ol>\n<li>\n<p><strong>编译以上的gimbal_camera_ws的源码，在当前工作空间的根目录下执行命令：</strong><br>\n<code>$ catkin_make</code></p>\n</li>\n<li>\n<p><strong>编译完成后，链接好相机的USB和串口的USB转换头到工控机上，使用命令：</strong><br>\n<code>$ ls /dev/video* </code> <code>$ ls /dev/ttyUSB0</code><strong>去确认是否已经读取到云台相机和云台串口。</strong></p>\n</li>\n</ol>\n<p><em><strong>备注：在代码中，默认启动和绑定的设备是&quot; /dev/video0&quot; 和 “/dev/ttyUSB0”， 工控<br>\n机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException</strong></em></p>\n<hr>\n<h3 id=\"Docker从机部署\">Docker从机部署</h3>\n<p><strong>Docker镜像目前存储在公司的Harbor仓库中，当前维护版本镜像名为</strong><br>\n<strong>pix/gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1</strong></p>\n<ol>\n<li>\n<p><strong>实例化一个容器，镜像构建容器的启动命令：</strong><br>\n<code>$docker run -it --rm --gpus all --net host --privileged --ipc=host --volume /home/t/gimbal-camera-tracking/ gimbal_camera_ws/:/workspace/gimbal_camera_ws  xxx镜像名</code></p>\n</li>\n<li>\n<p><strong>启动之后会进入到容器的内部，然后会将本地的源码映射到docker环境中的/workspace</strong><br>\n<strong>文件夹下的/gimbal_camera_ws</strong></p>\n</li>\n</ol>\n<p><em><strong>备注：在镜像构建容器的启动命令时，需要加入自己将源码放置的路径，如上命令中，&quot;/home/t/gimbal-camera-tracking/gimbal_camera_ws&quot;就是工控机默认将源码</strong></em><br>\n<em><strong>放在/home/t/路径下，源码的拉取下来的文件夹名为&quot;gimbal-camera-tracking&quot;</strong></em></p>\n<hr>\n<h3 id=\"系统启动方式\">系统启动方式</h3>\n<p><strong>完成上面的主从机的部署后，确认主机的gimbal_camera_ws和从机docker中映射的</strong><br>\n<strong>gimbal_camera_ws中的源码都已经编译好了，然后将&quot;source /home/t/</strong><br>\n<strong>gimbal_camera_ws/devel/setup.bash&quot;和&quot;source /workspace/gimbal_camera_ws/devel/setup.bash&quot;</strong><br>\n<strong>分别添加到主机和docker从机的&quot;~/.bashrc&quot;环境中, 重新source下环境，以初始化环境变量的配置。</strong></p>\n<p><strong>接下来是启动步骤：</strong></p>\n<ol>\n<li><strong>检查是否可以读取到相机图像和串口信息</strong><br>\n使用<code>$ cheese</code>或者<code>$ rosrun  fv_tracking web_cam</code>,命令来读取相机图像，看是否能够打开web_cam节点并读到云台相机图像。<br>\n使用<code>$ rosrun serial_ros serial_node</code> 和<code>$ rosrun serial_ros moni</code>，看是否能够读取到云台的姿态信息输出。<br>\n确认都能读取到数据之后，将cheese关闭，<strong>serial_node和moni就不用关闭。</strong><br>\n如图：</li>\n</ol>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-159d3456.png\" width=\"100\" height=\"100\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-cddd0cdf.png\" width=\"100\" height=\"100\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-923e402c.png\" width=\"100\" height=\"100\">\n</center>\n<ol start=\"2\">\n<li>\n<p><strong>启动相机节点</strong><br>\n在主机中，使用命令<code>$ rosrun  fv_tracking web_cam</code>，启动了相机视频流读取节点。</p>\n</li>\n<li>\n<p><strong>启动YOLOX的ROS节点</strong><br>\n在docker中，使用命令<code>$ roslaunch yolox_ros yolox_ros.launch</code>，启动了检测器，检测实时图像中的物体。</p>\n</li>\n<li>\n<p><strong>启动SiamFC++的ROS节点, 确认你需要跟踪的目标在当前相机的视野范围内</strong><br>\n在docker中，使用命令<code>$ roslaunch siamfc_ros siamfc_ros.launch</code>，跟踪器成功锁定需要被跟踪的目标。</p>\n</li>\n<li>\n<p><strong>启动主机中的fv_tracking包中的track_kcf的ROS节点, 确认被跟踪的物体是你想要的目标</strong><br>\n在主机中，使用命令<code>$ rosrun fv_tracking tracker_kcf</code>，云台开始转动跟随被锁定的目标。</p>\n</li>\n<li>\n<p><strong>启动底盘控制节点, 确认云台已经成功跟随需要被跟踪的目标</strong><br>\n在docker中，使用命令<code>$ roslaunch chassis_control chassis_control.launch</code>, 开启底盘控制节点，输出控制指令。</p>\n</li>\n<li>\n<p><strong>启动底盘驱动ROS节点</strong><br>\n在主机中，使用命令<code>$ roslaunch pix_driver pix_driver_read.launch</code><br>\n<code>$ roslaunch pix_driver pix_driver_write.launch</code></p>\n</li>\n<li>\n<p><strong>确定车辆状态，在遥控器上选择MOD为 self-driving模式，如图已经切换到自动驾驶模式，使用MOD按键进行切换</strong></p>\n</li>\n</ol>\n<div align=\"center\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-8bd94d2e.png\" width=\"400\"></div>\n<hr>\n<h2 id=\"硬件介绍\">硬件介绍</h2>\n<h3 id=\"云台硬件规格\">云台硬件规格</h3>\n<ol>\n<li><strong>吊舱云台是根据客户需要进行选型，选择支持变焦（10倍）的云台，具有400万有效像素，</strong><br>\n<strong>三自由度，最大航向角为±150度无极旋转，工作电流为240mA(@12V)，重量400g（含相机）。</strong></li>\n<li><strong>云台内部有两路视频流，一路1080P 30FPS本地H.264压缩，存储在设备内，另一路输</strong><br>\n<strong>出1080P 60FPS格式的HDMI信号，用于无线图传，支持PWM和串口控制。</strong></li>\n<li><strong>云台可以自动对焦，对焦时间小于1s，焦距范围为 F = 4.9 ~ 49mm，105dB宽动态范</strong><br>\n<strong>围，温度工作范围-10 至 55 摄氏度。</strong></li>\n</ol>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220114-e0917cd4.png\" width=\"100\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220112-4b9a80b5.png\" width=\"100\">\n</center>\n<hr>\n<h2 id=\"算法框架介绍\">算法框架介绍</h2>\n<p><strong>云台跟踪联动算法分为两大部分，第一部分是基于深度学习的目标检测算法和目标跟踪算法，<br>\n第二部分是基于PID调节的云台控制算法和底盘控制算法。</strong></p>\n<p><strong>流程图如下：</strong></p>\n<div align=\"center\"><img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220112-76ab38e2.png\" width=\"40%\"></div>\n<p><strong>淡蓝色部分为检测跟踪算法的流程，深蓝色部分为云台和底盘联动控制算法部分</strong></p>\n<hr>\n<h2 id=\"云台ROS驱动介绍\">云台ROS驱动介绍</h2>\n<p><strong>云台ROS驱动，根据云台供应商给的串口指令集进行串口编码控制，在源码中serial_ros包主要用来<br>\n将云台的控制指令通过ROS进行转换为串口的指令下发给云台控制板。其中主要需要启动的ROS节点是<br>\nserial_node 和 moni。</strong></p>\n<p>启动方式为：<br>\n<code>$ rosrun serial_ros serial_node</code><br>\n<code>$ rosrun serial_ros moni</code></p>\n<p><strong>同时云台驱动中也包含了相机启动和KCF跟踪的fv_tracking包，主要启动的ROS节点为<br>\nweb_cam和tracker_kcf</strong>。</p>\n<p>启动方式为：<br>\n<code>$ rosrun fv_tracking web_cam</code><br>\n<code>$rosrun fv_tracking tracker_kcf</code></p>\n<p><strong>启动后，相机画面及跟踪窗口效果如下：</strong></p>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220116-67901f3c.png\" width=\"400\">\n</center>\n<hr>\n<h2 id=\"检测算法介绍\">检测算法介绍</h2>\n<p><strong>使用的目标检测算法为YOLOX，YOLOX的github<a href=\"https://github.com/Megvii-BaseDetection/YOLOX\" title=\"YOLOX\">链接</a></strong><br>\n<strong>将YOLOX的源码整合为ROS代码下，改为yolox_ros的ROS包</strong></p>\n<p>启动方式为：<br>\n<code>$roslaunch yolox_ros yolox_ros.launch</code><br>\n<strong>启动之后，yolox_ros节点将会订阅来自云台相机的原始图像数据，生成检测结果并展示在桌面上。</strong></p>\n<p><strong>yolo_ros</strong>包中发布的话题为：<strong>“/yolox/bounding_boxes”</strong> 和 <strong>&quot; /yolox/image_raw&quot;</strong>。<br>\n<strong>yolo_ros</strong>包中订阅的话题为：<strong>“/camera/rgb/image_raw”</strong>。</p>\n<p><strong>启动后，检测算法窗口效果如下：</strong></p>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220116-76d60e91.png\" width=\"400\">\n</center>\n<hr>\n<h2 id=\"跟踪算法介绍\">跟踪算法介绍</h2>\n<p><strong>使用的目标跟踪算法SiamFC++，SiamFC++的github<a href=\"https://github.com/MegviiDetection/video_analyst\" title=\"SiamFC++\">链接</a></strong>。<br>\n<strong>基于SiamFC++的源码整合为ROS代码下，开发了siamfc_ros的ROS包</strong>。</p>\n<p>启动方式为：<br>\n<code>$roslaunch siamfc_ros siamfc_ros.launch</code><br>\n<strong>启动之后，siamfc_ros节点将会订阅来自云台相机的原始图像数据，以及来自yolox_ros节</strong><br>\n<strong>点的检测到的物体选框，物体选框当前为自动选则模式，选择被检测到的第一个物体，下一步迭代</strong><br>\n<strong>会添加，优化一个ID选择器，让用户可以选择当前画面中被检测到物体的ID，然后输入到跟踪</strong><br>\n<strong>器中，会生成跟踪结果并展示在桌面上。</strong></p>\n<p><strong>siamfc_ros</strong>包中发布的话题为：<strong>“/siamfc/image_raw”</strong> 和 <strong>“/siamfc/bounding_box”</strong>。<br>\n<strong>siamfc_ros</strong>包中订阅的话题为：<strong>&quot;/yolox/bounding_boxes &quot;</strong> 和 <strong>&quot;/camera/rgb/image_raw &quot;</strong>。</p>\n<p><strong>启动后，跟踪算法窗口效果如下：</strong></p>\n<center class=\"half\">\n    <img src=\"/2023/03/20/Gimbal-Camera-Trachking/003/20220116-3b5f0f20.png\" width=\"400\">\n</center>\n---\n<h2 id=\"联动控制算法介绍\">联动控制算法介绍</h2>\n<p><strong>联动控制算法主要流程是依赖云台的PID调节控制输出的yaw值和底盘的yaw值进行关联，当</strong><br>\n<strong>云台跟随目标进行转动的时候，将云台的yaw值实时发出到ROS话题</strong>&quot;/gimbal_camera/rpy_data&quot;<strong>上，然后底盘根据云台的</strong><br>\n<strong>yaw值进行处理，转换为底盘的航向角。</strong></p>\n<h3 id=\"云台控制算法\">云台控制算法</h3>\n<p><strong>云台控制算法采用的PID进行调节，PID控制的error主要是根据图像中心十字靶心像素坐标和被跟踪物体的boundingbox的物体中心的像素坐标的误差。</strong></p>\n<h3 id=\"底盘控制算法\">底盘控制算法</h3>\n<p><strong>底盘的控制算法，其中一部分基于视觉的大致观测距离判断进行线速度的控制，另一部分基于云台的yaw值<br>\n实时对角速度进行控制。</strong></p>\n<hr>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2023-03-20T11:34:03.966Z","updated":"2022-08-21T02:57:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clu8483se0003u9rl6lpsaiwc","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\">Quick Start</h2>\n<h3 id=\"Create-a-new-post\">Create a new post</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\">Run server</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\">Generate static files</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\">Deploy to remote sites</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover":"https://www.synotech.top:5523/wallpaper/5f363d07a24be_270_185.jpg","cover_type":"img","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\">Quick Start</h2>\n<h3 id=\"Create-a-new-post\">Create a new post</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\">Run server</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\">Generate static files</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\">Deploy to remote sites</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"MachineLeaning-ChapterOne","date":"2022-08-22T14:27:42.000Z","updated":"2022-08-22T14:27:42.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/robotics-arm.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":true,"_content":"\n## ***MachineLearning笔记整合***\n\n本笔记主要记录在学习A First Course in Machine Learning这本书时候，一些重要的知识点，形成知识汇总。\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n### 大纲：\n1. [概述](#概述 \"概述\")\n2. [线性建模](#线性建模 \"线性建模\")\n    2.1. [什么是线性模型](#什么是线性模型 \"什么是线性模型\")\n    2.2. [什么是损失函数](#什么是损失函数 \"什么是损失函数\")\n    2.3. [对损失函数求偏导](#对损失函数求偏导 \"对损失函数求偏导\")\n    2.4. [二阶导数的意义图解](#二阶导数的意义图解 \"二阶导数的意义图解\")\n    2.5. [$w_0$和$w_1$的二阶导数](#$w_0$和$w_1$的二阶导数 \"$w_0$和$w_1$的二阶导数\")\n3. [预测](#预测 \"预测\")\n4. [向量/矩阵](#向量/矩阵 \"向量/矩阵\")\n    4.1 [矩阵的乘法](#矩阵的乘法 \"矩阵的乘法\")\n5. [例子](#例子 \"例子\")\n\t5.1. [数值的例子](#数值的例子 \"数值的例子\")\n6. [线性模型的非线性响应](#线性模型的非线性响应 \"线性模型的非线性响应\")\n7. [泛化和过拟合](#泛化和过拟合 \"泛化和过拟合\")\n\t7.1. [验证数据](#验证数据 \"验证数据\")\n\t7.1. [交叉验证](#交叉验证 \"交叉验证\")\n8. [正则化最小二乘法](#正则化最小二乘法 \"正则化最小二乘法\")\n9. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n10. [引用文献](#引用文献 \"引用文献\")\n## 概述\n第一章：线性建模：最小二乘法\n\n### 线性建模\n\n#### 什么是线性模型\n定义的模型假设是线性关系 ，好的模型的定义是：一条尽可能与所有数据点接近的直线。\n衡量一个特定模型与数据点接近程度的普遍方法是用平方差，预测值和本来值的平方差值。\n![20220404-c90356a3](https://www.synotech.top:5523/uploads/2023/04/29/202304291418734.png)\n\n#### 什么是损失函数\n这个表达式称为平方损失函数，**squard loss function**。\n\n![20220404-06906ed3](https://www.synotech.top:5523/uploads/2023/04/29/202304291419640.png)\n\n![20220405-998602da](https://www.synotech.top:5523/uploads/2023/04/29/202304291419924.png)\n每年的Loss，用平均损失值的方式如上图，求解。\n\n![20220405-4b653a5e](https://www.synotech.top:5523/uploads/2023/04/29/202304291419942.png)\n最后我们再用如上图，求解argmin，\"找到最小化参数\"。平方损失的最小化是函数估计的最小二乘法\n的基础。\n同时，也有类似绝对损失函数，也可以用来回归，如下：\n![20220405-eab99959](https://www.synotech.top:5523/uploads/2023/04/29/202304291420987.png)\n\n#### 对损失函数求偏导\n\n对损失函数求导数，先对 ${w^1}$ ${w^2}$ 分别求偏导数，以及二阶导数\n![20220405-41245f4f](https://www.synotech.top:5523/uploads/2023/04/29/202304291420452.png)\n$w_1$的偏导数如下：\n![20220405-9b2895c7](https://www.synotech.top:5523/uploads/2023/04/29/202304291421776.png)\n$w_0$的偏导数如下：\n![20220405-7e9d0b45](https://www.synotech.top:5523/uploads/2023/04/29/202304291421103.png)\n\n#### 二阶导数的意义图解\n\n概念：函数梯度 $$f(x)=\\frac{P(x)}{Q(x)}$$ 一阶导数为0的点（拐点），但是无法确定是最大还是最小值，\n所以还需要求二阶导数。一般的，一个函数可能有多个拐点，如果二阶导数是正的常量，那么这个函数只有一个最小值点。\n\n图解如下：\n![20220405-dff73fa3](https://www.synotech.top:5523/uploads/2023/04/29/202304291421101.png)\n\n#### $w_0$和$w_1$的二阶导数\n\n导数等于0，对 $w_0$ 进行求偏导\n![20220405-a6a42af7](https://www.synotech.top:5523/uploads/2023/04/29/202304291422108.png)\n\n用这个 $w_0$的表达式，待入到刚才对$w_1$求的偏导数式子中，\n![20220405-c075dee1](https://www.synotech.top:5523/uploads/2023/04/29/202304291422420.png)\n\n再定义几个平均量，如下替换后：\n![20220405-e2ff73cf](https://www.synotech.top:5523/uploads/2023/04/29/202304291422999.png)\n\n书中，有完整的一个奥运会数据的最小二乘拟合的例子，根据刚才总结出来的公式，求解 $w_0$和 $w_1$，\n最后，就能得到拟合最好的线性函数。\n\n\n\n\n### 预测\n根据上面，用线性模型拟合一个小数据集产生的结果模型预测未来的结果，还是非常具有局限性，所以需要更加复杂的模型。\n\n如图，生成的男子和女子的奥运会模型，当时间递增的时候，获胜时间，也就是速度越来越快，直到获胜时间为0，那么速度将是无限大，显然，没有这种可能，说明模型的预测是不合理的。\n\n![20220405-34745253](https://www.synotech.top:5523/uploads/2023/04/29/202304291423139.png)\n\n\n### 向量/矩阵\n\n字母x表示标量，大写X表示矩阵，小写表示向量，$\\overline x$表示的是向量x。\n\n#### 矩阵的乘法\n矩阵乘法的计算方式如下：\n![20220405-a5b9d88a](https://www.synotech.top:5523/uploads/2023/04/29/202304291423499.png)\n\n损失函数的矩阵形式如下：\n![20220405-7d662ac4](https://www.synotech.top:5523/uploads/2023/04/29/202304291423152.png) \n\n乘积的转置\n![20220405-f1a06055](https://www.synotech.top:5523/uploads/2023/04/29/202304291423972.png)\n\n乘积转置的化简\n![20220405-4225951f](https://www.synotech.top:5523/uploads/2023/04/29/202304291423133.png)\n\n形成 $w_0 w_1$的均值和微分的简洁表示：\n![20220405-09163d0e](https://www.synotech.top:5523/uploads/2023/04/29/202304291424504.png)\n\n求得恒等式等于0的表达式：\n![20220405-f9890acf](https://www.synotech.top:5523/uploads/2023/04/29/202304291424648.png)\n\n$\\hat w$的矩阵公式：\n![20220405-cd1519fb](https://www.synotech.top:5523/uploads/2023/04/29/202304291424989.png)例子\n\n二维空间的表达，矩阵公式和之前得到的标量公式一致。\n![20220405-48750e2d](https://www.synotech.top:5523/uploads/2023/04/29/202304291425717.png)\n\n重写如下：\n![20220405-ed4f3265](https://www.synotech.top:5523/uploads/2023/04/29/202304291425970.png)\n\n计算出，$\\hat w$的矩阵表达：\n![20220405-83dab511](https://www.synotech.top:5523/uploads/2023/04/29/202304291425421.png) \n\n$\\hat w_1$如下：\n![20220405-457455e9](https://www.synotech.top:5523/uploads/2023/04/29/202304291425783.png)\n\n$\\hat w_0$\n![20220405-f843c893](https://www.synotech.top:5523/uploads/2023/04/29/202304291426193.png)\n\n#### 数值的例子\n根据刚才的案例数据，写成矩阵的形式，求解出来的结果和之前一致。\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291426002.png\" alt=\"20220405-a067c6cf\" style=\"zoom:80%;\" />\n\n\n### 线性模型的非线性响应\n可以根据从参数$w$和$x$的线性关系，增加列$x_n^2$，扩展数据矩阵$X$, 如下：\n![20220405-af372d35](https://www.synotech.top:5523/uploads/2023/04/29/202304291426434.png)\n\n同时，加入一个参数$w$,\n$w = \\begin{bmatrix} w_0\\\\ w_1\\\\ w_2 \\end{bmatrix}$。\n\n结果如下：\n![20220405-410a67fb](https://www.synotech.top:5523/uploads/2023/04/29/202304291427392.png)\n\n添加多个$x$项，形成多项式，能够表达更复杂的函数。\n![20220405-40ed843a](https://www.synotech.top:5523/uploads/2023/04/29/202304291427924.png) \n但是，无法预测合理的结果，尤其再观测数据范围之外的。（这里其实就是因为对样本数据已经过拟合了)\n\n###泛化和过拟合\n\n#### 验证数据\n验证集用来验证模型的预测性能，数据可以单独提供或者从原始训练数据集种抽出一部分。\n训练损失随着多项式阶（模型复杂度）的增加单调递减，然后，验证损失随着阶增加递增。如图所示：\n![20220405-dfeec3f1](https://www.synotech.top:5523/uploads/2023/04/29/202304291427081.png) \n\n泛化能力图解：\n![20220405-0abae8b4](https://www.synotech.top:5523/uploads/2023/04/29/202304291428710.png)\n\n#### 交叉验证\n因为验证集计算的损失对于验证集数据的选择敏感，所以使用交叉验证的方式，\n\n将数据集分成K等粉数据，K-1份作为训练集，每份单独作为验证集，轮流做验证，最后得出K\n个损失值的平均值为最后的损失值。K=N 也就是，每一个观测数据依次作为测试N-1个对象训练\n得到的模型，这种特殊情况为留一交叉验证，LOOCV。\n\n\n因为进行样本次数的验证方式需要耗费大量的时间，所以一般会减少K值，缩放到一个小值。\n\n\n### 正则化最小二乘法\n\n正则化也能实现避免模型过拟合的功能，在5阶多项式中，w的绝对值的总和越大，则模型的越复杂，我们用 $\\sum_{r=1}^n$  $w_i^2$来定义模型的复杂度。\n\n![20220405-e582528a](https://www.synotech.top:5523/uploads/2023/04/29/202304291428689.png) \n之前的Loss函数，夹上一个复杂度惩罚项，再求偏导数，化简得到正则化最小二乘法。\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428739.png\" alt=\"20220405-4a0db26b\" style=\"zoom:80%;\" /> \n如下图，当 $\\lambda$=0的时候，5阶多项式可以精确地表达6个合成数据点。\n![20220405-040bd4c6](https://www.synotech.top:5523/uploads/2023/04/29/202304291428169.png)\n\n$\\lambda$值的选择对于过拟合和泛化的折中是一样的，如果值太小，函数就可能很复杂，值\n太大，又不利于逼近数据。我们可以使用交叉验证的方式选择最好预测性能的 $\\lambda$值。\n\n***\n### ***后续章节内容预告***：\n\n***\n### 引用文献：\n","source":"_posts/001-MachineLearning/MachineLeaning-ChapterOne.md","raw":"---\ntitle: MachineLeaning-ChapterOne\ndate: 2022-08-22 22:27:42\nupdated: 2022-08-22 22:27:42\ntags: A First Course in ML\ncategories: MachineLearning\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax: true\n---\n\n## ***MachineLearning笔记整合***\n\n本笔记主要记录在学习A First Course in Machine Learning这本书时候，一些重要的知识点，形成知识汇总。\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n### 大纲：\n1. [概述](#概述 \"概述\")\n2. [线性建模](#线性建模 \"线性建模\")\n    2.1. [什么是线性模型](#什么是线性模型 \"什么是线性模型\")\n    2.2. [什么是损失函数](#什么是损失函数 \"什么是损失函数\")\n    2.3. [对损失函数求偏导](#对损失函数求偏导 \"对损失函数求偏导\")\n    2.4. [二阶导数的意义图解](#二阶导数的意义图解 \"二阶导数的意义图解\")\n    2.5. [$w_0$和$w_1$的二阶导数](#$w_0$和$w_1$的二阶导数 \"$w_0$和$w_1$的二阶导数\")\n3. [预测](#预测 \"预测\")\n4. [向量/矩阵](#向量/矩阵 \"向量/矩阵\")\n    4.1 [矩阵的乘法](#矩阵的乘法 \"矩阵的乘法\")\n5. [例子](#例子 \"例子\")\n\t5.1. [数值的例子](#数值的例子 \"数值的例子\")\n6. [线性模型的非线性响应](#线性模型的非线性响应 \"线性模型的非线性响应\")\n7. [泛化和过拟合](#泛化和过拟合 \"泛化和过拟合\")\n\t7.1. [验证数据](#验证数据 \"验证数据\")\n\t7.1. [交叉验证](#交叉验证 \"交叉验证\")\n8. [正则化最小二乘法](#正则化最小二乘法 \"正则化最小二乘法\")\n9. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n10. [引用文献](#引用文献 \"引用文献\")\n## 概述\n第一章：线性建模：最小二乘法\n\n### 线性建模\n\n#### 什么是线性模型\n定义的模型假设是线性关系 ，好的模型的定义是：一条尽可能与所有数据点接近的直线。\n衡量一个特定模型与数据点接近程度的普遍方法是用平方差，预测值和本来值的平方差值。\n![20220404-c90356a3](https://www.synotech.top:5523/uploads/2023/04/29/202304291418734.png)\n\n#### 什么是损失函数\n这个表达式称为平方损失函数，**squard loss function**。\n\n![20220404-06906ed3](https://www.synotech.top:5523/uploads/2023/04/29/202304291419640.png)\n\n![20220405-998602da](https://www.synotech.top:5523/uploads/2023/04/29/202304291419924.png)\n每年的Loss，用平均损失值的方式如上图，求解。\n\n![20220405-4b653a5e](https://www.synotech.top:5523/uploads/2023/04/29/202304291419942.png)\n最后我们再用如上图，求解argmin，\"找到最小化参数\"。平方损失的最小化是函数估计的最小二乘法\n的基础。\n同时，也有类似绝对损失函数，也可以用来回归，如下：\n![20220405-eab99959](https://www.synotech.top:5523/uploads/2023/04/29/202304291420987.png)\n\n#### 对损失函数求偏导\n\n对损失函数求导数，先对 ${w^1}$ ${w^2}$ 分别求偏导数，以及二阶导数\n![20220405-41245f4f](https://www.synotech.top:5523/uploads/2023/04/29/202304291420452.png)\n$w_1$的偏导数如下：\n![20220405-9b2895c7](https://www.synotech.top:5523/uploads/2023/04/29/202304291421776.png)\n$w_0$的偏导数如下：\n![20220405-7e9d0b45](https://www.synotech.top:5523/uploads/2023/04/29/202304291421103.png)\n\n#### 二阶导数的意义图解\n\n概念：函数梯度 $$f(x)=\\frac{P(x)}{Q(x)}$$ 一阶导数为0的点（拐点），但是无法确定是最大还是最小值，\n所以还需要求二阶导数。一般的，一个函数可能有多个拐点，如果二阶导数是正的常量，那么这个函数只有一个最小值点。\n\n图解如下：\n![20220405-dff73fa3](https://www.synotech.top:5523/uploads/2023/04/29/202304291421101.png)\n\n#### $w_0$和$w_1$的二阶导数\n\n导数等于0，对 $w_0$ 进行求偏导\n![20220405-a6a42af7](https://www.synotech.top:5523/uploads/2023/04/29/202304291422108.png)\n\n用这个 $w_0$的表达式，待入到刚才对$w_1$求的偏导数式子中，\n![20220405-c075dee1](https://www.synotech.top:5523/uploads/2023/04/29/202304291422420.png)\n\n再定义几个平均量，如下替换后：\n![20220405-e2ff73cf](https://www.synotech.top:5523/uploads/2023/04/29/202304291422999.png)\n\n书中，有完整的一个奥运会数据的最小二乘拟合的例子，根据刚才总结出来的公式，求解 $w_0$和 $w_1$，\n最后，就能得到拟合最好的线性函数。\n\n\n\n\n### 预测\n根据上面，用线性模型拟合一个小数据集产生的结果模型预测未来的结果，还是非常具有局限性，所以需要更加复杂的模型。\n\n如图，生成的男子和女子的奥运会模型，当时间递增的时候，获胜时间，也就是速度越来越快，直到获胜时间为0，那么速度将是无限大，显然，没有这种可能，说明模型的预测是不合理的。\n\n![20220405-34745253](https://www.synotech.top:5523/uploads/2023/04/29/202304291423139.png)\n\n\n### 向量/矩阵\n\n字母x表示标量，大写X表示矩阵，小写表示向量，$\\overline x$表示的是向量x。\n\n#### 矩阵的乘法\n矩阵乘法的计算方式如下：\n![20220405-a5b9d88a](https://www.synotech.top:5523/uploads/2023/04/29/202304291423499.png)\n\n损失函数的矩阵形式如下：\n![20220405-7d662ac4](https://www.synotech.top:5523/uploads/2023/04/29/202304291423152.png) \n\n乘积的转置\n![20220405-f1a06055](https://www.synotech.top:5523/uploads/2023/04/29/202304291423972.png)\n\n乘积转置的化简\n![20220405-4225951f](https://www.synotech.top:5523/uploads/2023/04/29/202304291423133.png)\n\n形成 $w_0 w_1$的均值和微分的简洁表示：\n![20220405-09163d0e](https://www.synotech.top:5523/uploads/2023/04/29/202304291424504.png)\n\n求得恒等式等于0的表达式：\n![20220405-f9890acf](https://www.synotech.top:5523/uploads/2023/04/29/202304291424648.png)\n\n$\\hat w$的矩阵公式：\n![20220405-cd1519fb](https://www.synotech.top:5523/uploads/2023/04/29/202304291424989.png)例子\n\n二维空间的表达，矩阵公式和之前得到的标量公式一致。\n![20220405-48750e2d](https://www.synotech.top:5523/uploads/2023/04/29/202304291425717.png)\n\n重写如下：\n![20220405-ed4f3265](https://www.synotech.top:5523/uploads/2023/04/29/202304291425970.png)\n\n计算出，$\\hat w$的矩阵表达：\n![20220405-83dab511](https://www.synotech.top:5523/uploads/2023/04/29/202304291425421.png) \n\n$\\hat w_1$如下：\n![20220405-457455e9](https://www.synotech.top:5523/uploads/2023/04/29/202304291425783.png)\n\n$\\hat w_0$\n![20220405-f843c893](https://www.synotech.top:5523/uploads/2023/04/29/202304291426193.png)\n\n#### 数值的例子\n根据刚才的案例数据，写成矩阵的形式，求解出来的结果和之前一致。\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291426002.png\" alt=\"20220405-a067c6cf\" style=\"zoom:80%;\" />\n\n\n### 线性模型的非线性响应\n可以根据从参数$w$和$x$的线性关系，增加列$x_n^2$，扩展数据矩阵$X$, 如下：\n![20220405-af372d35](https://www.synotech.top:5523/uploads/2023/04/29/202304291426434.png)\n\n同时，加入一个参数$w$,\n$w = \\begin{bmatrix} w_0\\\\ w_1\\\\ w_2 \\end{bmatrix}$。\n\n结果如下：\n![20220405-410a67fb](https://www.synotech.top:5523/uploads/2023/04/29/202304291427392.png)\n\n添加多个$x$项，形成多项式，能够表达更复杂的函数。\n![20220405-40ed843a](https://www.synotech.top:5523/uploads/2023/04/29/202304291427924.png) \n但是，无法预测合理的结果，尤其再观测数据范围之外的。（这里其实就是因为对样本数据已经过拟合了)\n\n###泛化和过拟合\n\n#### 验证数据\n验证集用来验证模型的预测性能，数据可以单独提供或者从原始训练数据集种抽出一部分。\n训练损失随着多项式阶（模型复杂度）的增加单调递减，然后，验证损失随着阶增加递增。如图所示：\n![20220405-dfeec3f1](https://www.synotech.top:5523/uploads/2023/04/29/202304291427081.png) \n\n泛化能力图解：\n![20220405-0abae8b4](https://www.synotech.top:5523/uploads/2023/04/29/202304291428710.png)\n\n#### 交叉验证\n因为验证集计算的损失对于验证集数据的选择敏感，所以使用交叉验证的方式，\n\n将数据集分成K等粉数据，K-1份作为训练集，每份单独作为验证集，轮流做验证，最后得出K\n个损失值的平均值为最后的损失值。K=N 也就是，每一个观测数据依次作为测试N-1个对象训练\n得到的模型，这种特殊情况为留一交叉验证，LOOCV。\n\n\n因为进行样本次数的验证方式需要耗费大量的时间，所以一般会减少K值，缩放到一个小值。\n\n\n### 正则化最小二乘法\n\n正则化也能实现避免模型过拟合的功能，在5阶多项式中，w的绝对值的总和越大，则模型的越复杂，我们用 $\\sum_{r=1}^n$  $w_i^2$来定义模型的复杂度。\n\n![20220405-e582528a](https://www.synotech.top:5523/uploads/2023/04/29/202304291428689.png) \n之前的Loss函数，夹上一个复杂度惩罚项，再求偏导数，化简得到正则化最小二乘法。\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428739.png\" alt=\"20220405-4a0db26b\" style=\"zoom:80%;\" /> \n如下图，当 $\\lambda$=0的时候，5阶多项式可以精确地表达6个合成数据点。\n![20220405-040bd4c6](https://www.synotech.top:5523/uploads/2023/04/29/202304291428169.png)\n\n$\\lambda$值的选择对于过拟合和泛化的折中是一样的，如果值太小，函数就可能很复杂，值\n太大，又不利于逼近数据。我们可以使用交叉验证的方式选择最好预测性能的 $\\lambda$值。\n\n***\n### ***后续章节内容预告***：\n\n***\n### 引用文献：\n","slug":"001-MachineLearning/MachineLeaning-ChapterOne","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483si0006u9rlh9bia7u4","content":"<h2 id=\"MachineLearning笔记整合\"><em><strong>MachineLearning笔记整合</strong></em></h2>\n<p>本笔记主要记录在学习A First Course in Machine Learning这本书时候，一些重要的知识点，形成知识汇总。</p>\n<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h3 id=\"大纲：\">大纲：</h3>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E5%BB%BA%E6%A8%A1\" title=\"线性建模\">线性建模</a><br>\n2.1. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\" title=\"什么是线性模型\">什么是线性模型</a><br>\n2.2. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\" title=\"什么是损失函数\">什么是损失函数</a><br>\n2.3. <a href=\"#%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC\" title=\"对损失函数求偏导\">对损失函数求偏导</a><br>\n2.4. <a href=\"#%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%9B%BE%E8%A7%A3\" title=\"二阶导数的意义图解\">二阶导数的意义图解</a><br>\n2.5. <a href=\"#$w_0$%E5%92%8C$w_1$%E7%9A%84%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0\" title=\"$w_0$和$w_1$的二阶导数\">$w_0$和$w_1$的二阶导数</a></li>\n<li><a href=\"#%E9%A2%84%E6%B5%8B\" title=\"预测\">预测</a></li>\n<li><a href=\"#%E5%90%91%E9%87%8F/%E7%9F%A9%E9%98%B5\" title=\"向量/矩阵\">向量/矩阵</a><br>\n4.1 <a href=\"#%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B9%98%E6%B3%95\" title=\"矩阵的乘法\">矩阵的乘法</a></li>\n<li><a href=\"#%E4%BE%8B%E5%AD%90\" title=\"例子\">例子</a><br>\n5.1. <a href=\"#%E6%95%B0%E5%80%BC%E7%9A%84%E4%BE%8B%E5%AD%90\" title=\"数值的例子\">数值的例子</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%93%8D%E5%BA%94\" title=\"线性模型的非线性响应\">线性模型的非线性响应</a></li>\n<li><a href=\"#%E6%B3%9B%E5%8C%96%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88\" title=\"泛化和过拟合\">泛化和过拟合</a><br>\n7.1. <a href=\"#%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE\" title=\"验证数据\">验证数据</a><br>\n7.1. <a href=\"#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81\" title=\"交叉验证\">交叉验证</a></li>\n<li><a href=\"#%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95\" title=\"正则化最小二乘法\">正则化最小二乘法</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a></li>\n</ol>\n<h2 id=\"概述\">概述</h2>\n<p>第一章：线性建模：最小二乘法</p>\n<h3 id=\"线性建模\">线性建模</h3>\n<h4 id=\"什么是线性模型\">什么是线性模型</h4>\n<p>定义的模型假设是线性关系 ，好的模型的定义是：一条尽可能与所有数据点接近的直线。<br>\n衡量一个特定模型与数据点接近程度的普遍方法是用平方差，预测值和本来值的平方差值。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291418734.png\" alt=\"20220404-c90356a3\"></p>\n<h4 id=\"什么是损失函数\">什么是损失函数</h4>\n<p>这个表达式称为平方损失函数，<strong>squard loss function</strong>。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291419640.png\" alt=\"20220404-06906ed3\"></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291419924.png\" alt=\"20220405-998602da\"><br>\n每年的Loss，用平均损失值的方式如上图，求解。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291419942.png\" alt=\"20220405-4b653a5e\"><br>\n最后我们再用如上图，求解argmin，“找到最小化参数”。平方损失的最小化是函数估计的最小二乘法<br>\n的基础。<br>\n同时，也有类似绝对损失函数，也可以用来回归，如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291420987.png\" alt=\"20220405-eab99959\"></p>\n<h4 id=\"对损失函数求偏导\">对损失函数求偏导</h4>\n<p>对损失函数求导数，先对 ${w^1}$ ${w^2}$ 分别求偏导数，以及二阶导数<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291420452.png\" alt=\"20220405-41245f4f\"><br>\n$w_1$的偏导数如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291421776.png\" alt=\"20220405-9b2895c7\"><br>\n$w_0$的偏导数如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291421103.png\" alt=\"20220405-7e9d0b45\"></p>\n<h4 id=\"二阶导数的意义图解\">二阶导数的意义图解</h4>\n<p>概念：函数梯度 $$f(x)=\\frac{P(x)}{Q(x)}$$ 一阶导数为0的点（拐点），但是无法确定是最大还是最小值，<br>\n所以还需要求二阶导数。一般的，一个函数可能有多个拐点，如果二阶导数是正的常量，那么这个函数只有一个最小值点。</p>\n<p>图解如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291421101.png\" alt=\"20220405-dff73fa3\"></p>\n<h4 id=\"w-0-和-w-1-的二阶导数\">$w_0$和$w_1$的二阶导数</h4>\n<p>导数等于0，对 $w_0$ 进行求偏导<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291422108.png\" alt=\"20220405-a6a42af7\"></p>\n<p>用这个 $w_0$的表达式，待入到刚才对$w_1$求的偏导数式子中，<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291422420.png\" alt=\"20220405-c075dee1\"></p>\n<p>再定义几个平均量，如下替换后：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291422999.png\" alt=\"20220405-e2ff73cf\"></p>\n<p>书中，有完整的一个奥运会数据的最小二乘拟合的例子，根据刚才总结出来的公式，求解 $w_0$和 $w_1$，<br>\n最后，就能得到拟合最好的线性函数。</p>\n<h3 id=\"预测\">预测</h3>\n<p>根据上面，用线性模型拟合一个小数据集产生的结果模型预测未来的结果，还是非常具有局限性，所以需要更加复杂的模型。</p>\n<p>如图，生成的男子和女子的奥运会模型，当时间递增的时候，获胜时间，也就是速度越来越快，直到获胜时间为0，那么速度将是无限大，显然，没有这种可能，说明模型的预测是不合理的。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423139.png\" alt=\"20220405-34745253\"></p>\n<h3 id=\"向量-矩阵\">向量/矩阵</h3>\n<p>字母x表示标量，大写X表示矩阵，小写表示向量，$\\overline x$表示的是向量x。</p>\n<h4 id=\"矩阵的乘法\">矩阵的乘法</h4>\n<p>矩阵乘法的计算方式如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423499.png\" alt=\"20220405-a5b9d88a\"></p>\n<p>损失函数的矩阵形式如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423152.png\" alt=\"20220405-7d662ac4\"></p>\n<p>乘积的转置<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423972.png\" alt=\"20220405-f1a06055\"></p>\n<p>乘积转置的化简<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423133.png\" alt=\"20220405-4225951f\"></p>\n<p>形成 $w_0 w_1$的均值和微分的简洁表示：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291424504.png\" alt=\"20220405-09163d0e\"></p>\n<p>求得恒等式等于0的表达式：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291424648.png\" alt=\"20220405-f9890acf\"></p>\n<p>$\\hat w$的矩阵公式：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291424989.png\" alt=\"20220405-cd1519fb\">例子</p>\n<p>二维空间的表达，矩阵公式和之前得到的标量公式一致。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291425717.png\" alt=\"20220405-48750e2d\"></p>\n<p>重写如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291425970.png\" alt=\"20220405-ed4f3265\"></p>\n<p>计算出，$\\hat w$的矩阵表达：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291425421.png\" alt=\"20220405-83dab511\"></p>\n<p>$\\hat w_1$如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291425783.png\" alt=\"20220405-457455e9\"></p>\n<p>$\\hat w_0$<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291426193.png\" alt=\"20220405-f843c893\"></p>\n<h4 id=\"数值的例子\">数值的例子</h4>\n<p>根据刚才的案例数据，写成矩阵的形式，求解出来的结果和之前一致。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291426002.png\" alt=\"20220405-a067c6cf\" style=\"zoom:80%;\"></p>\n<h3 id=\"线性模型的非线性响应\">线性模型的非线性响应</h3>\n<p>可以根据从参数$w$和$x$的线性关系，增加列$x_n^2$，扩展数据矩阵$X$, 如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291426434.png\" alt=\"20220405-af372d35\"></p>\n<p>同时，加入一个参数$w$,<br>\n$w = \\begin{bmatrix} w_0\\ w_1\\ w_2 \\end{bmatrix}$。</p>\n<p>结果如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291427392.png\" alt=\"20220405-410a67fb\"></p>\n<p>添加多个$x$项，形成多项式，能够表达更复杂的函数。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291427924.png\" alt=\"20220405-40ed843a\"><br>\n但是，无法预测合理的结果，尤其再观测数据范围之外的。（这里其实就是因为对样本数据已经过拟合了)</p>\n<p>###泛化和过拟合</p>\n<h4 id=\"验证数据\">验证数据</h4>\n<p>验证集用来验证模型的预测性能，数据可以单独提供或者从原始训练数据集种抽出一部分。<br>\n训练损失随着多项式阶（模型复杂度）的增加单调递减，然后，验证损失随着阶增加递增。如图所示：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291427081.png\" alt=\"20220405-dfeec3f1\"></p>\n<p>泛化能力图解：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428710.png\" alt=\"20220405-0abae8b4\"></p>\n<h4 id=\"交叉验证\">交叉验证</h4>\n<p>因为验证集计算的损失对于验证集数据的选择敏感，所以使用交叉验证的方式，</p>\n<p>将数据集分成K等粉数据，K-1份作为训练集，每份单独作为验证集，轮流做验证，最后得出K<br>\n个损失值的平均值为最后的损失值。K=N 也就是，每一个观测数据依次作为测试N-1个对象训练<br>\n得到的模型，这种特殊情况为留一交叉验证，LOOCV。</p>\n<p>因为进行样本次数的验证方式需要耗费大量的时间，所以一般会减少K值，缩放到一个小值。</p>\n<h3 id=\"正则化最小二乘法\">正则化最小二乘法</h3>\n<p>正则化也能实现避免模型过拟合的功能，在5阶多项式中，w的绝对值的总和越大，则模型的越复杂，我们用 $\\sum_{r=1}^n$  $w_i^2$来定义模型的复杂度。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428689.png\" alt=\"20220405-e582528a\"><br>\n之前的Loss函数，夹上一个复杂度惩罚项，再求偏导数，化简得到正则化最小二乘法。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428739.png\" alt=\"20220405-4a0db26b\" style=\"zoom:80%;\"><br>\n如下图，当 $\\lambda$=0的时候，5阶多项式可以精确地表达6个合成数据点。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428169.png\" alt=\"20220405-040bd4c6\"></p>\n<p>$\\lambda$值的选择对于过拟合和泛化的折中是一样的，如果值太小，函数就可能很复杂，值<br>\n太大，又不利于逼近数据。我们可以使用交叉验证的方式选择最好预测性能的 $\\lambda$值。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h2 id=\"MachineLearning笔记整合\"><em><strong>MachineLearning笔记整合</strong></em></h2>\n<p>本笔记主要记录在学习A First Course in Machine Learning这本书时候，一些重要的知识点，形成知识汇总。</p>\n<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h3 id=\"大纲：\">大纲：</h3>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E5%BB%BA%E6%A8%A1\" title=\"线性建模\">线性建模</a><br>\n2.1. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\" title=\"什么是线性模型\">什么是线性模型</a><br>\n2.2. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\" title=\"什么是损失函数\">什么是损失函数</a><br>\n2.3. <a href=\"#%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC\" title=\"对损失函数求偏导\">对损失函数求偏导</a><br>\n2.4. <a href=\"#%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%9B%BE%E8%A7%A3\" title=\"二阶导数的意义图解\">二阶导数的意义图解</a><br>\n2.5. <a href=\"#$w_0$%E5%92%8C$w_1$%E7%9A%84%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0\" title=\"$w_0$和$w_1$的二阶导数\">$w_0$和$w_1$的二阶导数</a></li>\n<li><a href=\"#%E9%A2%84%E6%B5%8B\" title=\"预测\">预测</a></li>\n<li><a href=\"#%E5%90%91%E9%87%8F/%E7%9F%A9%E9%98%B5\" title=\"向量/矩阵\">向量/矩阵</a><br>\n4.1 <a href=\"#%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B9%98%E6%B3%95\" title=\"矩阵的乘法\">矩阵的乘法</a></li>\n<li><a href=\"#%E4%BE%8B%E5%AD%90\" title=\"例子\">例子</a><br>\n5.1. <a href=\"#%E6%95%B0%E5%80%BC%E7%9A%84%E4%BE%8B%E5%AD%90\" title=\"数值的例子\">数值的例子</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%93%8D%E5%BA%94\" title=\"线性模型的非线性响应\">线性模型的非线性响应</a></li>\n<li><a href=\"#%E6%B3%9B%E5%8C%96%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88\" title=\"泛化和过拟合\">泛化和过拟合</a><br>\n7.1. <a href=\"#%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE\" title=\"验证数据\">验证数据</a><br>\n7.1. <a href=\"#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81\" title=\"交叉验证\">交叉验证</a></li>\n<li><a href=\"#%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95\" title=\"正则化最小二乘法\">正则化最小二乘法</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a></li>\n</ol>\n<h2 id=\"概述\">概述</h2>\n<p>第一章：线性建模：最小二乘法</p>\n<h3 id=\"线性建模\">线性建模</h3>\n<h4 id=\"什么是线性模型\">什么是线性模型</h4>\n<p>定义的模型假设是线性关系 ，好的模型的定义是：一条尽可能与所有数据点接近的直线。<br>\n衡量一个特定模型与数据点接近程度的普遍方法是用平方差，预测值和本来值的平方差值。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291418734.png\" alt=\"20220404-c90356a3\"></p>\n<h4 id=\"什么是损失函数\">什么是损失函数</h4>\n<p>这个表达式称为平方损失函数，<strong>squard loss function</strong>。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291419640.png\" alt=\"20220404-06906ed3\"></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291419924.png\" alt=\"20220405-998602da\"><br>\n每年的Loss，用平均损失值的方式如上图，求解。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291419942.png\" alt=\"20220405-4b653a5e\"><br>\n最后我们再用如上图，求解argmin，“找到最小化参数”。平方损失的最小化是函数估计的最小二乘法<br>\n的基础。<br>\n同时，也有类似绝对损失函数，也可以用来回归，如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291420987.png\" alt=\"20220405-eab99959\"></p>\n<h4 id=\"对损失函数求偏导\">对损失函数求偏导</h4>\n<p>对损失函数求导数，先对 ${w^1}$ ${w^2}$ 分别求偏导数，以及二阶导数<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291420452.png\" alt=\"20220405-41245f4f\"><br>\n$w_1$的偏导数如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291421776.png\" alt=\"20220405-9b2895c7\"><br>\n$w_0$的偏导数如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291421103.png\" alt=\"20220405-7e9d0b45\"></p>\n<h4 id=\"二阶导数的意义图解\">二阶导数的意义图解</h4>\n<p>概念：函数梯度 $$f(x)=\\frac{P(x)}{Q(x)}$$ 一阶导数为0的点（拐点），但是无法确定是最大还是最小值，<br>\n所以还需要求二阶导数。一般的，一个函数可能有多个拐点，如果二阶导数是正的常量，那么这个函数只有一个最小值点。</p>\n<p>图解如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291421101.png\" alt=\"20220405-dff73fa3\"></p>\n<h4 id=\"w-0-和-w-1-的二阶导数\">$w_0$和$w_1$的二阶导数</h4>\n<p>导数等于0，对 $w_0$ 进行求偏导<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291422108.png\" alt=\"20220405-a6a42af7\"></p>\n<p>用这个 $w_0$的表达式，待入到刚才对$w_1$求的偏导数式子中，<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291422420.png\" alt=\"20220405-c075dee1\"></p>\n<p>再定义几个平均量，如下替换后：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291422999.png\" alt=\"20220405-e2ff73cf\"></p>\n<p>书中，有完整的一个奥运会数据的最小二乘拟合的例子，根据刚才总结出来的公式，求解 $w_0$和 $w_1$，<br>\n最后，就能得到拟合最好的线性函数。</p>\n<h3 id=\"预测\">预测</h3>\n<p>根据上面，用线性模型拟合一个小数据集产生的结果模型预测未来的结果，还是非常具有局限性，所以需要更加复杂的模型。</p>\n<p>如图，生成的男子和女子的奥运会模型，当时间递增的时候，获胜时间，也就是速度越来越快，直到获胜时间为0，那么速度将是无限大，显然，没有这种可能，说明模型的预测是不合理的。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423139.png\" alt=\"20220405-34745253\"></p>\n<h3 id=\"向量-矩阵\">向量/矩阵</h3>\n<p>字母x表示标量，大写X表示矩阵，小写表示向量，$\\overline x$表示的是向量x。</p>\n<h4 id=\"矩阵的乘法\">矩阵的乘法</h4>\n<p>矩阵乘法的计算方式如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423499.png\" alt=\"20220405-a5b9d88a\"></p>\n<p>损失函数的矩阵形式如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423152.png\" alt=\"20220405-7d662ac4\"></p>\n<p>乘积的转置<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423972.png\" alt=\"20220405-f1a06055\"></p>\n<p>乘积转置的化简<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291423133.png\" alt=\"20220405-4225951f\"></p>\n<p>形成 $w_0 w_1$的均值和微分的简洁表示：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291424504.png\" alt=\"20220405-09163d0e\"></p>\n<p>求得恒等式等于0的表达式：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291424648.png\" alt=\"20220405-f9890acf\"></p>\n<p>$\\hat w$的矩阵公式：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291424989.png\" alt=\"20220405-cd1519fb\">例子</p>\n<p>二维空间的表达，矩阵公式和之前得到的标量公式一致。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291425717.png\" alt=\"20220405-48750e2d\"></p>\n<p>重写如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291425970.png\" alt=\"20220405-ed4f3265\"></p>\n<p>计算出，$\\hat w$的矩阵表达：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291425421.png\" alt=\"20220405-83dab511\"></p>\n<p>$\\hat w_1$如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291425783.png\" alt=\"20220405-457455e9\"></p>\n<p>$\\hat w_0$<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291426193.png\" alt=\"20220405-f843c893\"></p>\n<h4 id=\"数值的例子\">数值的例子</h4>\n<p>根据刚才的案例数据，写成矩阵的形式，求解出来的结果和之前一致。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291426002.png\" alt=\"20220405-a067c6cf\" style=\"zoom:80%;\"></p>\n<h3 id=\"线性模型的非线性响应\">线性模型的非线性响应</h3>\n<p>可以根据从参数$w$和$x$的线性关系，增加列$x_n^2$，扩展数据矩阵$X$, 如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291426434.png\" alt=\"20220405-af372d35\"></p>\n<p>同时，加入一个参数$w$,<br>\n$w = \\begin{bmatrix} w_0\\ w_1\\ w_2 \\end{bmatrix}$。</p>\n<p>结果如下：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291427392.png\" alt=\"20220405-410a67fb\"></p>\n<p>添加多个$x$项，形成多项式，能够表达更复杂的函数。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291427924.png\" alt=\"20220405-40ed843a\"><br>\n但是，无法预测合理的结果，尤其再观测数据范围之外的。（这里其实就是因为对样本数据已经过拟合了)</p>\n<p>###泛化和过拟合</p>\n<h4 id=\"验证数据\">验证数据</h4>\n<p>验证集用来验证模型的预测性能，数据可以单独提供或者从原始训练数据集种抽出一部分。<br>\n训练损失随着多项式阶（模型复杂度）的增加单调递减，然后，验证损失随着阶增加递增。如图所示：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291427081.png\" alt=\"20220405-dfeec3f1\"></p>\n<p>泛化能力图解：<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428710.png\" alt=\"20220405-0abae8b4\"></p>\n<h4 id=\"交叉验证\">交叉验证</h4>\n<p>因为验证集计算的损失对于验证集数据的选择敏感，所以使用交叉验证的方式，</p>\n<p>将数据集分成K等粉数据，K-1份作为训练集，每份单独作为验证集，轮流做验证，最后得出K<br>\n个损失值的平均值为最后的损失值。K=N 也就是，每一个观测数据依次作为测试N-1个对象训练<br>\n得到的模型，这种特殊情况为留一交叉验证，LOOCV。</p>\n<p>因为进行样本次数的验证方式需要耗费大量的时间，所以一般会减少K值，缩放到一个小值。</p>\n<h3 id=\"正则化最小二乘法\">正则化最小二乘法</h3>\n<p>正则化也能实现避免模型过拟合的功能，在5阶多项式中，w的绝对值的总和越大，则模型的越复杂，我们用 $\\sum_{r=1}^n$  $w_i^2$来定义模型的复杂度。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428689.png\" alt=\"20220405-e582528a\"><br>\n之前的Loss函数，夹上一个复杂度惩罚项，再求偏导数，化简得到正则化最小二乘法。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428739.png\" alt=\"20220405-4a0db26b\" style=\"zoom:80%;\"><br>\n如下图，当 $\\lambda$=0的时候，5阶多项式可以精确地表达6个合成数据点。<br>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291428169.png\" alt=\"20220405-040bd4c6\"></p>\n<p>$\\lambda$值的选择对于过拟合和泛化的折中是一样的，如果值太小，函数就可能很复杂，值<br>\n太大，又不利于逼近数据。我们可以使用交叉验证的方式选择最好预测性能的 $\\lambda$值。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n"},{"title":"MachineLearning-Normalization","date":"2022-08-25T14:24:38.000Z","updated":"2022-08-25T14:24:38.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/5f19587122afa_270_185.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":true,"_content":"\n# ***MachineLearning-Normalization***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [线性建模](#线性建模 \"线性建模\")\n>     2.1. [什么是线性模型](#什么是线性模型 \"什么是线性模型\")\n>     2.2. [什么是损失函数](#什么是损失函数 \"什么是损失函数\")\n>     2.3. [对损失函数求偏导](#对损失函数求偏导 \"对损失函数求偏导\")\n>     2.4. [二阶导数的意义图解](#二阶导数的意义图解 \"二阶导数的意义图解\")\n>     2.5. [$w_0$和$w_1$的二阶导数](#$w_0$和$w_1$的二阶导数 \"$w_0$和$w_1$的二阶导数\")\n>3. [预测](#预测 \"预测\")\n>8. [正则化最小二乘法](#正则化最小二乘法 \"正则化最小二乘法\")\n> 9. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n>10. [引用文献](#引用文献 \"引用文献\")#下载图片接口 \"下载图片接口\")\n\n\n\n### 概述\n\n为什么需要Normalization？\n\n深度学习模型训练的难度是，CNN包含了很多不同的hiden layer， 每个layer都会随着训练改变，hiden layer的输入分布也是会一致改变，这就导致hiden layer面临covariate shift的问题。Internal covariate shift(ICS)使得每层输入不再是独立的分布，上一层数据需要适应新的输入分布，数据输入激活函数时，会落入饱和区，使得学习效率过低，甚至梯度消失。\n\nNormlization的思想主要是通过归一化的手段，将每层的输入强行拉回到均值为0，方差为1的标准正态分布，使得激活输入值分布在非线性函数梯度敏感的区域内，避免梯度消失，加快训练速度。\n\n\n\n比如将sigmoid函数，BN将输入值分布在-1～1的区间内，这个区间的梯度值大，可以避免梯度消失，提高收敛的速度。\n\n归一化后，激活输入值分布在[-1,1]内，这导致非线性程度降低，这样本来需要非线性函数进行learning高维度的表达的效果就降低了，所以BN就是为了保证非线性的表达能力，对变换后的输入$X$进行了平移。使用scale shift操作，$y=scale * x + shift$， 这两个参数通过训练获得。\n\n均值的计算和方差的计算\n\n\n\n参考https://blog.csdn.net/litt1e/article/details/105817224\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n","source":"_posts/001-MachineLearning/MachineLearning-Normalization.md","raw":"---\ntitle: MachineLearning-Normalization\ndate: 2022-08-25 22:24:38\nupdated: 2022-08-25 22:24:38\ntags: A First Course in ML\ncategories: MachineLearning\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax: true\n---\n\n# ***MachineLearning-Normalization***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [线性建模](#线性建模 \"线性建模\")\n>     2.1. [什么是线性模型](#什么是线性模型 \"什么是线性模型\")\n>     2.2. [什么是损失函数](#什么是损失函数 \"什么是损失函数\")\n>     2.3. [对损失函数求偏导](#对损失函数求偏导 \"对损失函数求偏导\")\n>     2.4. [二阶导数的意义图解](#二阶导数的意义图解 \"二阶导数的意义图解\")\n>     2.5. [$w_0$和$w_1$的二阶导数](#$w_0$和$w_1$的二阶导数 \"$w_0$和$w_1$的二阶导数\")\n>3. [预测](#预测 \"预测\")\n>8. [正则化最小二乘法](#正则化最小二乘法 \"正则化最小二乘法\")\n> 9. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n>10. [引用文献](#引用文献 \"引用文献\")#下载图片接口 \"下载图片接口\")\n\n\n\n### 概述\n\n为什么需要Normalization？\n\n深度学习模型训练的难度是，CNN包含了很多不同的hiden layer， 每个layer都会随着训练改变，hiden layer的输入分布也是会一致改变，这就导致hiden layer面临covariate shift的问题。Internal covariate shift(ICS)使得每层输入不再是独立的分布，上一层数据需要适应新的输入分布，数据输入激活函数时，会落入饱和区，使得学习效率过低，甚至梯度消失。\n\nNormlization的思想主要是通过归一化的手段，将每层的输入强行拉回到均值为0，方差为1的标准正态分布，使得激活输入值分布在非线性函数梯度敏感的区域内，避免梯度消失，加快训练速度。\n\n\n\n比如将sigmoid函数，BN将输入值分布在-1～1的区间内，这个区间的梯度值大，可以避免梯度消失，提高收敛的速度。\n\n归一化后，激活输入值分布在[-1,1]内，这导致非线性程度降低，这样本来需要非线性函数进行learning高维度的表达的效果就降低了，所以BN就是为了保证非线性的表达能力，对变换后的输入$X$进行了平移。使用scale shift操作，$y=scale * x + shift$， 这两个参数通过训练获得。\n\n均值的计算和方差的计算\n\n\n\n参考https://blog.csdn.net/litt1e/article/details/105817224\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n","slug":"001-MachineLearning/MachineLearning-Normalization","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483sj0008u9rlbdjo92la","content":"<h1><em><strong>MachineLearning-Normalization</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E5%BB%BA%E6%A8%A1\" title=\"线性建模\">线性建模</a><br>\n2.1. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\" title=\"什么是线性模型\">什么是线性模型</a><br>\n2.2. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\" title=\"什么是损失函数\">什么是损失函数</a><br>\n2.3. <a href=\"#%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC\" title=\"对损失函数求偏导\">对损失函数求偏导</a><br>\n2.4. <a href=\"#%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%9B%BE%E8%A7%A3\" title=\"二阶导数的意义图解\">二阶导数的意义图解</a><br>\n2.5. <a href=\"#$w_0$%E5%92%8C$w_1$%E7%9A%84%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0\" title=\"$w_0$和$w_1$的二阶导数\">$w_0$和$w_1$的二阶导数</a></li>\n<li><a href=\"#%E9%A2%84%E6%B5%8B\" title=\"预测\">预测</a></li>\n<li><a href=\"#%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95\" title=\"正则化最小二乘法\">正则化最小二乘法</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a>#下载图片接口 “下载图片接口”)</li>\n</ol>\n</blockquote>\n<h3 id=\"概述\">概述</h3>\n<p>为什么需要Normalization？</p>\n<p>深度学习模型训练的难度是，CNN包含了很多不同的hiden layer， 每个layer都会随着训练改变，hiden layer的输入分布也是会一致改变，这就导致hiden layer面临covariate shift的问题。Internal covariate shift(ICS)使得每层输入不再是独立的分布，上一层数据需要适应新的输入分布，数据输入激活函数时，会落入饱和区，使得学习效率过低，甚至梯度消失。</p>\n<p>Normlization的思想主要是通过归一化的手段，将每层的输入强行拉回到均值为0，方差为1的标准正态分布，使得激活输入值分布在非线性函数梯度敏感的区域内，避免梯度消失，加快训练速度。</p>\n<p>比如将sigmoid函数，BN将输入值分布在-1～1的区间内，这个区间的梯度值大，可以避免梯度消失，提高收敛的速度。</p>\n<p>归一化后，激活输入值分布在[-1,1]内，这导致非线性程度降低，这样本来需要非线性函数进行learning高维度的表达的效果就降低了，所以BN就是为了保证非线性的表达能力，对变换后的输入$X$进行了平移。使用scale shift操作，$y=scale * x + shift$， 这两个参数通过训练获得。</p>\n<p>均值的计算和方差的计算</p>\n<p>参考<a href=\"https://blog.csdn.net/litt1e/article/details/105817224\">https://blog.csdn.net/litt1e/article/details/105817224</a></p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>MachineLearning-Normalization</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E5%BB%BA%E6%A8%A1\" title=\"线性建模\">线性建模</a><br>\n2.1. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\" title=\"什么是线性模型\">什么是线性模型</a><br>\n2.2. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\" title=\"什么是损失函数\">什么是损失函数</a><br>\n2.3. <a href=\"#%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC\" title=\"对损失函数求偏导\">对损失函数求偏导</a><br>\n2.4. <a href=\"#%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%9B%BE%E8%A7%A3\" title=\"二阶导数的意义图解\">二阶导数的意义图解</a><br>\n2.5. <a href=\"#$w_0$%E5%92%8C$w_1$%E7%9A%84%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0\" title=\"$w_0$和$w_1$的二阶导数\">$w_0$和$w_1$的二阶导数</a></li>\n<li><a href=\"#%E9%A2%84%E6%B5%8B\" title=\"预测\">预测</a></li>\n<li><a href=\"#%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95\" title=\"正则化最小二乘法\">正则化最小二乘法</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a>#下载图片接口 “下载图片接口”)</li>\n</ol>\n</blockquote>\n<h3 id=\"概述\">概述</h3>\n<p>为什么需要Normalization？</p>\n<p>深度学习模型训练的难度是，CNN包含了很多不同的hiden layer， 每个layer都会随着训练改变，hiden layer的输入分布也是会一致改变，这就导致hiden layer面临covariate shift的问题。Internal covariate shift(ICS)使得每层输入不再是独立的分布，上一层数据需要适应新的输入分布，数据输入激活函数时，会落入饱和区，使得学习效率过低，甚至梯度消失。</p>\n<p>Normlization的思想主要是通过归一化的手段，将每层的输入强行拉回到均值为0，方差为1的标准正态分布，使得激活输入值分布在非线性函数梯度敏感的区域内，避免梯度消失，加快训练速度。</p>\n<p>比如将sigmoid函数，BN将输入值分布在-1～1的区间内，这个区间的梯度值大，可以避免梯度消失，提高收敛的速度。</p>\n<p>归一化后，激活输入值分布在[-1,1]内，这导致非线性程度降低，这样本来需要非线性函数进行learning高维度的表达的效果就降低了，所以BN就是为了保证非线性的表达能力，对变换后的输入$X$进行了平移。使用scale shift操作，$y=scale * x + shift$， 这两个参数通过训练获得。</p>\n<p>均值的计算和方差的计算</p>\n<p>参考<a href=\"https://blog.csdn.net/litt1e/article/details/105817224\">https://blog.csdn.net/litt1e/article/details/105817224</a></p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n"},{"title":"人生感悟-开篇","date":"2022-08-26T16:24:41.000Z","updated":"2022-08-26T16:24:41.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"2022/08/27/005-LifeMemories/人生感悟-开篇/677558eb37581273-166153976382210.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":null,"password":"Lz-Bluet","_content":"\n# ***人生感悟-『开篇』-夜味***\n\n***写在前面：***\n\n深夜的寂静，于我而言，是一个沉淀心灵的时刻，很喜欢在深夜无人时，回忆品味人生的种种味道。由此，打算以博客的形式记录自己的感悟，作为人生感悟的开篇，取名：《夜味》。\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t            \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t------愿古稀之年，回头望，犹有滋味。\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tBlueNote.\n\n---\n\n\n\n# 夜的味道\n\n夜的味道有很多种，有时候，夜味如同，与所爱之人，巫山云雨之后的那份表面平静而内心翻涌的快感，念念不忘，企图贪婪地停留在高潮的时候越久越好。又有时候，夜味如同，忙碌一天之后，空荡的房间仅留着自己的呼吸声和那呆板无趣的天花板，还有那昏黄而没有生气的床头灯。\n\n不管是欢愉还是寂寞，你都有无限的想像空间和灵感，用来创作或者沉淀，亦或是回忆。\n\n\n\n## 人生出场\n\n我的人生，对于同是95后的同龄人来说，可能节奏太快，快到没有人能预料到，我已经有了一个可爱的女儿，她也在上个月满了一周岁。\n\n噢！忘了说了，我的“标签”有：四线城市出身的程序员，双非背景普通二本，英国留学生，算法工程师，“早婚早孕”的奶爸，不愿平庸也渴望自由的普通人，也是一个被现实社会框架约束的群众。\n\n对于一个四线城市出来的“小镇青年”，我经历过最底层贫穷的环境，到最早实现工业革命的资本主义国家的环境，再到国内的一线都市的环境。人生经历不说丰富（年龄在这），但善于观察的性格，让我的阅历和心理年龄，显现在了外表上，时常被猜测是90或者91年的，倒也挺好，这使得我的社交圈更多是80-90初的朋友，向前辈取经倒是对我的成才来说，帮助很大，感谢过往帮助我的朋友、老师和家人。\n\n\n\n## 小镇青年\n\n最近，很多自媒体都在标题说：“小镇青年”，“小镇刷题家”。我作为一份子，也想说说自己的“小镇经历”，高中那会，也就是2014年，我居住的小县城还是一个五线城市的一个没有摘除“贫困县”的地方，老家是更加偏远的一个不起眼几十户人家的小村落。\n\n就是在这样一个地方，大学生的诞生方式，都是通过早6晚10的高压学习，来获得一张去往大城市的录取通知书。在我这个县城，所谓的“尖子班”的学生，包括我，都是通过每日周而复始，疯狂的填鸭式教学和刷题中看到早晨第一缕阳光和晚上洒满街道的月光。难忘的四年，应该是初三和高中三年，因为，在这个县城，从初三开始就要选择，全县前360名学生，来重点培养以“制造”出能够获得二本、一本甚至清华北大的高材生。作为，也算是”书香门第“（教师家庭）出身的一份子，自然从小就被寄以最高的期望，2岁上幼儿园，5岁读旁听生，6岁念一年级，过早地希望我能够在学校的熏陶下，成长为一个优秀的人。长大后，作为一个成年人，这也是能理解的，因为，父母都是没有上过大学，甚至高中的普通老百姓。他们深知，在这种五线极其落后的地方，他们的孩子只能通过学习，考上大学，才有可能获得一张去往其他大城市的通知书，才能出人头地，不会被困在这种小县城，耕耘着老一辈人留下来的一亩三分地（小时候，老爸经常说，你要是学习不好，就只能回村里种祖宗留下来的地）。\n\n所以，我们这些“小镇青年”都是在被“恐吓”，被“洗脑”的情况下，努力地为了在不同考试选拔中，出类拔萃，增加自己考上一个好大学的概率。也是因为这样，有了”小镇做题家“的称谓，因为，刚刚高中毕业到大学，和我类似的群体真的发现，自己以前的18年，好像都只对做题特别擅长（我是个例外，应试能力特别差劲的拖后腿选手，laughing），没有其他擅长的兴趣爱好，没有可以用来社交的擅长技能。这时候，你发现自己原来真的很普通很普通，连和女生说话，都不敢正视人家的眼睛，多聊几句可能就害羞的脸红，班级活动特别不愿意露脸的一个小男孩。青春懵懂，但又不是傻，也有自己倾慕喜欢的女孩，不过我这种“小镇青年”刚进入大学的时候，是真的如同一张惨白的纸。\n\n不过，这么多年过去了，也经常被调侃为“有故事的人”，或者“有故事的李哥”，承蒙厚爱，一个97年初出生的小弟，被大家捧成了“哥”，时常在和故友们/新交朋友的社交局中成为被大家“挖料“的对象。\n\n\n\n## 乏味无趣的童年\n\n都说童年应该是色彩斑斓的，快乐的，无忧无虑的，而我，只知道自己从记事起，便被父母安排着提早进入小学，又或者因为没人照顾，托管到幼儿园。\n\n### 幼儿园\n\n至今，我都记得，老爸第一天送我去幼儿园门口，我哭闹着，企图挣脱幼儿园老师的双手，绝望地喊着，“我不想上幼儿园，我要爸爸，我要回家”。眼泪模糊的视线中，我看到铁门关闭的那刻，我依旧不死心地冲到铁栅栏那，喊着“我要回家，我要爸爸，我要妈妈”。\n\n他们都说我的记忆力好，我觉得可能是吧，我连很小的2岁多的事情都还记得，可能海马体在那个时候已经被激活了深层记忆，至今，回想起来，虽然是朦胧的，但是那个画面是一帧帧存在大脑里的。\n\n慢慢地我习惯了，虽然每天被老爸送到幼儿园的时候，还是要哭闹一会，老师应该很不喜欢我这种小孩吧。不知道多久，我和其他同学慢慢玩到一起，有了玩伴，我们玩着积木，玩着过家家和躲猫猫地游戏。吃我那时候最不喜欢吃的茄子，记忆犹新的是有一段时间，每天午餐都是茄子。小孩子的我，最开心的应该还是，老爸开摩托车来接我放学的时候吧，那时候，老爸有一辆红色的”嘉陵“摩托，每天去乡下给别人修电视机的工具，我记得山区或者小地方，路特别不好走，很容易打滑摔车。记忆犹新的是，有一天老爸接我放学，我穿着毛毛的外套，刚和小朋友们玩玩那种卡扣式的积木（实际叫雪花积木），特地去找了找图片：\n\n<img src=\"677558eb37581273-166153976382210.jpg\" width=\"50%\" height=\"50%\">\n\n\n\n\n\n那天，我记得坐在老爸的摩托车后座，老爸接我骑车回家，我们走过了一个特别大的水塔下面的通道，通道黑漆漆的，我很害怕通道两边会不会有什么怪物，但是靠在我爸爸背上应该很安全。老爸把我从摩托车上抱下来的时候，才发现我身上毛毛的外套还粘着一个雪花积木，印象里，那时候，我们住在一个特别老旧潮湿的地方，旁边都是一些青苔，环境很差。（后来我妈的回忆中确认，我们那时候很穷，只能住这样的临时板房）\n\n这是童年的开始，我很早就在A小镇的幼儿园度过了三年，后来年龄没到读小学的年龄，又把我送到了姑姑任教的小学当读旁听生。爸妈那时候，开着家电买卖维修的店铺，所以没有人能照顾我，因此，又被托管到了姑姑那里的小学（也在A小镇），那时候，我应该5岁。\n\n\n\n### 小学\n\n我上小学之前，爸妈把店铺搬到了B小镇上，一方面，离爷爷奶奶的老家近一点，他们可以过来帮忙照顾我，另一方面，我们那时候在A小镇的店铺旁边的邻居是一个特别不好相处，无端调事，而且特别眼红我爸会维修家电而人流量比较大。因为我爸为人老实本分，街坊邻居都很信赖我爸卖的电器也有售后维修，生意特别好那时。我爸妈都是那种，比较不愿争执，宁愿躲开的态度，所以那时候，选择了离开A小镇，去了人更多的B小镇。\n\n对我来说，搬家是特别兴奋也是特别无奈的事情，因为我那时候不知道，为什么要半夜就搬走，都没有机会和那时候的小伙伴告别。来到新的B小镇，那时候我还很小，我妈和我爸都只能在路边休息，等着租房的房东或者是亲戚起来给我们开门，那时候我记得很清楚，我妈很困，但是还是把我抱在怀里，安抚着我睡觉，那时候的感觉是，我以为我们没有了住的地方，我也离开了一些认识的小伙伴，那时候我还是只会说A小镇的方言，后来到了B小镇，大人都调侃我说“xx”老表，不会说这里的话（方言）。\n\n应该也就是半年或者一年不到，我爷爷跟我说：“你要去上小学了噢，不能天天在家玩“。那时候，我爷爷还是一个乡村小学校长，还没有退休。过了几天，就带我去小学报名，我也是哭着闹着说不去，可能小孩子都不愿意失去自己可以自由玩耍的时间吧，至少我记忆里，不是一个喜欢去学校的孩子。\n\n小学的开始，我妈就是那种，你考得不好，非常气愤暴躁的态度，很凶很严厉地批评我，甚至动手揍我。那时候，我们那小地方应该很多家庭都是崇尚“棍棒底下出孝子”的教育理念吧。反正，印象里面，B小镇上，哪个孩子犯了错，肯定回家少不了一顿打，随之而来的就是小孩的哭喊声，这种教育理念，在B小镇上，尤其在我爸的新店铺周边的家庭都类似。\n\n小学开始分班，懵懂的我，还是跟着其他孩子一样，听着老师严厉的管教和布置一些我不知道怎么做的作业。每天晚上，都是我爸或者我妈来手把手教我，写字拼音，抓着我的手书写一个个学校教的汉字。那时候，我特别怕我妈，一下不注意，我妈脾气就上来了，肯定又是一顿挨骂，一顿揍。不过，以后我自己的女儿，我肯定不会用这种，“反作用”的教育方式。\n\n\n***\n\n### ***后续章节内容预告***：\n\n今天，先到这里，写得很晚了，工作之后健身还是很爽的。下期，我会回忆聊聊我自己的小学情感，那时候，我还是用家里的DVD机放着CD光盘，听着光良的《童话》，孙燕姿的《绿光》，还有飞儿乐队的《千年之恋》，多么年轻的回忆啊。 ---2022/08/27 02:35\n","source":"_posts/005-LifeMemories/人生感悟-开篇.md","raw":"---\ntitle: 人生感悟-开篇\ndate: 2022-08-27 00:24:41\nupdated: 2022-08-27 00:24:41\ntags: \"感悟\"\ncategories: \n- 人生回忆\n- 人生感悟\nkeywords: \ndescription:\ntop_img: \ncomments: true\ncover: 677558eb37581273-166153976382210.jpg\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax:\npassword: Lz-Bluet\n\n---\n\n# ***人生感悟-『开篇』-夜味***\n\n***写在前面：***\n\n深夜的寂静，于我而言，是一个沉淀心灵的时刻，很喜欢在深夜无人时，回忆品味人生的种种味道。由此，打算以博客的形式记录自己的感悟，作为人生感悟的开篇，取名：《夜味》。\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t            \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t------愿古稀之年，回头望，犹有滋味。\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tBlueNote.\n\n---\n\n\n\n# 夜的味道\n\n夜的味道有很多种，有时候，夜味如同，与所爱之人，巫山云雨之后的那份表面平静而内心翻涌的快感，念念不忘，企图贪婪地停留在高潮的时候越久越好。又有时候，夜味如同，忙碌一天之后，空荡的房间仅留着自己的呼吸声和那呆板无趣的天花板，还有那昏黄而没有生气的床头灯。\n\n不管是欢愉还是寂寞，你都有无限的想像空间和灵感，用来创作或者沉淀，亦或是回忆。\n\n\n\n## 人生出场\n\n我的人生，对于同是95后的同龄人来说，可能节奏太快，快到没有人能预料到，我已经有了一个可爱的女儿，她也在上个月满了一周岁。\n\n噢！忘了说了，我的“标签”有：四线城市出身的程序员，双非背景普通二本，英国留学生，算法工程师，“早婚早孕”的奶爸，不愿平庸也渴望自由的普通人，也是一个被现实社会框架约束的群众。\n\n对于一个四线城市出来的“小镇青年”，我经历过最底层贫穷的环境，到最早实现工业革命的资本主义国家的环境，再到国内的一线都市的环境。人生经历不说丰富（年龄在这），但善于观察的性格，让我的阅历和心理年龄，显现在了外表上，时常被猜测是90或者91年的，倒也挺好，这使得我的社交圈更多是80-90初的朋友，向前辈取经倒是对我的成才来说，帮助很大，感谢过往帮助我的朋友、老师和家人。\n\n\n\n## 小镇青年\n\n最近，很多自媒体都在标题说：“小镇青年”，“小镇刷题家”。我作为一份子，也想说说自己的“小镇经历”，高中那会，也就是2014年，我居住的小县城还是一个五线城市的一个没有摘除“贫困县”的地方，老家是更加偏远的一个不起眼几十户人家的小村落。\n\n就是在这样一个地方，大学生的诞生方式，都是通过早6晚10的高压学习，来获得一张去往大城市的录取通知书。在我这个县城，所谓的“尖子班”的学生，包括我，都是通过每日周而复始，疯狂的填鸭式教学和刷题中看到早晨第一缕阳光和晚上洒满街道的月光。难忘的四年，应该是初三和高中三年，因为，在这个县城，从初三开始就要选择，全县前360名学生，来重点培养以“制造”出能够获得二本、一本甚至清华北大的高材生。作为，也算是”书香门第“（教师家庭）出身的一份子，自然从小就被寄以最高的期望，2岁上幼儿园，5岁读旁听生，6岁念一年级，过早地希望我能够在学校的熏陶下，成长为一个优秀的人。长大后，作为一个成年人，这也是能理解的，因为，父母都是没有上过大学，甚至高中的普通老百姓。他们深知，在这种五线极其落后的地方，他们的孩子只能通过学习，考上大学，才有可能获得一张去往其他大城市的通知书，才能出人头地，不会被困在这种小县城，耕耘着老一辈人留下来的一亩三分地（小时候，老爸经常说，你要是学习不好，就只能回村里种祖宗留下来的地）。\n\n所以，我们这些“小镇青年”都是在被“恐吓”，被“洗脑”的情况下，努力地为了在不同考试选拔中，出类拔萃，增加自己考上一个好大学的概率。也是因为这样，有了”小镇做题家“的称谓，因为，刚刚高中毕业到大学，和我类似的群体真的发现，自己以前的18年，好像都只对做题特别擅长（我是个例外，应试能力特别差劲的拖后腿选手，laughing），没有其他擅长的兴趣爱好，没有可以用来社交的擅长技能。这时候，你发现自己原来真的很普通很普通，连和女生说话，都不敢正视人家的眼睛，多聊几句可能就害羞的脸红，班级活动特别不愿意露脸的一个小男孩。青春懵懂，但又不是傻，也有自己倾慕喜欢的女孩，不过我这种“小镇青年”刚进入大学的时候，是真的如同一张惨白的纸。\n\n不过，这么多年过去了，也经常被调侃为“有故事的人”，或者“有故事的李哥”，承蒙厚爱，一个97年初出生的小弟，被大家捧成了“哥”，时常在和故友们/新交朋友的社交局中成为被大家“挖料“的对象。\n\n\n\n## 乏味无趣的童年\n\n都说童年应该是色彩斑斓的，快乐的，无忧无虑的，而我，只知道自己从记事起，便被父母安排着提早进入小学，又或者因为没人照顾，托管到幼儿园。\n\n### 幼儿园\n\n至今，我都记得，老爸第一天送我去幼儿园门口，我哭闹着，企图挣脱幼儿园老师的双手，绝望地喊着，“我不想上幼儿园，我要爸爸，我要回家”。眼泪模糊的视线中，我看到铁门关闭的那刻，我依旧不死心地冲到铁栅栏那，喊着“我要回家，我要爸爸，我要妈妈”。\n\n他们都说我的记忆力好，我觉得可能是吧，我连很小的2岁多的事情都还记得，可能海马体在那个时候已经被激活了深层记忆，至今，回想起来，虽然是朦胧的，但是那个画面是一帧帧存在大脑里的。\n\n慢慢地我习惯了，虽然每天被老爸送到幼儿园的时候，还是要哭闹一会，老师应该很不喜欢我这种小孩吧。不知道多久，我和其他同学慢慢玩到一起，有了玩伴，我们玩着积木，玩着过家家和躲猫猫地游戏。吃我那时候最不喜欢吃的茄子，记忆犹新的是有一段时间，每天午餐都是茄子。小孩子的我，最开心的应该还是，老爸开摩托车来接我放学的时候吧，那时候，老爸有一辆红色的”嘉陵“摩托，每天去乡下给别人修电视机的工具，我记得山区或者小地方，路特别不好走，很容易打滑摔车。记忆犹新的是，有一天老爸接我放学，我穿着毛毛的外套，刚和小朋友们玩玩那种卡扣式的积木（实际叫雪花积木），特地去找了找图片：\n\n<img src=\"677558eb37581273-166153976382210.jpg\" width=\"50%\" height=\"50%\">\n\n\n\n\n\n那天，我记得坐在老爸的摩托车后座，老爸接我骑车回家，我们走过了一个特别大的水塔下面的通道，通道黑漆漆的，我很害怕通道两边会不会有什么怪物，但是靠在我爸爸背上应该很安全。老爸把我从摩托车上抱下来的时候，才发现我身上毛毛的外套还粘着一个雪花积木，印象里，那时候，我们住在一个特别老旧潮湿的地方，旁边都是一些青苔，环境很差。（后来我妈的回忆中确认，我们那时候很穷，只能住这样的临时板房）\n\n这是童年的开始，我很早就在A小镇的幼儿园度过了三年，后来年龄没到读小学的年龄，又把我送到了姑姑任教的小学当读旁听生。爸妈那时候，开着家电买卖维修的店铺，所以没有人能照顾我，因此，又被托管到了姑姑那里的小学（也在A小镇），那时候，我应该5岁。\n\n\n\n### 小学\n\n我上小学之前，爸妈把店铺搬到了B小镇上，一方面，离爷爷奶奶的老家近一点，他们可以过来帮忙照顾我，另一方面，我们那时候在A小镇的店铺旁边的邻居是一个特别不好相处，无端调事，而且特别眼红我爸会维修家电而人流量比较大。因为我爸为人老实本分，街坊邻居都很信赖我爸卖的电器也有售后维修，生意特别好那时。我爸妈都是那种，比较不愿争执，宁愿躲开的态度，所以那时候，选择了离开A小镇，去了人更多的B小镇。\n\n对我来说，搬家是特别兴奋也是特别无奈的事情，因为我那时候不知道，为什么要半夜就搬走，都没有机会和那时候的小伙伴告别。来到新的B小镇，那时候我还很小，我妈和我爸都只能在路边休息，等着租房的房东或者是亲戚起来给我们开门，那时候我记得很清楚，我妈很困，但是还是把我抱在怀里，安抚着我睡觉，那时候的感觉是，我以为我们没有了住的地方，我也离开了一些认识的小伙伴，那时候我还是只会说A小镇的方言，后来到了B小镇，大人都调侃我说“xx”老表，不会说这里的话（方言）。\n\n应该也就是半年或者一年不到，我爷爷跟我说：“你要去上小学了噢，不能天天在家玩“。那时候，我爷爷还是一个乡村小学校长，还没有退休。过了几天，就带我去小学报名，我也是哭着闹着说不去，可能小孩子都不愿意失去自己可以自由玩耍的时间吧，至少我记忆里，不是一个喜欢去学校的孩子。\n\n小学的开始，我妈就是那种，你考得不好，非常气愤暴躁的态度，很凶很严厉地批评我，甚至动手揍我。那时候，我们那小地方应该很多家庭都是崇尚“棍棒底下出孝子”的教育理念吧。反正，印象里面，B小镇上，哪个孩子犯了错，肯定回家少不了一顿打，随之而来的就是小孩的哭喊声，这种教育理念，在B小镇上，尤其在我爸的新店铺周边的家庭都类似。\n\n小学开始分班，懵懂的我，还是跟着其他孩子一样，听着老师严厉的管教和布置一些我不知道怎么做的作业。每天晚上，都是我爸或者我妈来手把手教我，写字拼音，抓着我的手书写一个个学校教的汉字。那时候，我特别怕我妈，一下不注意，我妈脾气就上来了，肯定又是一顿挨骂，一顿揍。不过，以后我自己的女儿，我肯定不会用这种，“反作用”的教育方式。\n\n\n***\n\n### ***后续章节内容预告***：\n\n今天，先到这里，写得很晚了，工作之后健身还是很爽的。下期，我会回忆聊聊我自己的小学情感，那时候，我还是用家里的DVD机放着CD光盘，听着光良的《童话》，孙燕姿的《绿光》，还有飞儿乐队的《千年之恋》，多么年轻的回忆啊。 ---2022/08/27 02:35\n","slug":"005-LifeMemories/人生感悟-开篇","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483sl000bu9rl5ckvc1ss","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"69e37a440e0771af5cf278fbc500d63d59b31f6dea1dadbc134ae54349cb33e3\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"Here's something encrypted, password is required to continue reading.","more":"Here's something encrypted, password is required to continue reading.","origin":"<h1><em><strong>人生感悟-『开篇』-夜味</strong></em></h1>\n<p><em><strong>写在前面：</strong></em></p>\n<p>深夜的寂静，于我而言，是一个沉淀心灵的时刻，很喜欢在深夜无人时，回忆品味人生的种种味道。由此，打算以博客的形式记录自己的感悟，作为人生感悟的开篇，取名：《夜味》。</p>\n<p>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t            \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t------愿古稀之年，回头望，犹有滋味。</p>\n<p>​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tBlueNote.</p>\n<hr>\n<h1>夜的味道</h1>\n<p>夜的味道有很多种，有时候，夜味如同，与所爱之人，巫山云雨之后的那份表面平静而内心翻涌的快感，念念不忘，企图贪婪地停留在高潮的时候越久越好。又有时候，夜味如同，忙碌一天之后，空荡的房间仅留着自己的呼吸声和那呆板无趣的天花板，还有那昏黄而没有生气的床头灯。</p>\n<p>不管是欢愉还是寂寞，你都有无限的想像空间和灵感，用来创作或者沉淀，亦或是回忆。</p>\n<h2 id=\"人生出场\">人生出场</h2>\n<p>我的人生，对于同是95后的同龄人来说，可能节奏太快，快到没有人能预料到，我已经有了一个可爱的女儿，她也在上个月满了一周岁。</p>\n<p>噢！忘了说了，我的“标签”有：四线城市出身的程序员，双非背景普通二本，英国留学生，算法工程师，“早婚早孕”的奶爸，不愿平庸也渴望自由的普通人，也是一个被现实社会框架约束的群众。</p>\n<p>对于一个四线城市出来的“小镇青年”，我经历过最底层贫穷的环境，到最早实现工业革命的资本主义国家的环境，再到国内的一线都市的环境。人生经历不说丰富（年龄在这），但善于观察的性格，让我的阅历和心理年龄，显现在了外表上，时常被猜测是90或者91年的，倒也挺好，这使得我的社交圈更多是80-90初的朋友，向前辈取经倒是对我的成才来说，帮助很大，感谢过往帮助我的朋友、老师和家人。</p>\n<h2 id=\"小镇青年\">小镇青年</h2>\n<p>最近，很多自媒体都在标题说：“小镇青年”，“小镇刷题家”。我作为一份子，也想说说自己的“小镇经历”，高中那会，也就是2014年，我居住的小县城还是一个五线城市的一个没有摘除“贫困县”的地方，老家是更加偏远的一个不起眼几十户人家的小村落。</p>\n<p>就是在这样一个地方，大学生的诞生方式，都是通过早6晚10的高压学习，来获得一张去往大城市的录取通知书。在我这个县城，所谓的“尖子班”的学生，包括我，都是通过每日周而复始，疯狂的填鸭式教学和刷题中看到早晨第一缕阳光和晚上洒满街道的月光。难忘的四年，应该是初三和高中三年，因为，在这个县城，从初三开始就要选择，全县前360名学生，来重点培养以“制造”出能够获得二本、一本甚至清华北大的高材生。作为，也算是”书香门第“（教师家庭）出身的一份子，自然从小就被寄以最高的期望，2岁上幼儿园，5岁读旁听生，6岁念一年级，过早地希望我能够在学校的熏陶下，成长为一个优秀的人。长大后，作为一个成年人，这也是能理解的，因为，父母都是没有上过大学，甚至高中的普通老百姓。他们深知，在这种五线极其落后的地方，他们的孩子只能通过学习，考上大学，才有可能获得一张去往其他大城市的通知书，才能出人头地，不会被困在这种小县城，耕耘着老一辈人留下来的一亩三分地（小时候，老爸经常说，你要是学习不好，就只能回村里种祖宗留下来的地）。</p>\n<p>所以，我们这些“小镇青年”都是在被“恐吓”，被“洗脑”的情况下，努力地为了在不同考试选拔中，出类拔萃，增加自己考上一个好大学的概率。也是因为这样，有了”小镇做题家“的称谓，因为，刚刚高中毕业到大学，和我类似的群体真的发现，自己以前的18年，好像都只对做题特别擅长（我是个例外，应试能力特别差劲的拖后腿选手，laughing），没有其他擅长的兴趣爱好，没有可以用来社交的擅长技能。这时候，你发现自己原来真的很普通很普通，连和女生说话，都不敢正视人家的眼睛，多聊几句可能就害羞的脸红，班级活动特别不愿意露脸的一个小男孩。青春懵懂，但又不是傻，也有自己倾慕喜欢的女孩，不过我这种“小镇青年”刚进入大学的时候，是真的如同一张惨白的纸。</p>\n<p>不过，这么多年过去了，也经常被调侃为“有故事的人”，或者“有故事的李哥”，承蒙厚爱，一个97年初出生的小弟，被大家捧成了“哥”，时常在和故友们/新交朋友的社交局中成为被大家“挖料“的对象。</p>\n<h2 id=\"乏味无趣的童年\">乏味无趣的童年</h2>\n<p>都说童年应该是色彩斑斓的，快乐的，无忧无虑的，而我，只知道自己从记事起，便被父母安排着提早进入小学，又或者因为没人照顾，托管到幼儿园。</p>\n<h3 id=\"幼儿园\">幼儿园</h3>\n<p>至今，我都记得，老爸第一天送我去幼儿园门口，我哭闹着，企图挣脱幼儿园老师的双手，绝望地喊着，“我不想上幼儿园，我要爸爸，我要回家”。眼泪模糊的视线中，我看到铁门关闭的那刻，我依旧不死心地冲到铁栅栏那，喊着“我要回家，我要爸爸，我要妈妈”。</p>\n<p>他们都说我的记忆力好，我觉得可能是吧，我连很小的2岁多的事情都还记得，可能海马体在那个时候已经被激活了深层记忆，至今，回想起来，虽然是朦胧的，但是那个画面是一帧帧存在大脑里的。</p>\n<p>慢慢地我习惯了，虽然每天被老爸送到幼儿园的时候，还是要哭闹一会，老师应该很不喜欢我这种小孩吧。不知道多久，我和其他同学慢慢玩到一起，有了玩伴，我们玩着积木，玩着过家家和躲猫猫地游戏。吃我那时候最不喜欢吃的茄子，记忆犹新的是有一段时间，每天午餐都是茄子。小孩子的我，最开心的应该还是，老爸开摩托车来接我放学的时候吧，那时候，老爸有一辆红色的”嘉陵“摩托，每天去乡下给别人修电视机的工具，我记得山区或者小地方，路特别不好走，很容易打滑摔车。记忆犹新的是，有一天老爸接我放学，我穿着毛毛的外套，刚和小朋友们玩玩那种卡扣式的积木（实际叫雪花积木），特地去找了找图片：</p>\n<img src=\"/2022/08/27/005-LifeMemories/%E4%BA%BA%E7%94%9F%E6%84%9F%E6%82%9F-%E5%BC%80%E7%AF%87/677558eb37581273-166153976382210.jpg\" width=\"50%\" height=\"50%\">\n<p>那天，我记得坐在老爸的摩托车后座，老爸接我骑车回家，我们走过了一个特别大的水塔下面的通道，通道黑漆漆的，我很害怕通道两边会不会有什么怪物，但是靠在我爸爸背上应该很安全。老爸把我从摩托车上抱下来的时候，才发现我身上毛毛的外套还粘着一个雪花积木，印象里，那时候，我们住在一个特别老旧潮湿的地方，旁边都是一些青苔，环境很差。（后来我妈的回忆中确认，我们那时候很穷，只能住这样的临时板房）</p>\n<p>这是童年的开始，我很早就在A小镇的幼儿园度过了三年，后来年龄没到读小学的年龄，又把我送到了姑姑任教的小学当读旁听生。爸妈那时候，开着家电买卖维修的店铺，所以没有人能照顾我，因此，又被托管到了姑姑那里的小学（也在A小镇），那时候，我应该5岁。</p>\n<h3 id=\"小学\">小学</h3>\n<p>我上小学之前，爸妈把店铺搬到了B小镇上，一方面，离爷爷奶奶的老家近一点，他们可以过来帮忙照顾我，另一方面，我们那时候在A小镇的店铺旁边的邻居是一个特别不好相处，无端调事，而且特别眼红我爸会维修家电而人流量比较大。因为我爸为人老实本分，街坊邻居都很信赖我爸卖的电器也有售后维修，生意特别好那时。我爸妈都是那种，比较不愿争执，宁愿躲开的态度，所以那时候，选择了离开A小镇，去了人更多的B小镇。</p>\n<p>对我来说，搬家是特别兴奋也是特别无奈的事情，因为我那时候不知道，为什么要半夜就搬走，都没有机会和那时候的小伙伴告别。来到新的B小镇，那时候我还很小，我妈和我爸都只能在路边休息，等着租房的房东或者是亲戚起来给我们开门，那时候我记得很清楚，我妈很困，但是还是把我抱在怀里，安抚着我睡觉，那时候的感觉是，我以为我们没有了住的地方，我也离开了一些认识的小伙伴，那时候我还是只会说A小镇的方言，后来到了B小镇，大人都调侃我说“xx”老表，不会说这里的话（方言）。</p>\n<p>应该也就是半年或者一年不到，我爷爷跟我说：“你要去上小学了噢，不能天天在家玩“。那时候，我爷爷还是一个乡村小学校长，还没有退休。过了几天，就带我去小学报名，我也是哭着闹着说不去，可能小孩子都不愿意失去自己可以自由玩耍的时间吧，至少我记忆里，不是一个喜欢去学校的孩子。</p>\n<p>小学的开始，我妈就是那种，你考得不好，非常气愤暴躁的态度，很凶很严厉地批评我，甚至动手揍我。那时候，我们那小地方应该很多家庭都是崇尚“棍棒底下出孝子”的教育理念吧。反正，印象里面，B小镇上，哪个孩子犯了错，肯定回家少不了一顿打，随之而来的就是小孩的哭喊声，这种教育理念，在B小镇上，尤其在我爸的新店铺周边的家庭都类似。</p>\n<p>小学开始分班，懵懂的我，还是跟着其他孩子一样，听着老师严厉的管教和布置一些我不知道怎么做的作业。每天晚上，都是我爸或者我妈来手把手教我，写字拼音，抓着我的手书写一个个学校教的汉字。那时候，我特别怕我妈，一下不注意，我妈脾气就上来了，肯定又是一顿挨骂，一顿揍。不过，以后我自己的女儿，我肯定不会用这种，“反作用”的教育方式。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<p>今天，先到这里，写得很晚了，工作之后健身还是很爽的。下期，我会回忆聊聊我自己的小学情感，那时候，我还是用家里的DVD机放着CD光盘，听着光良的《童话》，孙燕姿的《绿光》，还有飞儿乐队的《千年之恋》，多么年轻的回忆啊。 —2022/08/27 02:35</p>\n","encrypt":true},{"title":"来上海的第三年","date":"2023-03-20T13:02:25.000Z","updated":"2023-03-20T13:02:25.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/5f056636a92cf_270_185.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":null,"password":"Lz-Bluet","_content":"\n# ***来上海的第三年***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n\n\n### 离职\n\n正是因为离职，我才终于有时间来更新博客了！（虽然这个时候还没有人看，只是当作自己的文字记录）来上海快有三个年头了，一切回忆看起来都是那么的艰难也颇有收获，一共在两家公司待过，一家是国内的上市快递公司（据说市值最高的时候有800亿），另一家是贵阳的新创业公司（刚去的时候只有50来人），目前200人左右，两家工作职位都是算法工程师，薪资也是从最开始的1w+涨到了3w+（税前不含绩效），税后到手也就是3w+一点。算不上有说成就，只是靠着自己不懈努力赚到的。\n\n 上一份工作给自己带来的收获和负面影响都是并存的，一句话总结就是：干着比工作职能更多的事情，拿着与之不相匹配的薪水，老板还想每天画饼，HR不干人事，领导日常PUA，加班加点看不到额外的奖金，不断的内耗让自己身心疲惫。\n\n\n\n### 上海工作第一年\n\n2020年3月，在疫情的大背景下，我从国外回来，3月的时候，武汉的疫情已经基本稳定了，但是这个时候国外开始全面爆发。幸运的是，我这张机票是在2月份就买好的，当时的原因主要是博士奖学金没有申请到。在那个时候很多人都劝我去读吧，既然申请到了两所英国不错的学校。可是在我当时的内心想法是，全自费让父母背负着压力给我读4年的博士，我觉得没有必要，还是要结合自己的实际情况来做决定，虽然我妈也劝我说不要担心钱的问题。\n\n\n\n3月14号晚，我在北京首都机场落地，落地之前，机组人员就收到地面通知，我们需要等待指令再降落。降落之后，又通知我们需要等待舱外的防疫工作人员的指令。这个时候，我看向我右侧的飞机，也是坐满了一飞机的人，看情况也是刚降落的，因为飞机出舱口能看到很多“大白”在忙碌着做准备工作，如临大敌！这时候，我知道估计要下飞机，我们还要等几个小时。飞机上也有穿着防化服，全副武装的留学生和老人，心里也知道，我也有可能会在机舱内感染，这一路，基本所有乘客都没有要餐食，大家连口水都不敢喝，因为都惧怕摘了口罩可能会感染病毒，我其实也是内心惶惶不安的，但我也知道，要来的躲不掉，甚至连后面隔离之后可能会危重的情况都想好了（事后，我爸说我回来前的一天和在县城卫生所隔离的那几天都是失眠的）。\n\n假装贴好图片了==!\n\n\n\n3月15号凌晨1点左右，我们在飞机上等了快两个小时的时候，终于外面的防疫人员通知机舱可以打开，但是有几个老外被单独的点名，提前出去（估计老外这个时候感染的概率相比我们中国人来说要高），其他有**中国护照**的人按照点名的顺序依次出来排队。这个时候，我从曼城出发到落地北京，已经过去了15个小时左右（不算上比利时转机的时间）。身心都是疲惫的，但是知道自己终于可以踏上中国的国土上的时候，内心当时是非常激动的，出舱的那一刻，感觉有了航天人员返回地球出舱的味道，终于能够呼吸到在中国地面上的空气了，不知道杨利伟等航天员返回地球的时候的感受是不是和我一样，哈哈。没有开心多久，我们被“大白们”引导至入境大厅，然后听到工作人员给我说了一声“欢迎回来”，激动的心又燃起来了，但是入境之后，我就看到大量的人集中在一个大厅，这里的工作人员都在维护秩序，当时一个工作人员用扩音器喊道：“我们接到防疫通知，可能在3.14号晚落地的各位同胞需要在北京当地隔离，然后再由到达目的地的防疫办安排专车接回到各省进行隔离，目前还没有接到确定的决定，需要稍等下！”，我想估计，我下趟飞机还不知道能不能去飞了，后来我们被放行了（事后看到通知是，3.15日以后入境北京的海外人员均需要在北京当地隔离，并且费用自理，我还是幸运的）。放行后，我们陆续引导到，做核酸以及信息登记的地方排队等候。因为，我还需要换乘飞上海的飞机，忘了当时为啥买到上海了，大概率估计是没买到北京直飞南昌的票，我买了北京飞上海的飞机，然后上海虹桥到南昌的高铁，绕了一个大圈，所以，我又赶急赶忙地问了工作人员早上飞上海的航班是不是还有，我记得当时是12点左右到达上海的那趟。一步步跟随着大白们的引导，又去扫描二维码，登记了一遍身份信息、航班信息，到达目的地信息等等，身心俱疲，当时觉得很不理解，为啥我们都做了3遍信息登记了。终于，我看到换乘的指引通道，赶忙跑去问我这个航班的登机口，刚到就听到，地乘小姐姐说：“我们在等待防疫通知，确认这个航班是否还能起飞，请大家耐心等待“。我心想，完犊子，总不能就只能待在北京了吧，估计这个时候中央也是在逐步确认有多少人刚好掐着3.14号12点之前入境的。还好，最后，飞机虽然晚点了快一个小时，我们还是成功登记，行李他们也是统一消毒给我们重新托运了，特殊情况，直接自己认领，然后转手，他们运到飞机那。\n\n\n\n3.15号中午12点左右，我终于落地上海浦东国际机场了，这个时候我已经有大概26到28个小时没吃没喝没休息了，真的人有点顶不住。因为高铁是从虹桥出发，我赶紧找到浦东做直达虹桥的大巴，一上车，我就累到找了一个周围没啥人的座位。还好手机充电宝满电带着的，给手机和我充会电，准备眯一会，实在是困的不行！\n\n\n\n\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n","source":"_posts/006-WorkExperience/来上海的第三年.md","raw":"---\ntitle: 来上海的第三年\ndate: 2023-03-20 21:02:25\nupdated: 2023-03-20 21:02:25\ntags: \"上海工作\"\ncategories: \n- 工作经验\n- 上海工作\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax:\npassword: Lz-Bluet\n---\n\n# ***来上海的第三年***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n\n\n### 离职\n\n正是因为离职，我才终于有时间来更新博客了！（虽然这个时候还没有人看，只是当作自己的文字记录）来上海快有三个年头了，一切回忆看起来都是那么的艰难也颇有收获，一共在两家公司待过，一家是国内的上市快递公司（据说市值最高的时候有800亿），另一家是贵阳的新创业公司（刚去的时候只有50来人），目前200人左右，两家工作职位都是算法工程师，薪资也是从最开始的1w+涨到了3w+（税前不含绩效），税后到手也就是3w+一点。算不上有说成就，只是靠着自己不懈努力赚到的。\n\n 上一份工作给自己带来的收获和负面影响都是并存的，一句话总结就是：干着比工作职能更多的事情，拿着与之不相匹配的薪水，老板还想每天画饼，HR不干人事，领导日常PUA，加班加点看不到额外的奖金，不断的内耗让自己身心疲惫。\n\n\n\n### 上海工作第一年\n\n2020年3月，在疫情的大背景下，我从国外回来，3月的时候，武汉的疫情已经基本稳定了，但是这个时候国外开始全面爆发。幸运的是，我这张机票是在2月份就买好的，当时的原因主要是博士奖学金没有申请到。在那个时候很多人都劝我去读吧，既然申请到了两所英国不错的学校。可是在我当时的内心想法是，全自费让父母背负着压力给我读4年的博士，我觉得没有必要，还是要结合自己的实际情况来做决定，虽然我妈也劝我说不要担心钱的问题。\n\n\n\n3月14号晚，我在北京首都机场落地，落地之前，机组人员就收到地面通知，我们需要等待指令再降落。降落之后，又通知我们需要等待舱外的防疫工作人员的指令。这个时候，我看向我右侧的飞机，也是坐满了一飞机的人，看情况也是刚降落的，因为飞机出舱口能看到很多“大白”在忙碌着做准备工作，如临大敌！这时候，我知道估计要下飞机，我们还要等几个小时。飞机上也有穿着防化服，全副武装的留学生和老人，心里也知道，我也有可能会在机舱内感染，这一路，基本所有乘客都没有要餐食，大家连口水都不敢喝，因为都惧怕摘了口罩可能会感染病毒，我其实也是内心惶惶不安的，但我也知道，要来的躲不掉，甚至连后面隔离之后可能会危重的情况都想好了（事后，我爸说我回来前的一天和在县城卫生所隔离的那几天都是失眠的）。\n\n假装贴好图片了==!\n\n\n\n3月15号凌晨1点左右，我们在飞机上等了快两个小时的时候，终于外面的防疫人员通知机舱可以打开，但是有几个老外被单独的点名，提前出去（估计老外这个时候感染的概率相比我们中国人来说要高），其他有**中国护照**的人按照点名的顺序依次出来排队。这个时候，我从曼城出发到落地北京，已经过去了15个小时左右（不算上比利时转机的时间）。身心都是疲惫的，但是知道自己终于可以踏上中国的国土上的时候，内心当时是非常激动的，出舱的那一刻，感觉有了航天人员返回地球出舱的味道，终于能够呼吸到在中国地面上的空气了，不知道杨利伟等航天员返回地球的时候的感受是不是和我一样，哈哈。没有开心多久，我们被“大白们”引导至入境大厅，然后听到工作人员给我说了一声“欢迎回来”，激动的心又燃起来了，但是入境之后，我就看到大量的人集中在一个大厅，这里的工作人员都在维护秩序，当时一个工作人员用扩音器喊道：“我们接到防疫通知，可能在3.14号晚落地的各位同胞需要在北京当地隔离，然后再由到达目的地的防疫办安排专车接回到各省进行隔离，目前还没有接到确定的决定，需要稍等下！”，我想估计，我下趟飞机还不知道能不能去飞了，后来我们被放行了（事后看到通知是，3.15日以后入境北京的海外人员均需要在北京当地隔离，并且费用自理，我还是幸运的）。放行后，我们陆续引导到，做核酸以及信息登记的地方排队等候。因为，我还需要换乘飞上海的飞机，忘了当时为啥买到上海了，大概率估计是没买到北京直飞南昌的票，我买了北京飞上海的飞机，然后上海虹桥到南昌的高铁，绕了一个大圈，所以，我又赶急赶忙地问了工作人员早上飞上海的航班是不是还有，我记得当时是12点左右到达上海的那趟。一步步跟随着大白们的引导，又去扫描二维码，登记了一遍身份信息、航班信息，到达目的地信息等等，身心俱疲，当时觉得很不理解，为啥我们都做了3遍信息登记了。终于，我看到换乘的指引通道，赶忙跑去问我这个航班的登机口，刚到就听到，地乘小姐姐说：“我们在等待防疫通知，确认这个航班是否还能起飞，请大家耐心等待“。我心想，完犊子，总不能就只能待在北京了吧，估计这个时候中央也是在逐步确认有多少人刚好掐着3.14号12点之前入境的。还好，最后，飞机虽然晚点了快一个小时，我们还是成功登记，行李他们也是统一消毒给我们重新托运了，特殊情况，直接自己认领，然后转手，他们运到飞机那。\n\n\n\n3.15号中午12点左右，我终于落地上海浦东国际机场了，这个时候我已经有大概26到28个小时没吃没喝没休息了，真的人有点顶不住。因为高铁是从虹桥出发，我赶紧找到浦东做直达虹桥的大巴，一上车，我就累到找了一个周围没啥人的座位。还好手机充电宝满电带着的，给手机和我充会电，准备眯一会，实在是困的不行！\n\n\n\n\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n","slug":"006-WorkExperience/来上海的第三年","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483sn000fu9rl1btrcb6n","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"e46a0e1c683c846f862c64eef9aee5c2d643c544e5812fb332d7e0b0a5f449e5\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"Here's something encrypted, password is required to continue reading.","more":"Here's something encrypted, password is required to continue reading.","origin":"<h1><em><strong>来上海的第三年</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h3 id=\"离职\">离职</h3>\n<p>正是因为离职，我才终于有时间来更新博客了！（虽然这个时候还没有人看，只是当作自己的文字记录）来上海快有三个年头了，一切回忆看起来都是那么的艰难也颇有收获，一共在两家公司待过，一家是国内的上市快递公司（据说市值最高的时候有800亿），另一家是贵阳的新创业公司（刚去的时候只有50来人），目前200人左右，两家工作职位都是算法工程师，薪资也是从最开始的1w+涨到了3w+（税前不含绩效），税后到手也就是3w+一点。算不上有说成就，只是靠着自己不懈努力赚到的。</p>\n<p>上一份工作给自己带来的收获和负面影响都是并存的，一句话总结就是：干着比工作职能更多的事情，拿着与之不相匹配的薪水，老板还想每天画饼，HR不干人事，领导日常PUA，加班加点看不到额外的奖金，不断的内耗让自己身心疲惫。</p>\n<h3 id=\"上海工作第一年\">上海工作第一年</h3>\n<p>2020年3月，在疫情的大背景下，我从国外回来，3月的时候，武汉的疫情已经基本稳定了，但是这个时候国外开始全面爆发。幸运的是，我这张机票是在2月份就买好的，当时的原因主要是博士奖学金没有申请到。在那个时候很多人都劝我去读吧，既然申请到了两所英国不错的学校。可是在我当时的内心想法是，全自费让父母背负着压力给我读4年的博士，我觉得没有必要，还是要结合自己的实际情况来做决定，虽然我妈也劝我说不要担心钱的问题。</p>\n<p>3月14号晚，我在北京首都机场落地，落地之前，机组人员就收到地面通知，我们需要等待指令再降落。降落之后，又通知我们需要等待舱外的防疫工作人员的指令。这个时候，我看向我右侧的飞机，也是坐满了一飞机的人，看情况也是刚降落的，因为飞机出舱口能看到很多“大白”在忙碌着做准备工作，如临大敌！这时候，我知道估计要下飞机，我们还要等几个小时。飞机上也有穿着防化服，全副武装的留学生和老人，心里也知道，我也有可能会在机舱内感染，这一路，基本所有乘客都没有要餐食，大家连口水都不敢喝，因为都惧怕摘了口罩可能会感染病毒，我其实也是内心惶惶不安的，但我也知道，要来的躲不掉，甚至连后面隔离之后可能会危重的情况都想好了（事后，我爸说我回来前的一天和在县城卫生所隔离的那几天都是失眠的）。</p>\n<p>假装贴好图片了==!</p>\n<p>3月15号凌晨1点左右，我们在飞机上等了快两个小时的时候，终于外面的防疫人员通知机舱可以打开，但是有几个老外被单独的点名，提前出去（估计老外这个时候感染的概率相比我们中国人来说要高），其他有<strong>中国护照</strong>的人按照点名的顺序依次出来排队。这个时候，我从曼城出发到落地北京，已经过去了15个小时左右（不算上比利时转机的时间）。身心都是疲惫的，但是知道自己终于可以踏上中国的国土上的时候，内心当时是非常激动的，出舱的那一刻，感觉有了航天人员返回地球出舱的味道，终于能够呼吸到在中国地面上的空气了，不知道杨利伟等航天员返回地球的时候的感受是不是和我一样，哈哈。没有开心多久，我们被“大白们”引导至入境大厅，然后听到工作人员给我说了一声“欢迎回来”，激动的心又燃起来了，但是入境之后，我就看到大量的人集中在一个大厅，这里的工作人员都在维护秩序，当时一个工作人员用扩音器喊道：“我们接到防疫通知，可能在3.14号晚落地的各位同胞需要在北京当地隔离，然后再由到达目的地的防疫办安排专车接回到各省进行隔离，目前还没有接到确定的决定，需要稍等下！”，我想估计，我下趟飞机还不知道能不能去飞了，后来我们被放行了（事后看到通知是，3.15日以后入境北京的海外人员均需要在北京当地隔离，并且费用自理，我还是幸运的）。放行后，我们陆续引导到，做核酸以及信息登记的地方排队等候。因为，我还需要换乘飞上海的飞机，忘了当时为啥买到上海了，大概率估计是没买到北京直飞南昌的票，我买了北京飞上海的飞机，然后上海虹桥到南昌的高铁，绕了一个大圈，所以，我又赶急赶忙地问了工作人员早上飞上海的航班是不是还有，我记得当时是12点左右到达上海的那趟。一步步跟随着大白们的引导，又去扫描二维码，登记了一遍身份信息、航班信息，到达目的地信息等等，身心俱疲，当时觉得很不理解，为啥我们都做了3遍信息登记了。终于，我看到换乘的指引通道，赶忙跑去问我这个航班的登机口，刚到就听到，地乘小姐姐说：“我们在等待防疫通知，确认这个航班是否还能起飞，请大家耐心等待“。我心想，完犊子，总不能就只能待在北京了吧，估计这个时候中央也是在逐步确认有多少人刚好掐着3.14号12点之前入境的。还好，最后，飞机虽然晚点了快一个小时，我们还是成功登记，行李他们也是统一消毒给我们重新托运了，特殊情况，直接自己认领，然后转手，他们运到飞机那。</p>\n<p>3.15号中午12点左右，我终于落地上海浦东国际机场了，这个时候我已经有大概26到28个小时没吃没喝没休息了，真的人有点顶不住。因为高铁是从虹桥出发，我赶紧找到浦东做直达虹桥的大巴，一上车，我就累到找了一个周围没啥人的座位。还好手机充电宝满电带着的，给手机和我充会电，准备眯一会，实在是困的不行！</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n","encrypt":true},{"title":"Pytorch介绍","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-09-01T02:37:37.000Z","updated":"2023-09-01T02:37:37.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/walle.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***Pytorch介绍***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n\n\n# Torch的背景\n\nTorch是一个与Numpy类似的张量（Tensor）操作库，与Numpy不同的是Torch对GPU支持的很好，Lua是Torch的上层包装。\n\n\n\n# PyTorch\n\nPyTorch是一个基于Torch的Python开源机器学习库，它主要由Facebook的人工智能研究小组开发。Uber的\"Pyro\"也是使用的这个库。\n\nPyTorch是一个Python包，提供两个高级功能：\n\n- 具有强大的GPU加速的张量计算（如NumPy）\n\n- 包含自动求导系统的的深度神经网络\n\n\n\n# Tensorflow VS Pytorch\n\n## 上手时间\n\nPyTorch本质上是Numpy的替代者，而且支持GPU、带有高级功能，可以用来搭建和训练深度神经网络。如果你熟悉Numpy、Python以及常见的深度学习概念（卷积层、循环层、SGD等），会非常容易上手PyTorch。\n\nTensorFlow可以看成是一个嵌入Python的编程语言。你写的TensorFlow代码会被Python编译成一张图，然后由TensorFlow执行引擎运行。我见过好多新手，因为这个增加的间接层而困扰。也正是因为同样的原因，TensorFlow有一些额外的概念需要学习，例如会话、图、变量作用域（variable scoping）、占位符等。\n\n\n\n## **图创建和调试**\n\nPytorch的图结构是动态的，在运行时构建。Tensorflow图结构是静态的。\n\n调试的时候，Pytorch可以像调试python代码一样，使用pdb并在任何地方可以设置断点。调试Tensorflow需要从会话请求中检查变量的情况，或者学会使用Tensorflow的调试器tfdbg。\n\n![](https://www.synotech.top:5523/uploads/2023/09/01/202309011045395 \"\")\n\n## 全面性\n\nTensorflow支持的功能比Pytorch更多，截止写博客的时间，这个差异很小了。Tensorflow有的更多功能点：\n\n- 沿维翻转张量（np.flip, np.flipud, np.fliplr）\n\n- 检查无穷与非数值张量（np.is_nan, np.is_inf）\n\n- 快速傅里叶变换（np.fft）\n\n## 序列化\n\n两种框架下保存和加载模型都很简单。PyTorch有一个特别简单的API，**可以保存模型的所有权重或pickle整个类**。TensorFlow的Saver对象也很易用，而且为检查提供了更多的选项。\n\nTensorFlow序列化的主要**优点是可以将整个图保存为protocol buffer**。包括参数和操作。然而图还能被**加载进其他支持的语言**（**C++、Java**）。这对于**部署堆栈至关重要**。理论上，当你想改动模型源代码但仍希望运行旧模型时非常有用。\n\n## 部署\n\n对于小规模的服务器端部署（例如一个Flask web server），两个框架都很简单。\n\n对于移动端和嵌入式部署，TensorFlow更好。不只是比PyTorch好，比大多数深度学习框架都要好。使用TensorFlow，部署在Android或iOS平台时只需要很小的工作量，**至少不必用Java或者C++重写模型的推断部分**。\n\n对于高性能服务器端的部署，还有TensorFlow Serving能用。除了性能之外，**TensorFlow Serving一个显著的优点是可以轻松的热插拔模型，而不会使服务失效**。\n\n## 数据加载\n\nPyTorch中用于加载数据的API设计的很棒。接口由**一个数据集、一个取样器和一个数据加载器构成**。数据加载器根据取样器的计划，基于数据集产生一个迭代器。**并行化数据加载简单的就像把num_workers参数传递给数据加载器一样简单。**\n\nTensorFlow中没有发现特别有用的数据加载工具。很多时候，并不总能直接把准备并行运行的预处理代码加入TensorFlow图。以及API本身冗长难学。\n\n## **设备管理**\n\nTensorFlow的**设备管理非常好用**。通常你不需要进行调整，因为**默认的设置就很好**。例如，TensorFlow会假设你想运行在GPU上（如果有的话）。而在PyTorch中，即使启用了CUDA，你**也需要明确把一切移入设备**。\n\nTensorFlow设备管理唯一的缺点是，默认情况下，它会占用所有的GPU显存。简单的解决办法是**指定CUDA_VISIBLE_DEVICES**。有时候大家会忘了这一点，所以GPU在空闲的时候，也会显得很忙。\n\n在PyTorch中，我发现**代码需要更频繁的检查CUDA是否可用**，以及**更明确的设备管理**。在编写能够同时在CPU和GPU上运行的代码时尤其如此。以及得把GPU上的**PyTorch变量转换为Numpy数组**，这就显得有点冗长。\n\n## **自定义扩展**\n\n两个框架都可以构建和绑定用C、C++、CUDA编写的自定义扩展。TensorFlow仍然需要更多的样板代码，尽管这对于支持多类型和设备可能更好。在PyTorch中，你只需为每个CPU和GPU编写一个接口和相应的实现。两个框架中编译扩展也是直接记性，并不需要在pip安装的内容之外下载任何头文件或者源代码。\n\n\n\n## TensorBoard\n\n![](https://www.synotech.top:5523/uploads/2023/09/01/202309011045600 \"\")\n\n**TensorBoard**是TensorFlow自带的可视化工具，用来查看**机器学习训练过程中数据的变化**。通过训练脚本中的几个代码段，你可以查看**任何模型的训练曲线和验证结果**。TensorBoard作为web服务运行，特别便于对于**无头结点上存储的结果进行可视化**。\n\n第一个是**tensorboard_logger**，第二个是**crayon**。tensorboard_logger库用起来甚至比TensorBoard的“摘要”更容易，尽管想用这个首先得安装TensorBoard。\n\n\n\n# 安装Pytorch环境\n\n一般建议准从官网给的指导，官网链接：\n\n安装以前的版本链接如下\n\n[PyTorch](https://pytorch.org/get-started/previous-versions/)\n\n比如我这里安装的指令如下：\n\n```text\npip install torch==1.12.0+cu102 /\ntorchvision==0.13.0+cu102 torchaudio==0.12.0 /\n--extra-index-url /\nhttps://download.pytorch.org/whl/cu102\n```\n\n建议根据你本地安装好的CUDA环境来选择版本，否则会出现很多奇怪的异常。\n\n安装好之后你可以使用以下命令输出torch的版本信息：\n\n```text\nimport torch\nprint(torch.__version__)\n```\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n","source":"_posts/013-DeepLearning/Pytorch介绍.md","raw":"---\ntitle: Pytorch介绍\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-09-01 10:37:37\nupdated: 2023-09-01 10:37:37\ntags: Pytorch\ncategories:\n- DeepLearning\n- Pytorch\nkeywords:\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***Pytorch介绍***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n\n\n# Torch的背景\n\nTorch是一个与Numpy类似的张量（Tensor）操作库，与Numpy不同的是Torch对GPU支持的很好，Lua是Torch的上层包装。\n\n\n\n# PyTorch\n\nPyTorch是一个基于Torch的Python开源机器学习库，它主要由Facebook的人工智能研究小组开发。Uber的\"Pyro\"也是使用的这个库。\n\nPyTorch是一个Python包，提供两个高级功能：\n\n- 具有强大的GPU加速的张量计算（如NumPy）\n\n- 包含自动求导系统的的深度神经网络\n\n\n\n# Tensorflow VS Pytorch\n\n## 上手时间\n\nPyTorch本质上是Numpy的替代者，而且支持GPU、带有高级功能，可以用来搭建和训练深度神经网络。如果你熟悉Numpy、Python以及常见的深度学习概念（卷积层、循环层、SGD等），会非常容易上手PyTorch。\n\nTensorFlow可以看成是一个嵌入Python的编程语言。你写的TensorFlow代码会被Python编译成一张图，然后由TensorFlow执行引擎运行。我见过好多新手，因为这个增加的间接层而困扰。也正是因为同样的原因，TensorFlow有一些额外的概念需要学习，例如会话、图、变量作用域（variable scoping）、占位符等。\n\n\n\n## **图创建和调试**\n\nPytorch的图结构是动态的，在运行时构建。Tensorflow图结构是静态的。\n\n调试的时候，Pytorch可以像调试python代码一样，使用pdb并在任何地方可以设置断点。调试Tensorflow需要从会话请求中检查变量的情况，或者学会使用Tensorflow的调试器tfdbg。\n\n![](https://www.synotech.top:5523/uploads/2023/09/01/202309011045395 \"\")\n\n## 全面性\n\nTensorflow支持的功能比Pytorch更多，截止写博客的时间，这个差异很小了。Tensorflow有的更多功能点：\n\n- 沿维翻转张量（np.flip, np.flipud, np.fliplr）\n\n- 检查无穷与非数值张量（np.is_nan, np.is_inf）\n\n- 快速傅里叶变换（np.fft）\n\n## 序列化\n\n两种框架下保存和加载模型都很简单。PyTorch有一个特别简单的API，**可以保存模型的所有权重或pickle整个类**。TensorFlow的Saver对象也很易用，而且为检查提供了更多的选项。\n\nTensorFlow序列化的主要**优点是可以将整个图保存为protocol buffer**。包括参数和操作。然而图还能被**加载进其他支持的语言**（**C++、Java**）。这对于**部署堆栈至关重要**。理论上，当你想改动模型源代码但仍希望运行旧模型时非常有用。\n\n## 部署\n\n对于小规模的服务器端部署（例如一个Flask web server），两个框架都很简单。\n\n对于移动端和嵌入式部署，TensorFlow更好。不只是比PyTorch好，比大多数深度学习框架都要好。使用TensorFlow，部署在Android或iOS平台时只需要很小的工作量，**至少不必用Java或者C++重写模型的推断部分**。\n\n对于高性能服务器端的部署，还有TensorFlow Serving能用。除了性能之外，**TensorFlow Serving一个显著的优点是可以轻松的热插拔模型，而不会使服务失效**。\n\n## 数据加载\n\nPyTorch中用于加载数据的API设计的很棒。接口由**一个数据集、一个取样器和一个数据加载器构成**。数据加载器根据取样器的计划，基于数据集产生一个迭代器。**并行化数据加载简单的就像把num_workers参数传递给数据加载器一样简单。**\n\nTensorFlow中没有发现特别有用的数据加载工具。很多时候，并不总能直接把准备并行运行的预处理代码加入TensorFlow图。以及API本身冗长难学。\n\n## **设备管理**\n\nTensorFlow的**设备管理非常好用**。通常你不需要进行调整，因为**默认的设置就很好**。例如，TensorFlow会假设你想运行在GPU上（如果有的话）。而在PyTorch中，即使启用了CUDA，你**也需要明确把一切移入设备**。\n\nTensorFlow设备管理唯一的缺点是，默认情况下，它会占用所有的GPU显存。简单的解决办法是**指定CUDA_VISIBLE_DEVICES**。有时候大家会忘了这一点，所以GPU在空闲的时候，也会显得很忙。\n\n在PyTorch中，我发现**代码需要更频繁的检查CUDA是否可用**，以及**更明确的设备管理**。在编写能够同时在CPU和GPU上运行的代码时尤其如此。以及得把GPU上的**PyTorch变量转换为Numpy数组**，这就显得有点冗长。\n\n## **自定义扩展**\n\n两个框架都可以构建和绑定用C、C++、CUDA编写的自定义扩展。TensorFlow仍然需要更多的样板代码，尽管这对于支持多类型和设备可能更好。在PyTorch中，你只需为每个CPU和GPU编写一个接口和相应的实现。两个框架中编译扩展也是直接记性，并不需要在pip安装的内容之外下载任何头文件或者源代码。\n\n\n\n## TensorBoard\n\n![](https://www.synotech.top:5523/uploads/2023/09/01/202309011045600 \"\")\n\n**TensorBoard**是TensorFlow自带的可视化工具，用来查看**机器学习训练过程中数据的变化**。通过训练脚本中的几个代码段，你可以查看**任何模型的训练曲线和验证结果**。TensorBoard作为web服务运行，特别便于对于**无头结点上存储的结果进行可视化**。\n\n第一个是**tensorboard_logger**，第二个是**crayon**。tensorboard_logger库用起来甚至比TensorBoard的“摘要”更容易，尽管想用这个首先得安装TensorBoard。\n\n\n\n# 安装Pytorch环境\n\n一般建议准从官网给的指导，官网链接：\n\n安装以前的版本链接如下\n\n[PyTorch](https://pytorch.org/get-started/previous-versions/)\n\n比如我这里安装的指令如下：\n\n```text\npip install torch==1.12.0+cu102 /\ntorchvision==0.13.0+cu102 torchaudio==0.12.0 /\n--extra-index-url /\nhttps://download.pytorch.org/whl/cu102\n```\n\n建议根据你本地安装好的CUDA环境来选择版本，否则会出现很多奇怪的异常。\n\n安装好之后你可以使用以下命令输出torch的版本信息：\n\n```text\nimport torch\nprint(torch.__version__)\n```\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n","slug":"013-DeepLearning/Pytorch介绍","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483so000gu9rl33z43oqx","content":"<h1><em><strong>Pytorch介绍</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>Torch的背景</h1>\n<p>Torch是一个与Numpy类似的张量（Tensor）操作库，与Numpy不同的是Torch对GPU支持的很好，Lua是Torch的上层包装。</p>\n<h1>PyTorch</h1>\n<p>PyTorch是一个基于Torch的Python开源机器学习库，它主要由Facebook的人工智能研究小组开发。Uber的&quot;Pyro&quot;也是使用的这个库。</p>\n<p>PyTorch是一个Python包，提供两个高级功能：</p>\n<ul>\n<li>\n<p>具有强大的GPU加速的张量计算（如NumPy）</p>\n</li>\n<li>\n<p>包含自动求导系统的的深度神经网络</p>\n</li>\n</ul>\n<h1>Tensorflow VS Pytorch</h1>\n<h2 id=\"上手时间\">上手时间</h2>\n<p>PyTorch本质上是Numpy的替代者，而且支持GPU、带有高级功能，可以用来搭建和训练深度神经网络。如果你熟悉Numpy、Python以及常见的深度学习概念（卷积层、循环层、SGD等），会非常容易上手PyTorch。</p>\n<p>TensorFlow可以看成是一个嵌入Python的编程语言。你写的TensorFlow代码会被Python编译成一张图，然后由TensorFlow执行引擎运行。我见过好多新手，因为这个增加的间接层而困扰。也正是因为同样的原因，TensorFlow有一些额外的概念需要学习，例如会话、图、变量作用域（variable scoping）、占位符等。</p>\n<h2 id=\"图创建和调试\"><strong>图创建和调试</strong></h2>\n<p>Pytorch的图结构是动态的，在运行时构建。Tensorflow图结构是静态的。</p>\n<p>调试的时候，Pytorch可以像调试python代码一样，使用pdb并在任何地方可以设置断点。调试Tensorflow需要从会话请求中检查变量的情况，或者学会使用Tensorflow的调试器tfdbg。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/09/01/202309011045395\" alt></p>\n<h2 id=\"全面性\">全面性</h2>\n<p>Tensorflow支持的功能比Pytorch更多，截止写博客的时间，这个差异很小了。Tensorflow有的更多功能点：</p>\n<ul>\n<li>\n<p>沿维翻转张量（np.flip, np.flipud, np.fliplr）</p>\n</li>\n<li>\n<p>检查无穷与非数值张量（np.is_nan, np.is_inf）</p>\n</li>\n<li>\n<p>快速傅里叶变换（np.fft）</p>\n</li>\n</ul>\n<h2 id=\"序列化\">序列化</h2>\n<p>两种框架下保存和加载模型都很简单。PyTorch有一个特别简单的API，<strong>可以保存模型的所有权重或pickle整个类</strong>。TensorFlow的Saver对象也很易用，而且为检查提供了更多的选项。</p>\n<p>TensorFlow序列化的主要<strong>优点是可以将整个图保存为protocol buffer</strong>。包括参数和操作。然而图还能被<strong>加载进其他支持的语言</strong>（<strong>C++、Java</strong>）。这对于<strong>部署堆栈至关重要</strong>。理论上，当你想改动模型源代码但仍希望运行旧模型时非常有用。</p>\n<h2 id=\"部署\">部署</h2>\n<p>对于小规模的服务器端部署（例如一个Flask web server），两个框架都很简单。</p>\n<p>对于移动端和嵌入式部署，TensorFlow更好。不只是比PyTorch好，比大多数深度学习框架都要好。使用TensorFlow，部署在Android或iOS平台时只需要很小的工作量，<strong>至少不必用Java或者C++重写模型的推断部分</strong>。</p>\n<p>对于高性能服务器端的部署，还有TensorFlow Serving能用。除了性能之外，<strong>TensorFlow Serving一个显著的优点是可以轻松的热插拔模型，而不会使服务失效</strong>。</p>\n<h2 id=\"数据加载\">数据加载</h2>\n<p>PyTorch中用于加载数据的API设计的很棒。接口由<strong>一个数据集、一个取样器和一个数据加载器构成</strong>。数据加载器根据取样器的计划，基于数据集产生一个迭代器。<strong>并行化数据加载简单的就像把num_workers参数传递给数据加载器一样简单。</strong></p>\n<p>TensorFlow中没有发现特别有用的数据加载工具。很多时候，并不总能直接把准备并行运行的预处理代码加入TensorFlow图。以及API本身冗长难学。</p>\n<h2 id=\"设备管理\"><strong>设备管理</strong></h2>\n<p>TensorFlow的<strong>设备管理非常好用</strong>。通常你不需要进行调整，因为<strong>默认的设置就很好</strong>。例如，TensorFlow会假设你想运行在GPU上（如果有的话）。而在PyTorch中，即使启用了CUDA，你<strong>也需要明确把一切移入设备</strong>。</p>\n<p>TensorFlow设备管理唯一的缺点是，默认情况下，它会占用所有的GPU显存。简单的解决办法是<strong>指定CUDA_VISIBLE_DEVICES</strong>。有时候大家会忘了这一点，所以GPU在空闲的时候，也会显得很忙。</p>\n<p>在PyTorch中，我发现<strong>代码需要更频繁的检查CUDA是否可用</strong>，以及<strong>更明确的设备管理</strong>。在编写能够同时在CPU和GPU上运行的代码时尤其如此。以及得把GPU上的<strong>PyTorch变量转换为Numpy数组</strong>，这就显得有点冗长。</p>\n<h2 id=\"自定义扩展\"><strong>自定义扩展</strong></h2>\n<p>两个框架都可以构建和绑定用C、C++、CUDA编写的自定义扩展。TensorFlow仍然需要更多的样板代码，尽管这对于支持多类型和设备可能更好。在PyTorch中，你只需为每个CPU和GPU编写一个接口和相应的实现。两个框架中编译扩展也是直接记性，并不需要在pip安装的内容之外下载任何头文件或者源代码。</p>\n<h2 id=\"TensorBoard\">TensorBoard</h2>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/09/01/202309011045600\" alt></p>\n<p><strong>TensorBoard</strong>是TensorFlow自带的可视化工具，用来查看<strong>机器学习训练过程中数据的变化</strong>。通过训练脚本中的几个代码段，你可以查看<strong>任何模型的训练曲线和验证结果</strong>。TensorBoard作为web服务运行，特别便于对于<strong>无头结点上存储的结果进行可视化</strong>。</p>\n<p>第一个是<strong>tensorboard_logger</strong>，第二个是<strong>crayon</strong>。tensorboard_logger库用起来甚至比TensorBoard的“摘要”更容易，尽管想用这个首先得安装TensorBoard。</p>\n<h1>安装Pytorch环境</h1>\n<p>一般建议准从官网给的指导，官网链接：</p>\n<p>安装以前的版本链接如下</p>\n<p><a href=\"https://pytorch.org/get-started/previous-versions/\">PyTorch</a></p>\n<p>比如我这里安装的指令如下：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch==1.12.0+cu102 /</span><br><span class=\"line\">torchvision==0.13.0+cu102 torchaudio==0.12.0 /</span><br><span class=\"line\">--extra-index-url /</span><br><span class=\"line\">https://download.pytorch.org/whl/cu102</span><br></pre></td></tr></table></figure>\n<p>建议根据你本地安装好的CUDA环境来选择版本，否则会出现很多奇怪的异常。</p>\n<p>安装好之后你可以使用以下命令输出torch的版本信息：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">print(torch.__version__)</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>Pytorch介绍</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>Torch的背景</h1>\n<p>Torch是一个与Numpy类似的张量（Tensor）操作库，与Numpy不同的是Torch对GPU支持的很好，Lua是Torch的上层包装。</p>\n<h1>PyTorch</h1>\n<p>PyTorch是一个基于Torch的Python开源机器学习库，它主要由Facebook的人工智能研究小组开发。Uber的&quot;Pyro&quot;也是使用的这个库。</p>\n<p>PyTorch是一个Python包，提供两个高级功能：</p>\n<ul>\n<li>\n<p>具有强大的GPU加速的张量计算（如NumPy）</p>\n</li>\n<li>\n<p>包含自动求导系统的的深度神经网络</p>\n</li>\n</ul>\n<h1>Tensorflow VS Pytorch</h1>\n<h2 id=\"上手时间\">上手时间</h2>\n<p>PyTorch本质上是Numpy的替代者，而且支持GPU、带有高级功能，可以用来搭建和训练深度神经网络。如果你熟悉Numpy、Python以及常见的深度学习概念（卷积层、循环层、SGD等），会非常容易上手PyTorch。</p>\n<p>TensorFlow可以看成是一个嵌入Python的编程语言。你写的TensorFlow代码会被Python编译成一张图，然后由TensorFlow执行引擎运行。我见过好多新手，因为这个增加的间接层而困扰。也正是因为同样的原因，TensorFlow有一些额外的概念需要学习，例如会话、图、变量作用域（variable scoping）、占位符等。</p>\n<h2 id=\"图创建和调试\"><strong>图创建和调试</strong></h2>\n<p>Pytorch的图结构是动态的，在运行时构建。Tensorflow图结构是静态的。</p>\n<p>调试的时候，Pytorch可以像调试python代码一样，使用pdb并在任何地方可以设置断点。调试Tensorflow需要从会话请求中检查变量的情况，或者学会使用Tensorflow的调试器tfdbg。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/09/01/202309011045395\" alt></p>\n<h2 id=\"全面性\">全面性</h2>\n<p>Tensorflow支持的功能比Pytorch更多，截止写博客的时间，这个差异很小了。Tensorflow有的更多功能点：</p>\n<ul>\n<li>\n<p>沿维翻转张量（np.flip, np.flipud, np.fliplr）</p>\n</li>\n<li>\n<p>检查无穷与非数值张量（np.is_nan, np.is_inf）</p>\n</li>\n<li>\n<p>快速傅里叶变换（np.fft）</p>\n</li>\n</ul>\n<h2 id=\"序列化\">序列化</h2>\n<p>两种框架下保存和加载模型都很简单。PyTorch有一个特别简单的API，<strong>可以保存模型的所有权重或pickle整个类</strong>。TensorFlow的Saver对象也很易用，而且为检查提供了更多的选项。</p>\n<p>TensorFlow序列化的主要<strong>优点是可以将整个图保存为protocol buffer</strong>。包括参数和操作。然而图还能被<strong>加载进其他支持的语言</strong>（<strong>C++、Java</strong>）。这对于<strong>部署堆栈至关重要</strong>。理论上，当你想改动模型源代码但仍希望运行旧模型时非常有用。</p>\n<h2 id=\"部署\">部署</h2>\n<p>对于小规模的服务器端部署（例如一个Flask web server），两个框架都很简单。</p>\n<p>对于移动端和嵌入式部署，TensorFlow更好。不只是比PyTorch好，比大多数深度学习框架都要好。使用TensorFlow，部署在Android或iOS平台时只需要很小的工作量，<strong>至少不必用Java或者C++重写模型的推断部分</strong>。</p>\n<p>对于高性能服务器端的部署，还有TensorFlow Serving能用。除了性能之外，<strong>TensorFlow Serving一个显著的优点是可以轻松的热插拔模型，而不会使服务失效</strong>。</p>\n<h2 id=\"数据加载\">数据加载</h2>\n<p>PyTorch中用于加载数据的API设计的很棒。接口由<strong>一个数据集、一个取样器和一个数据加载器构成</strong>。数据加载器根据取样器的计划，基于数据集产生一个迭代器。<strong>并行化数据加载简单的就像把num_workers参数传递给数据加载器一样简单。</strong></p>\n<p>TensorFlow中没有发现特别有用的数据加载工具。很多时候，并不总能直接把准备并行运行的预处理代码加入TensorFlow图。以及API本身冗长难学。</p>\n<h2 id=\"设备管理\"><strong>设备管理</strong></h2>\n<p>TensorFlow的<strong>设备管理非常好用</strong>。通常你不需要进行调整，因为<strong>默认的设置就很好</strong>。例如，TensorFlow会假设你想运行在GPU上（如果有的话）。而在PyTorch中，即使启用了CUDA，你<strong>也需要明确把一切移入设备</strong>。</p>\n<p>TensorFlow设备管理唯一的缺点是，默认情况下，它会占用所有的GPU显存。简单的解决办法是<strong>指定CUDA_VISIBLE_DEVICES</strong>。有时候大家会忘了这一点，所以GPU在空闲的时候，也会显得很忙。</p>\n<p>在PyTorch中，我发现<strong>代码需要更频繁的检查CUDA是否可用</strong>，以及<strong>更明确的设备管理</strong>。在编写能够同时在CPU和GPU上运行的代码时尤其如此。以及得把GPU上的<strong>PyTorch变量转换为Numpy数组</strong>，这就显得有点冗长。</p>\n<h2 id=\"自定义扩展\"><strong>自定义扩展</strong></h2>\n<p>两个框架都可以构建和绑定用C、C++、CUDA编写的自定义扩展。TensorFlow仍然需要更多的样板代码，尽管这对于支持多类型和设备可能更好。在PyTorch中，你只需为每个CPU和GPU编写一个接口和相应的实现。两个框架中编译扩展也是直接记性，并不需要在pip安装的内容之外下载任何头文件或者源代码。</p>\n<h2 id=\"TensorBoard\">TensorBoard</h2>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/09/01/202309011045600\" alt></p>\n<p><strong>TensorBoard</strong>是TensorFlow自带的可视化工具，用来查看<strong>机器学习训练过程中数据的变化</strong>。通过训练脚本中的几个代码段，你可以查看<strong>任何模型的训练曲线和验证结果</strong>。TensorBoard作为web服务运行，特别便于对于<strong>无头结点上存储的结果进行可视化</strong>。</p>\n<p>第一个是<strong>tensorboard_logger</strong>，第二个是<strong>crayon</strong>。tensorboard_logger库用起来甚至比TensorBoard的“摘要”更容易，尽管想用这个首先得安装TensorBoard。</p>\n<h1>安装Pytorch环境</h1>\n<p>一般建议准从官网给的指导，官网链接：</p>\n<p>安装以前的版本链接如下</p>\n<p><a href=\"https://pytorch.org/get-started/previous-versions/\">PyTorch</a></p>\n<p>比如我这里安装的指令如下：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch==1.12.0+cu102 /</span><br><span class=\"line\">torchvision==0.13.0+cu102 torchaudio==0.12.0 /</span><br><span class=\"line\">--extra-index-url /</span><br><span class=\"line\">https://download.pytorch.org/whl/cu102</span><br></pre></td></tr></table></figure>\n<p>建议根据你本地安装好的CUDA环境来选择版本，否则会出现很多奇怪的异常。</p>\n<p>安装好之后你可以使用以下命令输出torch的版本信息：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">print(torch.__version__)</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n"},{"title":"NVIDIA-Container-Toolkit","date":"2022-08-25T04:33:37.000Z","updated":"2022-08-25T04:33:37.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/robotics-arm.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":null,"_content":"\n# ***NVIDIA-Container-Toolkit笔记整合***\n\nNVIDIA Container Toolkit是支持多种Linux系统发行版本和不同容器引擎的一个工具包。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [线性建模](#线性建模 \"线性建模\")\n>     2.1. [什么是线性模型](#什么是线性模型 \"什么是线性模型\")\n>     2.2. [什么是损失函数](#什么是损失函数 \"什么是损失函数\")\n>     2.3. [对损失函数求偏导](#对损失函数求偏导 \"对损失函数求偏导\")\n>     2.4. [二阶导数的意义图解](#二阶导数的意义图解 \"二阶导数的意义图解\")\n>     2.5. [$w_0$和$w_1$的二阶导数](#$w_0$和$w_1$的二阶导数 \"$w_0$和$w_1$的二阶导数\")\n>3. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n>4. [引用文献](#引用文献 \"引用文献\")\n\n[TOC]\n\n\n\n### 概述\n\n参考官方的解释，NVIDIA Container Toolkit（以下简称NCT）允许用户来建立和运行GPU加速的容器。这个工具包包含了一个容器运行时库（runtime library)和一些共用组件来自动化配置容器，以使得容器可以利用主机的GPU，如图所示的架构：\n\n![5b208976-b632-11e5-8406-38d379ec46aa](https://www.synotech.top:5523/uploads/2023/04/23/202304231942295.png)\n\nNCT同时也支持不同的容器引擎[Docker](https://docs.docker.com/get-started/overview/), [LXC](https://linuxcontainers.org/), [Podman](http://podman.io/) etc.\n\n### 架构图\n\n在使用nvidia-docker wrapper的时候，整个组件的流程图如下所示：\n\n![nvidia-docker-arch-new](https://www.synotech.top:5523/uploads/2023/04/29/202304291433275.png)\n\n\n\n***\n\n### 引用文献：\n","source":"_posts/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit.md","raw":"---\ntitle: NVIDIA-Container-Toolkit\ndate: 2022-08-25 12:33:37\nupdated: 2022-08-25 12:33:37\ntags: [Container-Toolkit, Docker]\ncategories: \n- NVIDIA\n- Container-Toolkit\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax: \n---\n\n# ***NVIDIA-Container-Toolkit笔记整合***\n\nNVIDIA Container Toolkit是支持多种Linux系统发行版本和不同容器引擎的一个工具包。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [线性建模](#线性建模 \"线性建模\")\n>     2.1. [什么是线性模型](#什么是线性模型 \"什么是线性模型\")\n>     2.2. [什么是损失函数](#什么是损失函数 \"什么是损失函数\")\n>     2.3. [对损失函数求偏导](#对损失函数求偏导 \"对损失函数求偏导\")\n>     2.4. [二阶导数的意义图解](#二阶导数的意义图解 \"二阶导数的意义图解\")\n>     2.5. [$w_0$和$w_1$的二阶导数](#$w_0$和$w_1$的二阶导数 \"$w_0$和$w_1$的二阶导数\")\n>3. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n>4. [引用文献](#引用文献 \"引用文献\")\n\n[TOC]\n\n\n\n### 概述\n\n参考官方的解释，NVIDIA Container Toolkit（以下简称NCT）允许用户来建立和运行GPU加速的容器。这个工具包包含了一个容器运行时库（runtime library)和一些共用组件来自动化配置容器，以使得容器可以利用主机的GPU，如图所示的架构：\n\n![5b208976-b632-11e5-8406-38d379ec46aa](https://www.synotech.top:5523/uploads/2023/04/23/202304231942295.png)\n\nNCT同时也支持不同的容器引擎[Docker](https://docs.docker.com/get-started/overview/), [LXC](https://linuxcontainers.org/), [Podman](http://podman.io/) etc.\n\n### 架构图\n\n在使用nvidia-docker wrapper的时候，整个组件的流程图如下所示：\n\n![nvidia-docker-arch-new](https://www.synotech.top:5523/uploads/2023/04/29/202304291433275.png)\n\n\n\n***\n\n### 引用文献：\n","slug":"002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483sr000ju9rlfmgccau7","content":"<h1><em><strong>NVIDIA-Container-Toolkit笔记整合</strong></em></h1>\n<p>NVIDIA Container Toolkit是支持多种Linux系统发行版本和不同容器引擎的一个工具包。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E5%BB%BA%E6%A8%A1\" title=\"线性建模\">线性建模</a><br>\n2.1. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\" title=\"什么是线性模型\">什么是线性模型</a><br>\n2.2. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\" title=\"什么是损失函数\">什么是损失函数</a><br>\n2.3. <a href=\"#%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC\" title=\"对损失函数求偏导\">对损失函数求偏导</a><br>\n2.4. <a href=\"#%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%9B%BE%E8%A7%A3\" title=\"二阶导数的意义图解\">二阶导数的意义图解</a><br>\n2.5. <a href=\"#$w_0$%E5%92%8C$w_1$%E7%9A%84%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0\" title=\"$w_0$和$w_1$的二阶导数\">$w_0$和$w_1$的二阶导数</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a></li>\n</ol>\n</blockquote>\n<p>[TOC]</p>\n<h3 id=\"概述\">概述</h3>\n<p>参考官方的解释，NVIDIA Container Toolkit（以下简称NCT）允许用户来建立和运行GPU加速的容器。这个工具包包含了一个容器运行时库（runtime library)和一些共用组件来自动化配置容器，以使得容器可以利用主机的GPU，如图所示的架构：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/23/202304231942295.png\" alt=\"5b208976-b632-11e5-8406-38d379ec46aa\"></p>\n<p>NCT同时也支持不同的容器引擎<a href=\"https://docs.docker.com/get-started/overview/\">Docker</a>, <a href=\"https://linuxcontainers.org/\">LXC</a>, <a href=\"http://podman.io/\">Podman</a> etc.</p>\n<h3 id=\"架构图\">架构图</h3>\n<p>在使用nvidia-docker wrapper的时候，整个组件的流程图如下所示：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291433275.png\" alt=\"nvidia-docker-arch-new\"></p>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>NVIDIA-Container-Toolkit笔记整合</strong></em></h1>\n<p>NVIDIA Container Toolkit是支持多种Linux系统发行版本和不同容器引擎的一个工具包。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E5%BB%BA%E6%A8%A1\" title=\"线性建模\">线性建模</a><br>\n2.1. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\" title=\"什么是线性模型\">什么是线性模型</a><br>\n2.2. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\" title=\"什么是损失函数\">什么是损失函数</a><br>\n2.3. <a href=\"#%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC\" title=\"对损失函数求偏导\">对损失函数求偏导</a><br>\n2.4. <a href=\"#%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%9B%BE%E8%A7%A3\" title=\"二阶导数的意义图解\">二阶导数的意义图解</a><br>\n2.5. <a href=\"#$w_0$%E5%92%8C$w_1$%E7%9A%84%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0\" title=\"$w_0$和$w_1$的二阶导数\">$w_0$和$w_1$的二阶导数</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a></li>\n</ol>\n</blockquote>\n<p>[TOC]</p>\n<h3 id=\"概述\">概述</h3>\n<p>参考官方的解释，NVIDIA Container Toolkit（以下简称NCT）允许用户来建立和运行GPU加速的容器。这个工具包包含了一个容器运行时库（runtime library)和一些共用组件来自动化配置容器，以使得容器可以利用主机的GPU，如图所示的架构：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/23/202304231942295.png\" alt=\"5b208976-b632-11e5-8406-38d379ec46aa\"></p>\n<p>NCT同时也支持不同的容器引擎<a href=\"https://docs.docker.com/get-started/overview/\">Docker</a>, <a href=\"https://linuxcontainers.org/\">LXC</a>, <a href=\"http://podman.io/\">Podman</a> etc.</p>\n<h3 id=\"架构图\">架构图</h3>\n<p>在使用nvidia-docker wrapper的时候，整个组件的流程图如下所示：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291433275.png\" alt=\"nvidia-docker-arch-new\"></p>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n"},{"title":"ONNX概述","date":"2022-08-25T04:12:50.000Z","updated":"2022-08-25T04:12:50.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/5f17b43d50c03_270_185.png","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":null,"_content":"\n\n\n\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n------\n\n","source":"_posts/002-NVIDIA/02-ONNX/ONNX概述.md","raw":"---\ntitle: ONNX概述\ndate: 2022-08-25 12:12:50\nupdated: 2022-08-25 12:12:50\ntags: ONNX\ncategories:\n- NVIDIA\n- ONNX\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax: \n---\n\n\n\n\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n------\n\n","slug":"002-NVIDIA/02-ONNX/ONNX概述","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483ss000lu9rl24xu1gdu","content":"<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n"},{"title":"TensorRT安装指南","date":"2022-08-24T14:54:49.000Z","updated":"2022-08-24T14:54:49.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/walle.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":null,"_content":"\n## ***TensorRT安装指南***\n\n本笔记主要记录在学习TensorRT官方文档时，一些安装方式，不同的安装方式之间的区别。\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n# 版本\n\n版本发布参考archives，本文以TensorRT8.4.1进行描述:\n\n[Documentation Archives :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html)\n\n参考官方的文档TensorRT8.4.1\n\n[Installation Guide :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian)\n\nTensorRT版本对照表\n\n[Release Notes :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-8.html#rel-8-2-4)\n\n# 安装方式\n\n有这些方法：Using Debian or RPM packages, a pip wheel file, a tar file, or a zip file.\n\n## 离线预编译包安装\n\n使用官网提供的安装包：[__https://developer.nvidia.com/nvidia-tensorrt-8x-download__](https://developer.nvidia.com/nvidia-tensorrt-8x-download)\n\n<img alt=\"02-TensorRT安装指南-6ff62ded.png\" src=\"02-TensorRT安装指南-6ff62ded.png\" width=\"\" height=\"\" >\n**安装包方式注意的点：**\n\n1. 需要sudo权限\n\n1. 不能任意决定TenosrRT的安装位置\n\n1. 需要有同样通过deb/rpm包安装的CUDA及cuDNN\n\n1. 同一个主机只能安装一个小版本的TensorRT安装\n\n**步骤如下：**\n\n- 下载TRT本地repo，需要和你的linux系统版本及CPU架构匹配的版本\n\n- 手动从Debian仓库安装\n\n```shell\nos=\"ubuntuxx04\"\ntag=\"cudax.x-trt8.x.x.x-ga-yyyymmdd\"\n#比如：nv-tensorrt-repo-ubuntu1804-cuda11.6-trt8.4.1.5-ga-20220604_1-1_amd64.deb\n\nsudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb\nsudo apt-key add /var/nv-tensorrt-repo-${os}-${tag}/*.pub\n\nsudo apt-get update\nsudo apt-get install tensorrt\n#使用python3安装\npython3 -m pip install numpy\nsudo apt-get install python3-libnvinfer-dev\n#Tensorflow需要安装\npython3 -m pip install protobuf\nsudo apt-get install uff-converter-tf\n#使用ONNX\npython3 -m pip install numpy onnx\nsudo apt-get install onnx-graphsurgeon\n```\n\n- 验证TensorRT是否安装好了\n\n```shell\ndpkg -l | grep TensorRT\n#应该会输出以下内容：\nii  graphsurgeon-tf\t8.4.1-1+cuda11.6\tamd64\tGraphSurgeon for TensorRT package\nii  libnvinfer-bin\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT binaries\nii  libnvinfer-dev\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT development libraries and headers\nii  libnvinfer-plugin-dev\t8.4.1-1+cuda11.6\tamd64\tTensorRT plugin libraries\nii  libnvinfer-plugin8\t8.4.1-1+cuda11.6\tamd64\tTensorRT plugin libraries\nii  libnvinfer-samples\t8.4.1-1+cuda11.6\tall\tTensorRT samples\nii  libnvinfer8\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT runtime libraries\nii  libnvonnxparsers-dev\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT ONNX libraries\nii  libnvonnxparsers8\t8.4.1-1+cuda11.6\tamd64\tTensorRT ONNX libraries\nii  libnvparsers-dev\t8.4.1-1+cuda11.6\tamd64\tTensorRT parsers libraries\nii  libnvparsers8\t8.4.1-1+cuda11.6\tamd64\tTensorRT parsers libraries\nii  python3-libnvinfer\t8.4.1-1+cuda11.6\tamd64\tPython 3 bindings for TensorRT\nii  python3-libnvinfer-dev\t8.4.1-1+cuda11.6\tamd64\tPython 3 development package for TensorRT\nii  tensorrt\t\t8.4.1.x-1+cuda11.6 \tamd64\tMeta package of TensorRT\nii  uff-converter-tf\t8.4.1-1+cuda11.6\tamd64\tUFF converter for TensorRT package\nii  onnx-graphsurgeon   8.4.1-1+cuda11.6  amd64 ONNX GraphSurgeon for TensorRT package\n\n```\n\n\n\n## 在线网络编译包安装\n\n- **安装CUDA network repository**，如图所示，进入[__https://developer.nvidia.com/cuda-downloads__](https://developer.nvidia.com/cuda-downloads)，这种方式适用于已经很熟TensorRT，只想快速把应用的依赖配置起来，比如，在使用容器的时候，新手适合上面的方式。\n\n<img alt=\"02-TensorRT安装指南-a2f8136f.png\" src=\"/02-TensorRT安装指南-a2f8136f.png\" width=\"\" height=\"\" >\n\n比如：我这里选择了，Ubuntu 18.04 x86_64 deb(network)\n\n```shell\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\n#如果你已经有安装号的CUDA toolkit,则可以省略下面的两步\nsudo apt-get update\nsudo apt-get -y install cuda\n```\n\n执行上面提供的命令，即可安装CUDA在线版本，如果你本地已经安装好了CUDA，则可以省略apt install的指令，因为当下面安装TensorRT时，apt会自动下载所需要的CUDA和cuDNN依赖\n\n- 安装TensorRT包，根据自己需要选择下面的指令\n\n```shell\n# For only running TensorRT C++ applications\nsudo apt-get install tensorrt-libs\n# For also building TensorRT C++ applications:\nsudo apt-get install tensorrt-dev\n# For running TensorRT Python applications:\npython3 -m pip install numpy\nsudo apt-get install python3-libnvinfer\n\n```\n\n- 当第一步骤中，执行CUDA network repository的安装，Ubuntu会自动安装TensorRT对应最新版本的CUDA，下面的指令可以安装老版本CUDA对应着你现在本机已经安装的TensorRT版本。\n\n```shell\nversion=\"8.x.x.x-1+cudax.x\"\nsudo apt-get install tensorrt-dev=${version}\n\nsudo apt-mark hold tensorrt-dev\n如果需要升级到最新的TensorRT或者CUDA，执行下面的\nsudo apt-mark unhold tensorrt-dev\n\n```\n\n\n\n## APP Server安装（适用于生产部署）\n\n```shell\n使用apt-get安装Debian包即可\nthe libnvinfer8 package (C++) plus any additional library packages\nthe python3-libnvinfer package (Python 3.x)\n\n```\n\n\n\n## 交叉编译安装\n\n参考链接：  \n\n[Sample Support Guide :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#cross-compiling)\n\n暂时不更新这一部分。\n\n\n\n## Pip安装\n\n注意：**While the TensorRT packages also contain pip wheel files, those wheel files require the rest of the .deb or .rpm packages to be installed and will not work alone**（意思就是，TensorRT包中也包含了pip wheels, 但是这些wheels即使你使用了Pip安装，还是需要安装tensorRT的deb/rpm包，pip包无法单独运行）。我们需要的是，**standalone pip-installable TensorRT wheel files** ，这些pip包是可以不需要先安装的TensorRT deb/rpm包的。\n\n**The pip-installable nvidia-tensorrt Python wheel files only support Python versions 3.6 to 3.10 and CUDA 11.x at this time and will not work with other Python or CUDA versions.**\n\n同时，这些包只能支持 **python3.6 - 3.10及 CUDA11.x**的版本，其他的版本都不行，也只能运行在Linux x86_64的系统上（CentOS7以及Ubuntu18以上的最新版本）。\n\n- 确保nvidia-pyindex包已经安装，这个使用用来从NGC PyPI repo中获取其他附加Python模块的。(需要确保，你的pip 和setuptools不能太旧，过时，否则可能安装失败）\n\n```shell\npython3 -m pip install --upgrade setuptools pip\npip install nvidia-pyindex\n#可以添加下面的依赖到你项目的requirements.txt\n--extra-index-url https://pypi.ngc.nvidia.com\n```\n\n- 安装TensorRT pip包\n\n```shell\npython3 -m pip install --upgrade nvidia-tensorrt\n#这个安装命令，将会同时安装依赖的CUDA和cuDNN pip包，因为这些都是TensorRT的依赖\n如果安装的时候报错有类似下面的信息，那就是python版本不对/nvidia-pyindex没有安装好\n##################################################################\nThe package you are trying to install is only a placeholder project on PyPI.org repository.\nThis package is hosted on NVIDIA Python Package Index.\n\nThis package can be installed as:\n$ pip install nvidia-pyindex\n$ pip install nvidia-tensorrt\n##################################################################\n```\n\n- 验证是否安装好\n\n```shell\npython3\n>>> import tensorrt\n>>> print(tensorrt.__version__)\n>>> assert tensorrt.Builder(tensorrt.Logger())\n如果最后这个python指令报错，类似下面的输出信息，那么可能是没有安装好显卡驱动\n[TensorRT] ERROR: CUDA initialization failure with error 100. Please check your CUDA installation: ...\n\n```\n\n\n\n## Tar包安装\n\n1. 安装好你本机需要的CUDA（[__10.2__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.0 update 1__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.1 update 1__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.2 update 2__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.3 update 1__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.4 update 4__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.5 update 2__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.6 update 2__](https://developer.nvidia.com/cuda-toolkit-archive) or [__11.7__](https://developer.nvidia.com/cuda-toolkit-archive)）\n\n1. 安装好[__cuDNN 8.4.1__](https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-841)\n\n1. 安装好python3(可选）\n\n1. 下载TensorRT Tar包[Download](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#downloading \"Ensure you are a member of the NVIDIA Developer Program. If not, follow the prompts to gain access.\")\n\n1. 解压文件\n\n```shell\nversion=\"8.x.x.x\"\narch=$(uname -m)\ncuda=\"cuda-x.x\"\ncudnn=\"cudnn8.x\"\ntar -xzvf TensorRT-${version}.Linux.${arch}-gnu.${cuda}.${cudnn}.tar.gz\n#\n8.x.x.x is your TensorRT version\ncuda-x.x is CUDA version 10.2 or 11.6\ncudnn8.x is cuDNN version 8.4\n\n```\n\n1. 添加TensorRT lib文件夹的绝对路径到环境变量中\n\n```shell\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<TensorRT-${version}/lib>\n```\n\n1. 安装python TensorRT wheel包\n\n```shell\ncd TensorRT-${version}/python\npython3 -m pip install tensorrt-*-cp3x-none-linux_x86_64.whl\n```\n\n1. 安装 Python UFF wheel 包\n\n```shell\ncd TensorRT-${version}/uff\n\npython3 -m pip install uff-0.6.9-py2.py3-none-any.whl\n```\n\n1. 安装Python onnx-graphsurgeon /graphsurgeon  wheel 包\n\n```shell\ncd TensorRT-${version}/onnx_graphsurgeon\npython3 -m pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl\ncd TensorRT-${version}/graphsurgeon\npython3 -m pip install graphsurgeon-0.4.6-py2.py3-none-any.whl\n```\n\n\n\n## ZIP包安装（适用于windows)\n\n参考官方[__https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip__](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip)\n\n\n\n# 卸载方式\n\n1. 卸载libnvinfer8，通过deb包安装的\n1. 卸载uff-converter-tf, graphsurgeon-tf, and onnx-graphsurgeon\n\n```shell\nsudo apt-get purge graphsurgeon-tf onnx-graphsurgeon\nsudo apt-get purge uff-converter-tf #不卸载graphsurgeon-tf也可以\nsudo apt-get autoremove\n```\n\n3. 卸载Python TensorRT\n\n```shell\nsudo pip3 uninstall tensorrt\n```\n\n4. 卸载Python UFF\n\n```shell\nsudo pip3 uninstall uff\n```\n\n5. 卸载Python GraphSurgeon\n\n```shell\nsudo pip3 uninstall graphsurgeon\n```\n\n6. 卸载Python ONNX GraphSurgeon\n\n```shell\nsudo pip3 uninstall onnx-graphsurgeon\n```\n\n\n\n# PyCUDA\n\n确保安装了numpy\n\n```shell\npython3 -m pip install numpy\npython3 -m pip install 'pycuda<2021.1'\n```\n","source":"_posts/002-NVIDIA/03-TensorRT/TensorRT安装指南.md","raw":"---\ntitle: TensorRT安装指南\ndate: 2022-08-24 22:54:49\nupdated: 2022-08-24 22:54:49\ntags: TensorRT\ncategories: \n- NVIDIA\n- TensorRT\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax:\n---\n\n## ***TensorRT安装指南***\n\n本笔记主要记录在学习TensorRT官方文档时，一些安装方式，不同的安装方式之间的区别。\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n# 版本\n\n版本发布参考archives，本文以TensorRT8.4.1进行描述:\n\n[Documentation Archives :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html)\n\n参考官方的文档TensorRT8.4.1\n\n[Installation Guide :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian)\n\nTensorRT版本对照表\n\n[Release Notes :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-8.html#rel-8-2-4)\n\n# 安装方式\n\n有这些方法：Using Debian or RPM packages, a pip wheel file, a tar file, or a zip file.\n\n## 离线预编译包安装\n\n使用官网提供的安装包：[__https://developer.nvidia.com/nvidia-tensorrt-8x-download__](https://developer.nvidia.com/nvidia-tensorrt-8x-download)\n\n<img alt=\"02-TensorRT安装指南-6ff62ded.png\" src=\"02-TensorRT安装指南-6ff62ded.png\" width=\"\" height=\"\" >\n**安装包方式注意的点：**\n\n1. 需要sudo权限\n\n1. 不能任意决定TenosrRT的安装位置\n\n1. 需要有同样通过deb/rpm包安装的CUDA及cuDNN\n\n1. 同一个主机只能安装一个小版本的TensorRT安装\n\n**步骤如下：**\n\n- 下载TRT本地repo，需要和你的linux系统版本及CPU架构匹配的版本\n\n- 手动从Debian仓库安装\n\n```shell\nos=\"ubuntuxx04\"\ntag=\"cudax.x-trt8.x.x.x-ga-yyyymmdd\"\n#比如：nv-tensorrt-repo-ubuntu1804-cuda11.6-trt8.4.1.5-ga-20220604_1-1_amd64.deb\n\nsudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb\nsudo apt-key add /var/nv-tensorrt-repo-${os}-${tag}/*.pub\n\nsudo apt-get update\nsudo apt-get install tensorrt\n#使用python3安装\npython3 -m pip install numpy\nsudo apt-get install python3-libnvinfer-dev\n#Tensorflow需要安装\npython3 -m pip install protobuf\nsudo apt-get install uff-converter-tf\n#使用ONNX\npython3 -m pip install numpy onnx\nsudo apt-get install onnx-graphsurgeon\n```\n\n- 验证TensorRT是否安装好了\n\n```shell\ndpkg -l | grep TensorRT\n#应该会输出以下内容：\nii  graphsurgeon-tf\t8.4.1-1+cuda11.6\tamd64\tGraphSurgeon for TensorRT package\nii  libnvinfer-bin\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT binaries\nii  libnvinfer-dev\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT development libraries and headers\nii  libnvinfer-plugin-dev\t8.4.1-1+cuda11.6\tamd64\tTensorRT plugin libraries\nii  libnvinfer-plugin8\t8.4.1-1+cuda11.6\tamd64\tTensorRT plugin libraries\nii  libnvinfer-samples\t8.4.1-1+cuda11.6\tall\tTensorRT samples\nii  libnvinfer8\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT runtime libraries\nii  libnvonnxparsers-dev\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT ONNX libraries\nii  libnvonnxparsers8\t8.4.1-1+cuda11.6\tamd64\tTensorRT ONNX libraries\nii  libnvparsers-dev\t8.4.1-1+cuda11.6\tamd64\tTensorRT parsers libraries\nii  libnvparsers8\t8.4.1-1+cuda11.6\tamd64\tTensorRT parsers libraries\nii  python3-libnvinfer\t8.4.1-1+cuda11.6\tamd64\tPython 3 bindings for TensorRT\nii  python3-libnvinfer-dev\t8.4.1-1+cuda11.6\tamd64\tPython 3 development package for TensorRT\nii  tensorrt\t\t8.4.1.x-1+cuda11.6 \tamd64\tMeta package of TensorRT\nii  uff-converter-tf\t8.4.1-1+cuda11.6\tamd64\tUFF converter for TensorRT package\nii  onnx-graphsurgeon   8.4.1-1+cuda11.6  amd64 ONNX GraphSurgeon for TensorRT package\n\n```\n\n\n\n## 在线网络编译包安装\n\n- **安装CUDA network repository**，如图所示，进入[__https://developer.nvidia.com/cuda-downloads__](https://developer.nvidia.com/cuda-downloads)，这种方式适用于已经很熟TensorRT，只想快速把应用的依赖配置起来，比如，在使用容器的时候，新手适合上面的方式。\n\n<img alt=\"02-TensorRT安装指南-a2f8136f.png\" src=\"/02-TensorRT安装指南-a2f8136f.png\" width=\"\" height=\"\" >\n\n比如：我这里选择了，Ubuntu 18.04 x86_64 deb(network)\n\n```shell\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\n#如果你已经有安装号的CUDA toolkit,则可以省略下面的两步\nsudo apt-get update\nsudo apt-get -y install cuda\n```\n\n执行上面提供的命令，即可安装CUDA在线版本，如果你本地已经安装好了CUDA，则可以省略apt install的指令，因为当下面安装TensorRT时，apt会自动下载所需要的CUDA和cuDNN依赖\n\n- 安装TensorRT包，根据自己需要选择下面的指令\n\n```shell\n# For only running TensorRT C++ applications\nsudo apt-get install tensorrt-libs\n# For also building TensorRT C++ applications:\nsudo apt-get install tensorrt-dev\n# For running TensorRT Python applications:\npython3 -m pip install numpy\nsudo apt-get install python3-libnvinfer\n\n```\n\n- 当第一步骤中，执行CUDA network repository的安装，Ubuntu会自动安装TensorRT对应最新版本的CUDA，下面的指令可以安装老版本CUDA对应着你现在本机已经安装的TensorRT版本。\n\n```shell\nversion=\"8.x.x.x-1+cudax.x\"\nsudo apt-get install tensorrt-dev=${version}\n\nsudo apt-mark hold tensorrt-dev\n如果需要升级到最新的TensorRT或者CUDA，执行下面的\nsudo apt-mark unhold tensorrt-dev\n\n```\n\n\n\n## APP Server安装（适用于生产部署）\n\n```shell\n使用apt-get安装Debian包即可\nthe libnvinfer8 package (C++) plus any additional library packages\nthe python3-libnvinfer package (Python 3.x)\n\n```\n\n\n\n## 交叉编译安装\n\n参考链接：  \n\n[Sample Support Guide :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#cross-compiling)\n\n暂时不更新这一部分。\n\n\n\n## Pip安装\n\n注意：**While the TensorRT packages also contain pip wheel files, those wheel files require the rest of the .deb or .rpm packages to be installed and will not work alone**（意思就是，TensorRT包中也包含了pip wheels, 但是这些wheels即使你使用了Pip安装，还是需要安装tensorRT的deb/rpm包，pip包无法单独运行）。我们需要的是，**standalone pip-installable TensorRT wheel files** ，这些pip包是可以不需要先安装的TensorRT deb/rpm包的。\n\n**The pip-installable nvidia-tensorrt Python wheel files only support Python versions 3.6 to 3.10 and CUDA 11.x at this time and will not work with other Python or CUDA versions.**\n\n同时，这些包只能支持 **python3.6 - 3.10及 CUDA11.x**的版本，其他的版本都不行，也只能运行在Linux x86_64的系统上（CentOS7以及Ubuntu18以上的最新版本）。\n\n- 确保nvidia-pyindex包已经安装，这个使用用来从NGC PyPI repo中获取其他附加Python模块的。(需要确保，你的pip 和setuptools不能太旧，过时，否则可能安装失败）\n\n```shell\npython3 -m pip install --upgrade setuptools pip\npip install nvidia-pyindex\n#可以添加下面的依赖到你项目的requirements.txt\n--extra-index-url https://pypi.ngc.nvidia.com\n```\n\n- 安装TensorRT pip包\n\n```shell\npython3 -m pip install --upgrade nvidia-tensorrt\n#这个安装命令，将会同时安装依赖的CUDA和cuDNN pip包，因为这些都是TensorRT的依赖\n如果安装的时候报错有类似下面的信息，那就是python版本不对/nvidia-pyindex没有安装好\n##################################################################\nThe package you are trying to install is only a placeholder project on PyPI.org repository.\nThis package is hosted on NVIDIA Python Package Index.\n\nThis package can be installed as:\n$ pip install nvidia-pyindex\n$ pip install nvidia-tensorrt\n##################################################################\n```\n\n- 验证是否安装好\n\n```shell\npython3\n>>> import tensorrt\n>>> print(tensorrt.__version__)\n>>> assert tensorrt.Builder(tensorrt.Logger())\n如果最后这个python指令报错，类似下面的输出信息，那么可能是没有安装好显卡驱动\n[TensorRT] ERROR: CUDA initialization failure with error 100. Please check your CUDA installation: ...\n\n```\n\n\n\n## Tar包安装\n\n1. 安装好你本机需要的CUDA（[__10.2__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.0 update 1__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.1 update 1__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.2 update 2__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.3 update 1__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.4 update 4__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.5 update 2__](https://developer.nvidia.com/cuda-toolkit-archive), [__11.6 update 2__](https://developer.nvidia.com/cuda-toolkit-archive) or [__11.7__](https://developer.nvidia.com/cuda-toolkit-archive)）\n\n1. 安装好[__cuDNN 8.4.1__](https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-841)\n\n1. 安装好python3(可选）\n\n1. 下载TensorRT Tar包[Download](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#downloading \"Ensure you are a member of the NVIDIA Developer Program. If not, follow the prompts to gain access.\")\n\n1. 解压文件\n\n```shell\nversion=\"8.x.x.x\"\narch=$(uname -m)\ncuda=\"cuda-x.x\"\ncudnn=\"cudnn8.x\"\ntar -xzvf TensorRT-${version}.Linux.${arch}-gnu.${cuda}.${cudnn}.tar.gz\n#\n8.x.x.x is your TensorRT version\ncuda-x.x is CUDA version 10.2 or 11.6\ncudnn8.x is cuDNN version 8.4\n\n```\n\n1. 添加TensorRT lib文件夹的绝对路径到环境变量中\n\n```shell\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<TensorRT-${version}/lib>\n```\n\n1. 安装python TensorRT wheel包\n\n```shell\ncd TensorRT-${version}/python\npython3 -m pip install tensorrt-*-cp3x-none-linux_x86_64.whl\n```\n\n1. 安装 Python UFF wheel 包\n\n```shell\ncd TensorRT-${version}/uff\n\npython3 -m pip install uff-0.6.9-py2.py3-none-any.whl\n```\n\n1. 安装Python onnx-graphsurgeon /graphsurgeon  wheel 包\n\n```shell\ncd TensorRT-${version}/onnx_graphsurgeon\npython3 -m pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl\ncd TensorRT-${version}/graphsurgeon\npython3 -m pip install graphsurgeon-0.4.6-py2.py3-none-any.whl\n```\n\n\n\n## ZIP包安装（适用于windows)\n\n参考官方[__https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip__](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip)\n\n\n\n# 卸载方式\n\n1. 卸载libnvinfer8，通过deb包安装的\n1. 卸载uff-converter-tf, graphsurgeon-tf, and onnx-graphsurgeon\n\n```shell\nsudo apt-get purge graphsurgeon-tf onnx-graphsurgeon\nsudo apt-get purge uff-converter-tf #不卸载graphsurgeon-tf也可以\nsudo apt-get autoremove\n```\n\n3. 卸载Python TensorRT\n\n```shell\nsudo pip3 uninstall tensorrt\n```\n\n4. 卸载Python UFF\n\n```shell\nsudo pip3 uninstall uff\n```\n\n5. 卸载Python GraphSurgeon\n\n```shell\nsudo pip3 uninstall graphsurgeon\n```\n\n6. 卸载Python ONNX GraphSurgeon\n\n```shell\nsudo pip3 uninstall onnx-graphsurgeon\n```\n\n\n\n# PyCUDA\n\n确保安装了numpy\n\n```shell\npython3 -m pip install numpy\npython3 -m pip install 'pycuda<2021.1'\n```\n","slug":"002-NVIDIA/03-TensorRT/TensorRT安装指南","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483su000pu9rlf9auaj16","content":"<h2 id=\"TensorRT安装指南\"><em><strong>TensorRT安装指南</strong></em></h2>\n<p>本笔记主要记录在学习TensorRT官方文档时，一些安装方式，不同的安装方式之间的区别。</p>\n<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h1>版本</h1>\n<p>版本发布参考archives，本文以TensorRT8.4.1进行描述:</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html\">Documentation Archives :: NVIDIA Deep Learning TensorRT Documentation</a></p>\n<p>参考官方的文档TensorRT8.4.1</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian\">Installation Guide :: NVIDIA Deep Learning TensorRT Documentation</a></p>\n<p>TensorRT版本对照表</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-8.html#rel-8-2-4\">Release Notes :: NVIDIA Deep Learning TensorRT Documentation</a></p>\n<h1>安装方式</h1>\n<p>有这些方法：Using Debian or RPM packages, a pip wheel file, a tar file, or a zip file.</p>\n<h2 id=\"离线预编译包安装\">离线预编译包安装</h2>\n<p>使用官网提供的安装包：<a href=\"https://developer.nvidia.com/nvidia-tensorrt-8x-download\"><strong>https://developer.nvidia.com/nvidia-tensorrt-8x-download</strong></a></p>\n<img alt=\"02-TensorRT安装指南-6ff62ded.png\" src=\"/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/02-TensorRT安装指南-6ff62ded.png\" width height>\n**安装包方式注意的点：**\n<ol>\n<li>\n<p>需要sudo权限</p>\n</li>\n<li>\n<p>不能任意决定TenosrRT的安装位置</p>\n</li>\n<li>\n<p>需要有同样通过deb/rpm包安装的CUDA及cuDNN</p>\n</li>\n<li>\n<p>同一个主机只能安装一个小版本的TensorRT安装</p>\n</li>\n</ol>\n<p><strong>步骤如下：</strong></p>\n<ul>\n<li>\n<p>下载TRT本地repo，需要和你的linux系统版本及CPU架构匹配的版本</p>\n</li>\n<li>\n<p>手动从Debian仓库安装</p>\n</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">os=&quot;ubuntuxx04&quot;</span><br><span class=\"line\">tag=&quot;cudax.x-trt8.x.x.x-ga-yyyymmdd&quot;</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">比如：nv-tensorrt-repo-ubuntu1804-cuda11.6-trt8.4.1.5-ga-20220604_1-1_amd64.deb</span></span><br><span class=\"line\"></span><br><span class=\"line\">sudo dpkg -i nv-tensorrt-repo-$&#123;os&#125;-$&#123;tag&#125;_1-1_amd64.deb</span><br><span class=\"line\">sudo apt-key add /var/nv-tensorrt-repo-$&#123;os&#125;-$&#123;tag&#125;/*.pub</span><br><span class=\"line\"></span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get install tensorrt</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">使用python3安装</span></span><br><span class=\"line\">python3 -m pip install numpy</span><br><span class=\"line\">sudo apt-get install python3-libnvinfer-dev</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">Tensorflow需要安装</span></span><br><span class=\"line\">python3 -m pip install protobuf</span><br><span class=\"line\">sudo apt-get install uff-converter-tf</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">使用ONNX</span></span><br><span class=\"line\">python3 -m pip install numpy onnx</span><br><span class=\"line\">sudo apt-get install onnx-graphsurgeon</span><br></pre></td></tr></table></figure>\n<ul>\n<li>验证TensorRT是否安装好了</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dpkg -l | grep TensorRT</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">应该会输出以下内容：</span></span><br><span class=\"line\">ii  graphsurgeon-tf\t8.4.1-1+cuda11.6\tamd64\tGraphSurgeon for TensorRT package</span><br><span class=\"line\">ii  libnvinfer-bin\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT binaries</span><br><span class=\"line\">ii  libnvinfer-dev\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT development libraries and headers</span><br><span class=\"line\">ii  libnvinfer-plugin-dev\t8.4.1-1+cuda11.6\tamd64\tTensorRT plugin libraries</span><br><span class=\"line\">ii  libnvinfer-plugin8\t8.4.1-1+cuda11.6\tamd64\tTensorRT plugin libraries</span><br><span class=\"line\">ii  libnvinfer-samples\t8.4.1-1+cuda11.6\tall\tTensorRT samples</span><br><span class=\"line\">ii  libnvinfer8\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT runtime libraries</span><br><span class=\"line\">ii  libnvonnxparsers-dev\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT ONNX libraries</span><br><span class=\"line\">ii  libnvonnxparsers8\t8.4.1-1+cuda11.6\tamd64\tTensorRT ONNX libraries</span><br><span class=\"line\">ii  libnvparsers-dev\t8.4.1-1+cuda11.6\tamd64\tTensorRT parsers libraries</span><br><span class=\"line\">ii  libnvparsers8\t8.4.1-1+cuda11.6\tamd64\tTensorRT parsers libraries</span><br><span class=\"line\">ii  python3-libnvinfer\t8.4.1-1+cuda11.6\tamd64\tPython 3 bindings for TensorRT</span><br><span class=\"line\">ii  python3-libnvinfer-dev\t8.4.1-1+cuda11.6\tamd64\tPython 3 development package for TensorRT</span><br><span class=\"line\">ii  tensorrt\t\t8.4.1.x-1+cuda11.6 \tamd64\tMeta package of TensorRT</span><br><span class=\"line\">ii  uff-converter-tf\t8.4.1-1+cuda11.6\tamd64\tUFF converter for TensorRT package</span><br><span class=\"line\">ii  onnx-graphsurgeon   8.4.1-1+cuda11.6  amd64 ONNX GraphSurgeon for TensorRT package</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"在线网络编译包安装\">在线网络编译包安装</h2>\n<ul>\n<li><strong>安装CUDA network repository</strong>，如图所示，进入<a href=\"https://developer.nvidia.com/cuda-downloads\"><strong>https://developer.nvidia.com/cuda-downloads</strong></a>，这种方式适用于已经很熟TensorRT，只想快速把应用的依赖配置起来，比如，在使用容器的时候，新手适合上面的方式。</li>\n</ul>\n<img alt=\"02-TensorRT安装指南-a2f8136f.png\" src=\"/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/02-TensorRT安装指南-a2f8136f.png\" width height>\n<p>比如：我这里选择了，Ubuntu 18.04 x86_64 deb(network)</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb</span><br><span class=\"line\">sudo dpkg -i cuda-keyring_1.0-1_all.deb</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">如果你已经有安装号的CUDA toolkit,则可以省略下面的两步</span></span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get -y install cuda</span><br></pre></td></tr></table></figure>\n<p>执行上面提供的命令，即可安装CUDA在线版本，如果你本地已经安装好了CUDA，则可以省略apt install的指令，因为当下面安装TensorRT时，apt会自动下载所需要的CUDA和cuDNN依赖</p>\n<ul>\n<li>安装TensorRT包，根据自己需要选择下面的指令</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">For only running TensorRT C++ applications</span></span><br><span class=\"line\">sudo apt-get install tensorrt-libs</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">For also building TensorRT C++ applications:</span></span><br><span class=\"line\">sudo apt-get install tensorrt-dev</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">For running TensorRT Python applications:</span></span><br><span class=\"line\">python3 -m pip install numpy</span><br><span class=\"line\">sudo apt-get install python3-libnvinfer</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>当第一步骤中，执行CUDA network repository的安装，Ubuntu会自动安装TensorRT对应最新版本的CUDA，下面的指令可以安装老版本CUDA对应着你现在本机已经安装的TensorRT版本。</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version=&quot;8.x.x.x-1+cudax.x&quot;</span><br><span class=\"line\">sudo apt-get install tensorrt-dev=$&#123;version&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">sudo apt-mark hold tensorrt-dev</span><br><span class=\"line\">如果需要升级到最新的TensorRT或者CUDA，执行下面的</span><br><span class=\"line\">sudo apt-mark unhold tensorrt-dev</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"APP-Server安装（适用于生产部署）\">APP Server安装（适用于生产部署）</h2>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">使用apt-get安装Debian包即可</span><br><span class=\"line\">the libnvinfer8 package (C++) plus any additional library packages</span><br><span class=\"line\">the python3-libnvinfer package (Python 3.x)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"交叉编译安装\">交叉编译安装</h2>\n<p>参考链接：</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#cross-compiling\">Sample Support Guide :: NVIDIA Deep Learning TensorRT Documentation</a></p>\n<p>暂时不更新这一部分。</p>\n<h2 id=\"Pip安装\">Pip安装</h2>\n<p>注意：<strong>While the TensorRT packages also contain pip wheel files, those wheel files require the rest of the .deb or .rpm packages to be installed and will not work alone</strong>（意思就是，TensorRT包中也包含了pip wheels, 但是这些wheels即使你使用了Pip安装，还是需要安装tensorRT的deb/rpm包，pip包无法单独运行）。我们需要的是，<strong>standalone pip-installable TensorRT wheel files</strong> ，这些pip包是可以不需要先安装的TensorRT deb/rpm包的。</p>\n<p><strong>The pip-installable nvidia-tensorrt Python wheel files only support Python versions 3.6 to 3.10 and CUDA 11.x at this time and will not work with other Python or CUDA versions.</strong></p>\n<p>同时，这些包只能支持 <strong>python3.6 - 3.10及 CUDA11.x</strong>的版本，其他的版本都不行，也只能运行在Linux x86_64的系统上（CentOS7以及Ubuntu18以上的最新版本）。</p>\n<ul>\n<li>确保nvidia-pyindex包已经安装，这个使用用来从NGC PyPI repo中获取其他附加Python模块的。(需要确保，你的pip 和setuptools不能太旧，过时，否则可能安装失败）</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install --upgrade setuptools pip</span><br><span class=\"line\">pip install nvidia-pyindex</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">可以添加下面的依赖到你项目的requirements.txt</span></span><br><span class=\"line\">--extra-index-url https://pypi.ngc.nvidia.com</span><br></pre></td></tr></table></figure>\n<ul>\n<li>安装TensorRT pip包</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install --upgrade nvidia-tensorrt</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">这个安装命令，将会同时安装依赖的CUDA和cuDNN pip包，因为这些都是TensorRT的依赖</span></span><br><span class=\"line\">如果安装的时候报错有类似下面的信息，那就是python版本不对/nvidia-pyindex没有安装好</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#################################################################</span></span></span><br><span class=\"line\">The package you are trying to install is only a placeholder project on PyPI.org repository.</span><br><span class=\"line\">This package is hosted on NVIDIA Python Package Index.</span><br><span class=\"line\"></span><br><span class=\"line\">This package can be installed as:</span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">pip install nvidia-pyindex</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">pip install nvidia-tensorrt</span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#################################################################</span></span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>验证是否安装好</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3</span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; import tensorrt</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; <span class=\"built_in\">print</span>(tensorrt.__version__)</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; assert tensorrt.Builder(tensorrt.Logger())</span></span><br><span class=\"line\">如果最后这个python指令报错，类似下面的输出信息，那么可能是没有安装好显卡驱动</span><br><span class=\"line\">[TensorRT] ERROR: CUDA initialization failure with error 100. Please check your CUDA installation: ...</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"Tar包安装\">Tar包安装</h2>\n<ol>\n<li>\n<p>安装好你本机需要的CUDA（<a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>10.2</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.0 update 1</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.1 update 1</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.2 update 2</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.3 update 1</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.4 update 4</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.5 update 2</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.6 update 2</strong></a> or <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.7</strong></a>）</p>\n</li>\n<li>\n<p>安装好<a href=\"https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-841\"><strong>cuDNN 8.4.1</strong></a></p>\n</li>\n<li>\n<p>安装好python3(可选）</p>\n</li>\n<li>\n<p>下载TensorRT Tar包<a href=\"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#downloading\" title=\"Ensure you are a member of the NVIDIA Developer Program. If not, follow the prompts to gain access.\">Download</a></p>\n</li>\n<li>\n<p>解压文件</p>\n</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version=&quot;8.x.x.x&quot;</span><br><span class=\"line\">arch=$(uname -m)</span><br><span class=\"line\">cuda=&quot;cuda-x.x&quot;</span><br><span class=\"line\">cudnn=&quot;cudnn8.x&quot;</span><br><span class=\"line\">tar -xzvf TensorRT-$&#123;version&#125;.Linux.$&#123;arch&#125;-gnu.$&#123;cuda&#125;.$&#123;cudnn&#125;.tar.gz</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"></span></span><br><span class=\"line\"><span class=\"language-bash\">8.x.x.x is your TensorRT version</span></span><br><span class=\"line\">cuda-x.x is CUDA version 10.2 or 11.6</span><br><span class=\"line\">cudnn8.x is cuDNN version 8.4</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ol>\n<li>添加TensorRT lib文件夹的绝对路径到环境变量中</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;TensorRT-$&#123;version&#125;/lib&gt;</span><br></pre></td></tr></table></figure>\n<ol>\n<li>安装python TensorRT wheel包</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd TensorRT-$&#123;version&#125;/python</span><br><span class=\"line\">python3 -m pip install tensorrt-*-cp3x-none-linux_x86_64.whl</span><br></pre></td></tr></table></figure>\n<ol>\n<li>安装 Python UFF wheel 包</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd TensorRT-$&#123;version&#125;/uff</span><br><span class=\"line\"></span><br><span class=\"line\">python3 -m pip install uff-0.6.9-py2.py3-none-any.whl</span><br></pre></td></tr></table></figure>\n<ol>\n<li>安装Python onnx-graphsurgeon /graphsurgeon  wheel 包</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd TensorRT-$&#123;version&#125;/onnx_graphsurgeon</span><br><span class=\"line\">python3 -m pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl</span><br><span class=\"line\">cd TensorRT-$&#123;version&#125;/graphsurgeon</span><br><span class=\"line\">python3 -m pip install graphsurgeon-0.4.6-py2.py3-none-any.whl</span><br></pre></td></tr></table></figure>\n<h2 id=\"ZIP包安装（适用于windows\">ZIP包安装（适用于windows)</h2>\n<p>参考官方<a href=\"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip\"><strong>https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip</strong></a></p>\n<h1>卸载方式</h1>\n<ol>\n<li>卸载libnvinfer8，通过deb包安装的</li>\n<li>卸载uff-converter-tf, graphsurgeon-tf, and onnx-graphsurgeon</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get purge graphsurgeon-tf onnx-graphsurgeon</span><br><span class=\"line\">sudo apt-get purge uff-converter-tf #不卸载graphsurgeon-tf也可以</span><br><span class=\"line\">sudo apt-get autoremove</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>卸载Python TensorRT</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip3 uninstall tensorrt</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>卸载Python UFF</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip3 uninstall uff</span><br></pre></td></tr></table></figure>\n<ol start=\"5\">\n<li>卸载Python GraphSurgeon</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip3 uninstall graphsurgeon</span><br></pre></td></tr></table></figure>\n<ol start=\"6\">\n<li>卸载Python ONNX GraphSurgeon</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip3 uninstall onnx-graphsurgeon</span><br></pre></td></tr></table></figure>\n<h1>PyCUDA</h1>\n<p>确保安装了numpy</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install numpy</span><br><span class=\"line\">python3 -m pip install &#x27;pycuda&lt;2021.1&#x27;</span><br></pre></td></tr></table></figure>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h2 id=\"TensorRT安装指南\"><em><strong>TensorRT安装指南</strong></em></h2>\n<p>本笔记主要记录在学习TensorRT官方文档时，一些安装方式，不同的安装方式之间的区别。</p>\n<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h1>版本</h1>\n<p>版本发布参考archives，本文以TensorRT8.4.1进行描述:</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html\">Documentation Archives :: NVIDIA Deep Learning TensorRT Documentation</a></p>\n<p>参考官方的文档TensorRT8.4.1</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian\">Installation Guide :: NVIDIA Deep Learning TensorRT Documentation</a></p>\n<p>TensorRT版本对照表</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-8.html#rel-8-2-4\">Release Notes :: NVIDIA Deep Learning TensorRT Documentation</a></p>\n<h1>安装方式</h1>\n<p>有这些方法：Using Debian or RPM packages, a pip wheel file, a tar file, or a zip file.</p>\n<h2 id=\"离线预编译包安装\">离线预编译包安装</h2>\n<p>使用官网提供的安装包：<a href=\"https://developer.nvidia.com/nvidia-tensorrt-8x-download\"><strong>https://developer.nvidia.com/nvidia-tensorrt-8x-download</strong></a></p>\n<img alt=\"02-TensorRT安装指南-6ff62ded.png\" src=\"/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/02-TensorRT安装指南-6ff62ded.png\" width height>\n**安装包方式注意的点：**\n<ol>\n<li>\n<p>需要sudo权限</p>\n</li>\n<li>\n<p>不能任意决定TenosrRT的安装位置</p>\n</li>\n<li>\n<p>需要有同样通过deb/rpm包安装的CUDA及cuDNN</p>\n</li>\n<li>\n<p>同一个主机只能安装一个小版本的TensorRT安装</p>\n</li>\n</ol>\n<p><strong>步骤如下：</strong></p>\n<ul>\n<li>\n<p>下载TRT本地repo，需要和你的linux系统版本及CPU架构匹配的版本</p>\n</li>\n<li>\n<p>手动从Debian仓库安装</p>\n</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">os=&quot;ubuntuxx04&quot;</span><br><span class=\"line\">tag=&quot;cudax.x-trt8.x.x.x-ga-yyyymmdd&quot;</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">比如：nv-tensorrt-repo-ubuntu1804-cuda11.6-trt8.4.1.5-ga-20220604_1-1_amd64.deb</span></span><br><span class=\"line\"></span><br><span class=\"line\">sudo dpkg -i nv-tensorrt-repo-$&#123;os&#125;-$&#123;tag&#125;_1-1_amd64.deb</span><br><span class=\"line\">sudo apt-key add /var/nv-tensorrt-repo-$&#123;os&#125;-$&#123;tag&#125;/*.pub</span><br><span class=\"line\"></span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get install tensorrt</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">使用python3安装</span></span><br><span class=\"line\">python3 -m pip install numpy</span><br><span class=\"line\">sudo apt-get install python3-libnvinfer-dev</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">Tensorflow需要安装</span></span><br><span class=\"line\">python3 -m pip install protobuf</span><br><span class=\"line\">sudo apt-get install uff-converter-tf</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">使用ONNX</span></span><br><span class=\"line\">python3 -m pip install numpy onnx</span><br><span class=\"line\">sudo apt-get install onnx-graphsurgeon</span><br></pre></td></tr></table></figure>\n<ul>\n<li>验证TensorRT是否安装好了</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dpkg -l | grep TensorRT</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">应该会输出以下内容：</span></span><br><span class=\"line\">ii  graphsurgeon-tf\t8.4.1-1+cuda11.6\tamd64\tGraphSurgeon for TensorRT package</span><br><span class=\"line\">ii  libnvinfer-bin\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT binaries</span><br><span class=\"line\">ii  libnvinfer-dev\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT development libraries and headers</span><br><span class=\"line\">ii  libnvinfer-plugin-dev\t8.4.1-1+cuda11.6\tamd64\tTensorRT plugin libraries</span><br><span class=\"line\">ii  libnvinfer-plugin8\t8.4.1-1+cuda11.6\tamd64\tTensorRT plugin libraries</span><br><span class=\"line\">ii  libnvinfer-samples\t8.4.1-1+cuda11.6\tall\tTensorRT samples</span><br><span class=\"line\">ii  libnvinfer8\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT runtime libraries</span><br><span class=\"line\">ii  libnvonnxparsers-dev\t\t8.4.1-1+cuda11.6\tamd64\tTensorRT ONNX libraries</span><br><span class=\"line\">ii  libnvonnxparsers8\t8.4.1-1+cuda11.6\tamd64\tTensorRT ONNX libraries</span><br><span class=\"line\">ii  libnvparsers-dev\t8.4.1-1+cuda11.6\tamd64\tTensorRT parsers libraries</span><br><span class=\"line\">ii  libnvparsers8\t8.4.1-1+cuda11.6\tamd64\tTensorRT parsers libraries</span><br><span class=\"line\">ii  python3-libnvinfer\t8.4.1-1+cuda11.6\tamd64\tPython 3 bindings for TensorRT</span><br><span class=\"line\">ii  python3-libnvinfer-dev\t8.4.1-1+cuda11.6\tamd64\tPython 3 development package for TensorRT</span><br><span class=\"line\">ii  tensorrt\t\t8.4.1.x-1+cuda11.6 \tamd64\tMeta package of TensorRT</span><br><span class=\"line\">ii  uff-converter-tf\t8.4.1-1+cuda11.6\tamd64\tUFF converter for TensorRT package</span><br><span class=\"line\">ii  onnx-graphsurgeon   8.4.1-1+cuda11.6  amd64 ONNX GraphSurgeon for TensorRT package</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"在线网络编译包安装\">在线网络编译包安装</h2>\n<ul>\n<li><strong>安装CUDA network repository</strong>，如图所示，进入<a href=\"https://developer.nvidia.com/cuda-downloads\"><strong>https://developer.nvidia.com/cuda-downloads</strong></a>，这种方式适用于已经很熟TensorRT，只想快速把应用的依赖配置起来，比如，在使用容器的时候，新手适合上面的方式。</li>\n</ul>\n<img alt=\"02-TensorRT安装指南-a2f8136f.png\" src=\"/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/02-TensorRT安装指南-a2f8136f.png\" width height>\n<p>比如：我这里选择了，Ubuntu 18.04 x86_64 deb(network)</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb</span><br><span class=\"line\">sudo dpkg -i cuda-keyring_1.0-1_all.deb</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">如果你已经有安装号的CUDA toolkit,则可以省略下面的两步</span></span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get -y install cuda</span><br></pre></td></tr></table></figure>\n<p>执行上面提供的命令，即可安装CUDA在线版本，如果你本地已经安装好了CUDA，则可以省略apt install的指令，因为当下面安装TensorRT时，apt会自动下载所需要的CUDA和cuDNN依赖</p>\n<ul>\n<li>安装TensorRT包，根据自己需要选择下面的指令</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">For only running TensorRT C++ applications</span></span><br><span class=\"line\">sudo apt-get install tensorrt-libs</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">For also building TensorRT C++ applications:</span></span><br><span class=\"line\">sudo apt-get install tensorrt-dev</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">For running TensorRT Python applications:</span></span><br><span class=\"line\">python3 -m pip install numpy</span><br><span class=\"line\">sudo apt-get install python3-libnvinfer</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>当第一步骤中，执行CUDA network repository的安装，Ubuntu会自动安装TensorRT对应最新版本的CUDA，下面的指令可以安装老版本CUDA对应着你现在本机已经安装的TensorRT版本。</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version=&quot;8.x.x.x-1+cudax.x&quot;</span><br><span class=\"line\">sudo apt-get install tensorrt-dev=$&#123;version&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">sudo apt-mark hold tensorrt-dev</span><br><span class=\"line\">如果需要升级到最新的TensorRT或者CUDA，执行下面的</span><br><span class=\"line\">sudo apt-mark unhold tensorrt-dev</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"APP-Server安装（适用于生产部署）\">APP Server安装（适用于生产部署）</h2>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">使用apt-get安装Debian包即可</span><br><span class=\"line\">the libnvinfer8 package (C++) plus any additional library packages</span><br><span class=\"line\">the python3-libnvinfer package (Python 3.x)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"交叉编译安装\">交叉编译安装</h2>\n<p>参考链接：</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#cross-compiling\">Sample Support Guide :: NVIDIA Deep Learning TensorRT Documentation</a></p>\n<p>暂时不更新这一部分。</p>\n<h2 id=\"Pip安装\">Pip安装</h2>\n<p>注意：<strong>While the TensorRT packages also contain pip wheel files, those wheel files require the rest of the .deb or .rpm packages to be installed and will not work alone</strong>（意思就是，TensorRT包中也包含了pip wheels, 但是这些wheels即使你使用了Pip安装，还是需要安装tensorRT的deb/rpm包，pip包无法单独运行）。我们需要的是，<strong>standalone pip-installable TensorRT wheel files</strong> ，这些pip包是可以不需要先安装的TensorRT deb/rpm包的。</p>\n<p><strong>The pip-installable nvidia-tensorrt Python wheel files only support Python versions 3.6 to 3.10 and CUDA 11.x at this time and will not work with other Python or CUDA versions.</strong></p>\n<p>同时，这些包只能支持 <strong>python3.6 - 3.10及 CUDA11.x</strong>的版本，其他的版本都不行，也只能运行在Linux x86_64的系统上（CentOS7以及Ubuntu18以上的最新版本）。</p>\n<ul>\n<li>确保nvidia-pyindex包已经安装，这个使用用来从NGC PyPI repo中获取其他附加Python模块的。(需要确保，你的pip 和setuptools不能太旧，过时，否则可能安装失败）</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install --upgrade setuptools pip</span><br><span class=\"line\">pip install nvidia-pyindex</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">可以添加下面的依赖到你项目的requirements.txt</span></span><br><span class=\"line\">--extra-index-url https://pypi.ngc.nvidia.com</span><br></pre></td></tr></table></figure>\n<ul>\n<li>安装TensorRT pip包</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install --upgrade nvidia-tensorrt</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">这个安装命令，将会同时安装依赖的CUDA和cuDNN pip包，因为这些都是TensorRT的依赖</span></span><br><span class=\"line\">如果安装的时候报错有类似下面的信息，那就是python版本不对/nvidia-pyindex没有安装好</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#################################################################</span></span></span><br><span class=\"line\">The package you are trying to install is only a placeholder project on PyPI.org repository.</span><br><span class=\"line\">This package is hosted on NVIDIA Python Package Index.</span><br><span class=\"line\"></span><br><span class=\"line\">This package can be installed as:</span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">pip install nvidia-pyindex</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">pip install nvidia-tensorrt</span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#################################################################</span></span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>验证是否安装好</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3</span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; import tensorrt</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; <span class=\"built_in\">print</span>(tensorrt.__version__)</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; assert tensorrt.Builder(tensorrt.Logger())</span></span><br><span class=\"line\">如果最后这个python指令报错，类似下面的输出信息，那么可能是没有安装好显卡驱动</span><br><span class=\"line\">[TensorRT] ERROR: CUDA initialization failure with error 100. Please check your CUDA installation: ...</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"Tar包安装\">Tar包安装</h2>\n<ol>\n<li>\n<p>安装好你本机需要的CUDA（<a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>10.2</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.0 update 1</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.1 update 1</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.2 update 2</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.3 update 1</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.4 update 4</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.5 update 2</strong></a>, <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.6 update 2</strong></a> or <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\"><strong>11.7</strong></a>）</p>\n</li>\n<li>\n<p>安装好<a href=\"https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-841\"><strong>cuDNN 8.4.1</strong></a></p>\n</li>\n<li>\n<p>安装好python3(可选）</p>\n</li>\n<li>\n<p>下载TensorRT Tar包<a href=\"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#downloading\" title=\"Ensure you are a member of the NVIDIA Developer Program. If not, follow the prompts to gain access.\">Download</a></p>\n</li>\n<li>\n<p>解压文件</p>\n</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version=&quot;8.x.x.x&quot;</span><br><span class=\"line\">arch=$(uname -m)</span><br><span class=\"line\">cuda=&quot;cuda-x.x&quot;</span><br><span class=\"line\">cudnn=&quot;cudnn8.x&quot;</span><br><span class=\"line\">tar -xzvf TensorRT-$&#123;version&#125;.Linux.$&#123;arch&#125;-gnu.$&#123;cuda&#125;.$&#123;cudnn&#125;.tar.gz</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"></span></span><br><span class=\"line\"><span class=\"language-bash\">8.x.x.x is your TensorRT version</span></span><br><span class=\"line\">cuda-x.x is CUDA version 10.2 or 11.6</span><br><span class=\"line\">cudnn8.x is cuDNN version 8.4</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ol>\n<li>添加TensorRT lib文件夹的绝对路径到环境变量中</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;TensorRT-$&#123;version&#125;/lib&gt;</span><br></pre></td></tr></table></figure>\n<ol>\n<li>安装python TensorRT wheel包</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd TensorRT-$&#123;version&#125;/python</span><br><span class=\"line\">python3 -m pip install tensorrt-*-cp3x-none-linux_x86_64.whl</span><br></pre></td></tr></table></figure>\n<ol>\n<li>安装 Python UFF wheel 包</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd TensorRT-$&#123;version&#125;/uff</span><br><span class=\"line\"></span><br><span class=\"line\">python3 -m pip install uff-0.6.9-py2.py3-none-any.whl</span><br></pre></td></tr></table></figure>\n<ol>\n<li>安装Python onnx-graphsurgeon /graphsurgeon  wheel 包</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd TensorRT-$&#123;version&#125;/onnx_graphsurgeon</span><br><span class=\"line\">python3 -m pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl</span><br><span class=\"line\">cd TensorRT-$&#123;version&#125;/graphsurgeon</span><br><span class=\"line\">python3 -m pip install graphsurgeon-0.4.6-py2.py3-none-any.whl</span><br></pre></td></tr></table></figure>\n<h2 id=\"ZIP包安装（适用于windows\">ZIP包安装（适用于windows)</h2>\n<p>参考官方<a href=\"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip\"><strong>https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip</strong></a></p>\n<h1>卸载方式</h1>\n<ol>\n<li>卸载libnvinfer8，通过deb包安装的</li>\n<li>卸载uff-converter-tf, graphsurgeon-tf, and onnx-graphsurgeon</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get purge graphsurgeon-tf onnx-graphsurgeon</span><br><span class=\"line\">sudo apt-get purge uff-converter-tf #不卸载graphsurgeon-tf也可以</span><br><span class=\"line\">sudo apt-get autoremove</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>卸载Python TensorRT</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip3 uninstall tensorrt</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>卸载Python UFF</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip3 uninstall uff</span><br></pre></td></tr></table></figure>\n<ol start=\"5\">\n<li>卸载Python GraphSurgeon</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip3 uninstall graphsurgeon</span><br></pre></td></tr></table></figure>\n<ol start=\"6\">\n<li>卸载Python ONNX GraphSurgeon</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip3 uninstall onnx-graphsurgeon</span><br></pre></td></tr></table></figure>\n<h1>PyCUDA</h1>\n<p>确保安装了numpy</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install numpy</span><br><span class=\"line\">python3 -m pip install &#x27;pycuda&lt;2021.1&#x27;</span><br></pre></td></tr></table></figure>\n"},{"title":"TensorRT概述","date":"2022-08-24T15:47:00.000Z","updated":"2022-08-24T15:47:00.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/robotics-arm.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":null,"_content":"\n\n## ***TensorRT概述***\n\nTensorRT是一个NVIDIA给自己的显卡开发的一个对于深度学习模型进行加速推理及优化，以满足部署需求的C++ SDK（Software Development Kits), 常常和[ONNX]()（Open Neural Network Exchange）通用模型联系在一起。\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n### 大纲：\n\n>1. [概述](#概述 \"概述\")\n\n## 概述\n\n### TensorRT介绍\n\nNVIDIA TensorRT是一个用来优化训练后的深度学习模型的SDK，这个SDK可以在模型推理的时候提高性能。TensorRT包含了一个深度学习的推理优化器（优化训练好的模型）和一个执行器（部署推理）。使用TensoRT可以使你的模型运行时有更高的吞吐量和更低的延迟。\n\n**如图所示**：  \n\n<img alt=\"01-TensorRT概述-b0cc304a.png\" src=\"01-TensorRT概述-b0cc304a.png\" width=\"\" height=\"\" >\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n","source":"_posts/002-NVIDIA/03-TensorRT/TensorRT概述.md","raw":"---\ntitle: TensorRT概述\ndate: 2022-08-24 23:47:00\nupdated: 2022-08-24 23:47:00\ntags: TensorRT\ncategories: \n- NVIDIA\n- TensorRT\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax:\n---\n\n\n## ***TensorRT概述***\n\nTensorRT是一个NVIDIA给自己的显卡开发的一个对于深度学习模型进行加速推理及优化，以满足部署需求的C++ SDK（Software Development Kits), 常常和[ONNX]()（Open Neural Network Exchange）通用模型联系在一起。\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n### 大纲：\n\n>1. [概述](#概述 \"概述\")\n\n## 概述\n\n### TensorRT介绍\n\nNVIDIA TensorRT是一个用来优化训练后的深度学习模型的SDK，这个SDK可以在模型推理的时候提高性能。TensorRT包含了一个深度学习的推理优化器（优化训练好的模型）和一个执行器（部署推理）。使用TensoRT可以使你的模型运行时有更高的吞吐量和更低的延迟。\n\n**如图所示**：  \n\n<img alt=\"01-TensorRT概述-b0cc304a.png\" src=\"01-TensorRT概述-b0cc304a.png\" width=\"\" height=\"\" >\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n","slug":"002-NVIDIA/03-TensorRT/TensorRT概述","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483sv000ru9rla8xddl48","content":"<h2 id=\"TensorRT概述\"><em><strong>TensorRT概述</strong></em></h2>\n<p>TensorRT是一个NVIDIA给自己的显卡开发的一个对于深度学习模型进行加速推理及优化，以满足部署需求的C++ SDK（Software Development Kits), 常常和<a href>ONNX</a>（Open Neural Network Exchange）通用模型联系在一起。</p>\n<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h3 id=\"大纲：\">大纲：</h3>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n</ol>\n</blockquote>\n<h2 id=\"概述\">概述</h2>\n<h3 id=\"TensorRT介绍\">TensorRT介绍</h3>\n<p>NVIDIA TensorRT是一个用来优化训练后的深度学习模型的SDK，这个SDK可以在模型推理的时候提高性能。TensorRT包含了一个深度学习的推理优化器（优化训练好的模型）和一个执行器（部署推理）。使用TensoRT可以使你的模型运行时有更高的吞吐量和更低的延迟。</p>\n<p><strong>如图所示</strong>：</p>\n<img alt=\"01-TensorRT概述-b0cc304a.png\" src=\"/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT%E6%A6%82%E8%BF%B0/01-TensorRT概述-b0cc304a.png\" width height>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h2 id=\"TensorRT概述\"><em><strong>TensorRT概述</strong></em></h2>\n<p>TensorRT是一个NVIDIA给自己的显卡开发的一个对于深度学习模型进行加速推理及优化，以满足部署需求的C++ SDK（Software Development Kits), 常常和<a href>ONNX</a>（Open Neural Network Exchange）通用模型联系在一起。</p>\n<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h3 id=\"大纲：\">大纲：</h3>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n</ol>\n</blockquote>\n<h2 id=\"概述\">概述</h2>\n<h3 id=\"TensorRT介绍\">TensorRT介绍</h3>\n<p>NVIDIA TensorRT是一个用来优化训练后的深度学习模型的SDK，这个SDK可以在模型推理的时候提高性能。TensorRT包含了一个深度学习的推理优化器（优化训练好的模型）和一个执行器（部署推理）。使用TensoRT可以使你的模型运行时有更高的吞吐量和更低的延迟。</p>\n<p><strong>如图所示</strong>：</p>\n<img alt=\"01-TensorRT概述-b0cc304a.png\" src=\"/2022/08/24/002-NVIDIA/03-TensorRT/TensorRT%E6%A6%82%E8%BF%B0/01-TensorRT概述-b0cc304a.png\" width height>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n"},{"title":"cuBLAS概述","date":"2022-08-25T04:13:21.000Z","updated":"2022-08-25T04:13:21.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/walle.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":null,"_content":"\n\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n------\n\n","source":"_posts/002-NVIDIA/04-cuBLAS/cuBLAS概述.md","raw":"---\ntitle: cuBLAS概述\ndate: 2022-08-25 12:13:21\nupdated: 2022-08-25 12:13:21\ntags: cuBLAS\ncategories: \n- NVIDIA\n- cuBLAS\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax: \n---\n\n\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n------\n\n","slug":"002-NVIDIA/04-cuBLAS/cuBLAS概述","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483sx000vu9rl32g25n40","content":"<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n"},{"title":"Gimbal-Camera-Tracking","date":"2022-08-23T18:25:51.000Z","updated":"2022-08-23T18:25:51.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/SNN.jpeg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":true,"_content":"\n## 系统部署方式介绍\n**此项目还在迭代中，部署方式采用本地host环境结合Docker环境进行算法的部署。\n以后将会优化镜像部署，不使用本地环境，这样部署效率更高。**\n\n源码将要分成两部分分别部署在主从机上。**  \n\n---\n### Host主机部署\n\n**主机上需要部署的源码是：fv_tracking, gimbal_control, serial_ros, bboxes_ex_msgs进行部署。**  \n**创建一个gimbal_camera_ws工作空间, 将刚才从gitlab上拉取的源码中以上提到的四个包放在**  \n**gimbal_camera_ws的src文件夹中。**\n\n1. **编译以上的gimbal_camera_ws的源码，在当前工作空间的根目录下执行命令：**\n`$ catkin_make`\n\n2. **编译完成后，链接好相机的USB和串口的USB转换头到工控机上，使用命令：**  \n`$ ls /dev/video* ` `$ ls /dev/ttyUSB0`**去确认是否已经读取到云台相机和云台串口。**  \n\n***备注：在代码中，默认启动和绑定的设备是\" /dev/video0\" 和 \"/dev/ttyUSB0\"， 工控\n机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException***\n\n---\n\n### Docker从机部署\n**Docker镜像目前存储在公司的Harbor仓库中，当前维护版本镜像名为**  \n**pix/gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1**\n\n1. **实例化一个容器，镜像构建容器的启动命令：**  \n`$docker run -it --rm --gpus all\n--net host --privileged --ipc=host --volume /home/t/gimbal-camera-tracking/\ngimbal_camera_ws/:/workspace/gimbal_camera_ws  xxx镜像名`  \n\n2. **启动之后会进入到容器的内部，然后会将本地的源码映射到docker环境中的/workspace**  \n**文件夹下的/gimbal_camera_ws**\n\n***备注：在镜像构建容器的启动命令时，需要加入自己将源码放置的路径，如上命令中，\"/home/t/gimbal-camera-tracking/gimbal_camera_ws\"就是工控机默认将源码***  \n***放在/home/t/路径下，源码的拉取下来的文件夹名为\"gimbal-camera-tracking\"***   \n\n---\n\n### 系统启动方式\n\n**完成上面的主从机的部署后，确认主机的gimbal_camera_ws和从机docker中映射的**  \n**gimbal_camera_ws中的源码都已经编译好了，然后将\"source /home/t/**  \n**gimbal_camera_ws/devel/setup.bash\"和\"source /workspace/gimbal_camera_ws/devel/setup.bash\"**  \n**分别添加到主机和docker从机的\"~/.bashrc\"环境中, 重新source下环境，以初始化环境变量的配置。**  \n\n**接下来是启动步骤：**  \n\n1. **检查是否可以读取到相机图像和串口信息**  \n    使用`$ cheese`或者`$ rosrun  fv_tracking web_cam`,命令来读取相机图像，看是否能够打开web_cam节点并读到云台相机图像。  \n    使用`$ rosrun serial_ros serial_node` 和`$ rosrun serial_ros moni`，看是否能够读取到云台的姿态信息输出。  \n    确认都能读取到数据之后，将cheese关闭，**serial_node和moni就不用关闭。**\n    如图：  \n<center class=\"half\">\n    <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291508803.png\" width=\"200\" height=\"200\"/><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291510788.png\" alt=\"20220114-cddd0cdf\" width=\"200\" height=\"200\"/><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291511050.png\" alt=\"20220114-923e402c\" width=\"200\" height=\"200\"/>\n</center>\n\n\n\n\n\n2. **启动相机节点**  \n    在主机中，使用命令`$ rosrun  fv_tracking web_cam`，启动了相机视频流读取节点。\n\n3. **启动YOLOX的ROS节点**  \n    在docker中，使用命令`$ roslaunch yolox_ros yolox_ros.launch`，启动了检测器，检测实时图像中的物体。\n\n4. **启动SiamFC++的ROS节点, 确认你需要跟踪的目标在当前相机的视野范围内**  \n    在docker中，使用命令`$ roslaunch siamfc_ros siamfc_ros.launch`，跟踪器成功锁定需要被跟踪的目标。\n\n5. **启动主机中的fv_tracking包中的track_kcf的ROS节点, 确认被跟踪的物体是你想要的目标**  \n    在主机中，使用命令`$ rosrun fv_tracking tracker_kcf`，云台开始转动跟随被锁定的目标。\n\n6. **启动底盘控制节点, 确认云台已经成功跟随需要被跟踪的目标**  \n    在docker中，使用命令`$ roslaunch chassis_control chassis_control.launch`, 开启底盘控制节点，输出控制指令。\n\n7. **启动底盘驱动ROS节点**  \n    在主机中，使用命令`$ roslaunch pix_driver pix_driver_read.launch`  \n    `$ roslaunch pix_driver pix_driver_write.launch`\n\n8. **确定车辆状态，在遥控器上选择MOD为 self-driving模式，如图已经切换到自动驾驶模式，使用MOD按键进行切换**  \n\n   <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291513665.png\" alt=\"20220114-8bd94d2e\" style=\"zoom: 25%;\" />\n---\n\n## 硬件介绍\n\n### 云台硬件规格\n1. **吊舱云台是根据客户需要进行选型，选择支持变焦（10倍）的云台，具有400万有效像素，**\n    **三自由度，最大航向角为+-150度无极旋转，工作电流为240mA(@12V)，重量400g（含相机）。**\n2. **云台内部有两路视频流，一路1080P 30FPS本地H.264压缩，存储在设备内，另一路输**\n    **出1080P 60FPS格式的HDMI信号，用于无线图传，支持PWM和串口控制。**\n3. **云台可以自动对焦，对焦时间小于1s，焦距范围为 F = 4.9 ~ 49mm，105dB宽动态范**\n    **围，温度工作范围-10 至 55 摄氏度。**\n<center class=\"half\">\n    <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291515746.png\" alt=\"20220114-e0917cd4\" width=\"200\" height=\"200\"/><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291518515.png\" alt=\"20220112-4b9a80b5\" width=\"200\" height=\"200\"/>\n</center>\n\n\n---\n## 算法框架介绍\n**云台跟踪联动算法分为两大部分，第一部分是基于深度学习的目标检测算法和目标跟踪算法，\n第二部分是基于PID调节的云台控制算法和底盘控制算法。**  \n\n**流程图如下：**   \n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291525682.png\" style=\"zoom: 50%;\" />\n\n**淡蓝色部分为检测跟踪算法的流程，深蓝色部分为云台和底盘联动控制算法部分**\n\n---\n\n## 云台ROS驱动介绍\n**云台ROS驱动，根据云台供应商给的串口指令集进行串口编码控制，在源码中serial_ros包主要用来\n将云台的控制指令通过ROS进行转换为串口的指令下发给云台控制板。其中主要需要启动的ROS节点是\nserial_node 和 moni。**\n\n启动方式为：  \n`$ rosrun serial_ros serial_node`  \n`$ rosrun serial_ros moni`\n\n**同时云台驱动中也包含了相机启动和KCF跟踪的fv_tracking包，主要启动的ROS节点为\nweb_cam和tracker_kcf**。\n\n启动方式为：  \n`$ rosrun fv_tracking web_cam`  \n`$rosrun fv_tracking tracker_kcf`\n\n**启动后，相机画面及跟踪窗口效果如下：**  \n\n![20220116-67901f3c](https://www.synotech.top:5523/uploads/2023/04/29/202304291526605.png)\n\n\n---\n\n## 检测算法介绍\n**使用的目标检测算法为YOLOX，YOLOX的github[链接](https://github.com/Megvii-BaseDetection/YOLOX \"YOLOX\")**  \n**将YOLOX的源码整合为ROS代码下，改为yolox_ros的ROS包**\n\n启动方式为：  \n`$roslaunch yolox_ros yolox_ros.launch`  \n**启动之后，yolox_ros节点将会订阅来自云台相机的原始图像数据，生成检测结果并展示在桌面上。**\n\n**yolo_ros**包中发布的话题为：**\"/yolox/bounding_boxes\"** 和 **\" /yolox/image_raw\"**。  \n**yolo_ros**包中订阅的话题为：**\"/camera/rgb/image_raw\"**。\n\n**启动后，检测算法窗口效果如下：**  \n\n![20220116-76d60e91](https://www.synotech.top:5523/uploads/2023/04/29/202304291526682.png)\n\n\n---\n\n## 跟踪算法介绍\n**使用的目标跟踪算法SiamFC++，SiamFC++的github[链接](https://github.com/MegviiDetection/video_analyst \"SiamFC++\")**。\n**基于SiamFC++的源码整合为ROS代码下，开发了siamfc_ros的ROS包**。\n\n启动方式为：  \n`$roslaunch siamfc_ros siamfc_ros.launch`  \n**启动之后，siamfc_ros节点将会订阅来自云台相机的原始图像数据，以及来自yolox_ros节**  \n**点的检测到的物体选框，物体选框当前为自动选则模式，选择被检测到的第一个物体，下一步迭代**  \n**会添加，优化一个ID选择器，让用户可以选择当前画面中被检测到物体的ID，然后输入到跟踪**  \n**器中，会生成跟踪结果并展示在桌面上。**\n\n**siamfc_ros**包中发布的话题为：**\"/siamfc/image_raw\"** 和 **\"/siamfc/bounding_box\"**。  \n**siamfc_ros**包中订阅的话题为：**\"/yolox/bounding_boxes \"** 和 **\"/camera/rgb/image_raw \"**。\n\n**启动后，跟踪算法窗口效果如下：**  \n\n![20220116-3b5f0f20](https://www.synotech.top:5523/uploads/2023/04/29/202304291527559.png)\n\n---\n\n## 联动控制算法介绍\n**联动控制算法主要流程是依赖云台的PID调节控制输出的yaw值和底盘的yaw值进行关联，当**  \n**云台跟随目标进行转动的时候，将云台的yaw值实时发出到ROS话题**\"/gimbal_camera/rpy_data\"**上，然后底盘根据云台的**  \n**yaw值进行处理，转换为底盘的航向角。**\n\n### 云台控制算法\n**云台控制算法采用的PID进行调节，PID控制的error主要是根据图像中心十字靶心像素坐标和被跟踪物体的boundingbox的物体中心的像素坐标的误差。**\n\n\n\n### 底盘控制算法\n**底盘的控制算法，其中一部分基于视觉的大致观测距离判断进行线速度的控制，另一部分基于云台的yaw值\n实时对角速度进行控制。**\n\n---\n","source":"_posts/003-Projects/01-视觉跟踪/Gimbal-Camera-Tracking.md","raw":"---\ntitle: Gimbal-Camera-Tracking\ndate: 2022-08-24 02:25:51\nupdated: 2022-08-24 02:25:51\ntags: Gimbal Camera Tracking\ncategories: \n- Projects\n- 视觉跟踪\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax: true\n---\n\n## 系统部署方式介绍\n**此项目还在迭代中，部署方式采用本地host环境结合Docker环境进行算法的部署。\n以后将会优化镜像部署，不使用本地环境，这样部署效率更高。**\n\n源码将要分成两部分分别部署在主从机上。**  \n\n---\n### Host主机部署\n\n**主机上需要部署的源码是：fv_tracking, gimbal_control, serial_ros, bboxes_ex_msgs进行部署。**  \n**创建一个gimbal_camera_ws工作空间, 将刚才从gitlab上拉取的源码中以上提到的四个包放在**  \n**gimbal_camera_ws的src文件夹中。**\n\n1. **编译以上的gimbal_camera_ws的源码，在当前工作空间的根目录下执行命令：**\n`$ catkin_make`\n\n2. **编译完成后，链接好相机的USB和串口的USB转换头到工控机上，使用命令：**  \n`$ ls /dev/video* ` `$ ls /dev/ttyUSB0`**去确认是否已经读取到云台相机和云台串口。**  \n\n***备注：在代码中，默认启动和绑定的设备是\" /dev/video0\" 和 \"/dev/ttyUSB0\"， 工控\n机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException***\n\n---\n\n### Docker从机部署\n**Docker镜像目前存储在公司的Harbor仓库中，当前维护版本镜像名为**  \n**pix/gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1**\n\n1. **实例化一个容器，镜像构建容器的启动命令：**  \n`$docker run -it --rm --gpus all\n--net host --privileged --ipc=host --volume /home/t/gimbal-camera-tracking/\ngimbal_camera_ws/:/workspace/gimbal_camera_ws  xxx镜像名`  \n\n2. **启动之后会进入到容器的内部，然后会将本地的源码映射到docker环境中的/workspace**  \n**文件夹下的/gimbal_camera_ws**\n\n***备注：在镜像构建容器的启动命令时，需要加入自己将源码放置的路径，如上命令中，\"/home/t/gimbal-camera-tracking/gimbal_camera_ws\"就是工控机默认将源码***  \n***放在/home/t/路径下，源码的拉取下来的文件夹名为\"gimbal-camera-tracking\"***   \n\n---\n\n### 系统启动方式\n\n**完成上面的主从机的部署后，确认主机的gimbal_camera_ws和从机docker中映射的**  \n**gimbal_camera_ws中的源码都已经编译好了，然后将\"source /home/t/**  \n**gimbal_camera_ws/devel/setup.bash\"和\"source /workspace/gimbal_camera_ws/devel/setup.bash\"**  \n**分别添加到主机和docker从机的\"~/.bashrc\"环境中, 重新source下环境，以初始化环境变量的配置。**  \n\n**接下来是启动步骤：**  \n\n1. **检查是否可以读取到相机图像和串口信息**  \n    使用`$ cheese`或者`$ rosrun  fv_tracking web_cam`,命令来读取相机图像，看是否能够打开web_cam节点并读到云台相机图像。  \n    使用`$ rosrun serial_ros serial_node` 和`$ rosrun serial_ros moni`，看是否能够读取到云台的姿态信息输出。  \n    确认都能读取到数据之后，将cheese关闭，**serial_node和moni就不用关闭。**\n    如图：  \n<center class=\"half\">\n    <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291508803.png\" width=\"200\" height=\"200\"/><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291510788.png\" alt=\"20220114-cddd0cdf\" width=\"200\" height=\"200\"/><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291511050.png\" alt=\"20220114-923e402c\" width=\"200\" height=\"200\"/>\n</center>\n\n\n\n\n\n2. **启动相机节点**  \n    在主机中，使用命令`$ rosrun  fv_tracking web_cam`，启动了相机视频流读取节点。\n\n3. **启动YOLOX的ROS节点**  \n    在docker中，使用命令`$ roslaunch yolox_ros yolox_ros.launch`，启动了检测器，检测实时图像中的物体。\n\n4. **启动SiamFC++的ROS节点, 确认你需要跟踪的目标在当前相机的视野范围内**  \n    在docker中，使用命令`$ roslaunch siamfc_ros siamfc_ros.launch`，跟踪器成功锁定需要被跟踪的目标。\n\n5. **启动主机中的fv_tracking包中的track_kcf的ROS节点, 确认被跟踪的物体是你想要的目标**  \n    在主机中，使用命令`$ rosrun fv_tracking tracker_kcf`，云台开始转动跟随被锁定的目标。\n\n6. **启动底盘控制节点, 确认云台已经成功跟随需要被跟踪的目标**  \n    在docker中，使用命令`$ roslaunch chassis_control chassis_control.launch`, 开启底盘控制节点，输出控制指令。\n\n7. **启动底盘驱动ROS节点**  \n    在主机中，使用命令`$ roslaunch pix_driver pix_driver_read.launch`  \n    `$ roslaunch pix_driver pix_driver_write.launch`\n\n8. **确定车辆状态，在遥控器上选择MOD为 self-driving模式，如图已经切换到自动驾驶模式，使用MOD按键进行切换**  \n\n   <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291513665.png\" alt=\"20220114-8bd94d2e\" style=\"zoom: 25%;\" />\n---\n\n## 硬件介绍\n\n### 云台硬件规格\n1. **吊舱云台是根据客户需要进行选型，选择支持变焦（10倍）的云台，具有400万有效像素，**\n    **三自由度，最大航向角为+-150度无极旋转，工作电流为240mA(@12V)，重量400g（含相机）。**\n2. **云台内部有两路视频流，一路1080P 30FPS本地H.264压缩，存储在设备内，另一路输**\n    **出1080P 60FPS格式的HDMI信号，用于无线图传，支持PWM和串口控制。**\n3. **云台可以自动对焦，对焦时间小于1s，焦距范围为 F = 4.9 ~ 49mm，105dB宽动态范**\n    **围，温度工作范围-10 至 55 摄氏度。**\n<center class=\"half\">\n    <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291515746.png\" alt=\"20220114-e0917cd4\" width=\"200\" height=\"200\"/><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291518515.png\" alt=\"20220112-4b9a80b5\" width=\"200\" height=\"200\"/>\n</center>\n\n\n---\n## 算法框架介绍\n**云台跟踪联动算法分为两大部分，第一部分是基于深度学习的目标检测算法和目标跟踪算法，\n第二部分是基于PID调节的云台控制算法和底盘控制算法。**  \n\n**流程图如下：**   \n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291525682.png\" style=\"zoom: 50%;\" />\n\n**淡蓝色部分为检测跟踪算法的流程，深蓝色部分为云台和底盘联动控制算法部分**\n\n---\n\n## 云台ROS驱动介绍\n**云台ROS驱动，根据云台供应商给的串口指令集进行串口编码控制，在源码中serial_ros包主要用来\n将云台的控制指令通过ROS进行转换为串口的指令下发给云台控制板。其中主要需要启动的ROS节点是\nserial_node 和 moni。**\n\n启动方式为：  \n`$ rosrun serial_ros serial_node`  \n`$ rosrun serial_ros moni`\n\n**同时云台驱动中也包含了相机启动和KCF跟踪的fv_tracking包，主要启动的ROS节点为\nweb_cam和tracker_kcf**。\n\n启动方式为：  \n`$ rosrun fv_tracking web_cam`  \n`$rosrun fv_tracking tracker_kcf`\n\n**启动后，相机画面及跟踪窗口效果如下：**  \n\n![20220116-67901f3c](https://www.synotech.top:5523/uploads/2023/04/29/202304291526605.png)\n\n\n---\n\n## 检测算法介绍\n**使用的目标检测算法为YOLOX，YOLOX的github[链接](https://github.com/Megvii-BaseDetection/YOLOX \"YOLOX\")**  \n**将YOLOX的源码整合为ROS代码下，改为yolox_ros的ROS包**\n\n启动方式为：  \n`$roslaunch yolox_ros yolox_ros.launch`  \n**启动之后，yolox_ros节点将会订阅来自云台相机的原始图像数据，生成检测结果并展示在桌面上。**\n\n**yolo_ros**包中发布的话题为：**\"/yolox/bounding_boxes\"** 和 **\" /yolox/image_raw\"**。  \n**yolo_ros**包中订阅的话题为：**\"/camera/rgb/image_raw\"**。\n\n**启动后，检测算法窗口效果如下：**  \n\n![20220116-76d60e91](https://www.synotech.top:5523/uploads/2023/04/29/202304291526682.png)\n\n\n---\n\n## 跟踪算法介绍\n**使用的目标跟踪算法SiamFC++，SiamFC++的github[链接](https://github.com/MegviiDetection/video_analyst \"SiamFC++\")**。\n**基于SiamFC++的源码整合为ROS代码下，开发了siamfc_ros的ROS包**。\n\n启动方式为：  \n`$roslaunch siamfc_ros siamfc_ros.launch`  \n**启动之后，siamfc_ros节点将会订阅来自云台相机的原始图像数据，以及来自yolox_ros节**  \n**点的检测到的物体选框，物体选框当前为自动选则模式，选择被检测到的第一个物体，下一步迭代**  \n**会添加，优化一个ID选择器，让用户可以选择当前画面中被检测到物体的ID，然后输入到跟踪**  \n**器中，会生成跟踪结果并展示在桌面上。**\n\n**siamfc_ros**包中发布的话题为：**\"/siamfc/image_raw\"** 和 **\"/siamfc/bounding_box\"**。  \n**siamfc_ros**包中订阅的话题为：**\"/yolox/bounding_boxes \"** 和 **\"/camera/rgb/image_raw \"**。\n\n**启动后，跟踪算法窗口效果如下：**  \n\n![20220116-3b5f0f20](https://www.synotech.top:5523/uploads/2023/04/29/202304291527559.png)\n\n---\n\n## 联动控制算法介绍\n**联动控制算法主要流程是依赖云台的PID调节控制输出的yaw值和底盘的yaw值进行关联，当**  \n**云台跟随目标进行转动的时候，将云台的yaw值实时发出到ROS话题**\"/gimbal_camera/rpy_data\"**上，然后底盘根据云台的**  \n**yaw值进行处理，转换为底盘的航向角。**\n\n### 云台控制算法\n**云台控制算法采用的PID进行调节，PID控制的error主要是根据图像中心十字靶心像素坐标和被跟踪物体的boundingbox的物体中心的像素坐标的误差。**\n\n\n\n### 底盘控制算法\n**底盘的控制算法，其中一部分基于视觉的大致观测距离判断进行线速度的控制，另一部分基于云台的yaw值\n实时对角速度进行控制。**\n\n---\n","slug":"003-Projects/01-视觉跟踪/Gimbal-Camera-Tracking","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483sy000xu9rl135v6mjw","content":"<h2 id=\"系统部署方式介绍\">系统部署方式介绍</h2>\n<p><strong>此项目还在迭代中，部署方式采用本地host环境结合Docker环境进行算法的部署。<br>\n以后将会优化镜像部署，不使用本地环境，这样部署效率更高。</strong></p>\n<p>源码将要分成两部分分别部署在主从机上。**</p>\n<hr>\n<h3 id=\"Host主机部署\">Host主机部署</h3>\n<p><strong>主机上需要部署的源码是：fv_tracking, gimbal_control, serial_ros, bboxes_ex_msgs进行部署。</strong><br>\n<strong>创建一个gimbal_camera_ws工作空间, 将刚才从gitlab上拉取的源码中以上提到的四个包放在</strong><br>\n<strong>gimbal_camera_ws的src文件夹中。</strong></p>\n<ol>\n<li>\n<p><strong>编译以上的gimbal_camera_ws的源码，在当前工作空间的根目录下执行命令：</strong><br>\n<code>$ catkin_make</code></p>\n</li>\n<li>\n<p><strong>编译完成后，链接好相机的USB和串口的USB转换头到工控机上，使用命令：</strong><br>\n<code>$ ls /dev/video* </code> <code>$ ls /dev/ttyUSB0</code><strong>去确认是否已经读取到云台相机和云台串口。</strong></p>\n</li>\n</ol>\n<p><em><strong>备注：在代码中，默认启动和绑定的设备是&quot; /dev/video0&quot; 和 “/dev/ttyUSB0”， 工控<br>\n机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException</strong></em></p>\n<hr>\n<h3 id=\"Docker从机部署\">Docker从机部署</h3>\n<p><strong>Docker镜像目前存储在公司的Harbor仓库中，当前维护版本镜像名为</strong><br>\n<strong>pix/gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1</strong></p>\n<ol>\n<li>\n<p><strong>实例化一个容器，镜像构建容器的启动命令：</strong><br>\n<code>$docker run -it --rm --gpus all --net host --privileged --ipc=host --volume /home/t/gimbal-camera-tracking/ gimbal_camera_ws/:/workspace/gimbal_camera_ws  xxx镜像名</code></p>\n</li>\n<li>\n<p><strong>启动之后会进入到容器的内部，然后会将本地的源码映射到docker环境中的/workspace</strong><br>\n<strong>文件夹下的/gimbal_camera_ws</strong></p>\n</li>\n</ol>\n<p><em><strong>备注：在镜像构建容器的启动命令时，需要加入自己将源码放置的路径，如上命令中，&quot;/home/t/gimbal-camera-tracking/gimbal_camera_ws&quot;就是工控机默认将源码</strong></em><br>\n<em><strong>放在/home/t/路径下，源码的拉取下来的文件夹名为&quot;gimbal-camera-tracking&quot;</strong></em></p>\n<hr>\n<h3 id=\"系统启动方式\">系统启动方式</h3>\n<p><strong>完成上面的主从机的部署后，确认主机的gimbal_camera_ws和从机docker中映射的</strong><br>\n<strong>gimbal_camera_ws中的源码都已经编译好了，然后将&quot;source /home/t/</strong><br>\n<strong>gimbal_camera_ws/devel/setup.bash&quot;和&quot;source /workspace/gimbal_camera_ws/devel/setup.bash&quot;</strong><br>\n<strong>分别添加到主机和docker从机的&quot;~/.bashrc&quot;环境中, 重新source下环境，以初始化环境变量的配置。</strong></p>\n<p><strong>接下来是启动步骤：</strong></p>\n<ol>\n<li><strong>检查是否可以读取到相机图像和串口信息</strong><br>\n使用<code>$ cheese</code>或者<code>$ rosrun  fv_tracking web_cam</code>,命令来读取相机图像，看是否能够打开web_cam节点并读到云台相机图像。<br>\n使用<code>$ rosrun serial_ros serial_node</code> 和<code>$ rosrun serial_ros moni</code>，看是否能够读取到云台的姿态信息输出。<br>\n确认都能读取到数据之后，将cheese关闭，<strong>serial_node和moni就不用关闭。</strong><br>\n如图：</li>\n</ol>\n<center class=\"half\">\n    <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291508803.png\" width=\"200\" height=\"200\"><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291510788.png\" alt=\"20220114-cddd0cdf\" width=\"200\" height=\"200\"><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291511050.png\" alt=\"20220114-923e402c\" width=\"200\" height=\"200\">\n</center>\n<ol start=\"2\">\n<li>\n<p><strong>启动相机节点</strong><br>\n在主机中，使用命令<code>$ rosrun  fv_tracking web_cam</code>，启动了相机视频流读取节点。</p>\n</li>\n<li>\n<p><strong>启动YOLOX的ROS节点</strong><br>\n在docker中，使用命令<code>$ roslaunch yolox_ros yolox_ros.launch</code>，启动了检测器，检测实时图像中的物体。</p>\n</li>\n<li>\n<p><strong>启动SiamFC++的ROS节点, 确认你需要跟踪的目标在当前相机的视野范围内</strong><br>\n在docker中，使用命令<code>$ roslaunch siamfc_ros siamfc_ros.launch</code>，跟踪器成功锁定需要被跟踪的目标。</p>\n</li>\n<li>\n<p><strong>启动主机中的fv_tracking包中的track_kcf的ROS节点, 确认被跟踪的物体是你想要的目标</strong><br>\n在主机中，使用命令<code>$ rosrun fv_tracking tracker_kcf</code>，云台开始转动跟随被锁定的目标。</p>\n</li>\n<li>\n<p><strong>启动底盘控制节点, 确认云台已经成功跟随需要被跟踪的目标</strong><br>\n在docker中，使用命令<code>$ roslaunch chassis_control chassis_control.launch</code>, 开启底盘控制节点，输出控制指令。</p>\n</li>\n<li>\n<p><strong>启动底盘驱动ROS节点</strong><br>\n在主机中，使用命令<code>$ roslaunch pix_driver pix_driver_read.launch</code><br>\n<code>$ roslaunch pix_driver pix_driver_write.launch</code></p>\n</li>\n<li>\n<p><strong>确定车辆状态，在遥控器上选择MOD为 self-driving模式，如图已经切换到自动驾驶模式，使用MOD按键进行切换</strong></p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291513665.png\" alt=\"20220114-8bd94d2e\" style=\"zoom: 25%;\">\n</li>\n</ol>\n<hr>\n<h2 id=\"硬件介绍\">硬件介绍</h2>\n<h3 id=\"云台硬件规格\">云台硬件规格</h3>\n<ol>\n<li><strong>吊舱云台是根据客户需要进行选型，选择支持变焦（10倍）的云台，具有400万有效像素，</strong><br>\n<strong>三自由度，最大航向角为±150度无极旋转，工作电流为240mA(@12V)，重量400g（含相机）。</strong></li>\n<li><strong>云台内部有两路视频流，一路1080P 30FPS本地H.264压缩，存储在设备内，另一路输</strong><br>\n<strong>出1080P 60FPS格式的HDMI信号，用于无线图传，支持PWM和串口控制。</strong></li>\n<li><strong>云台可以自动对焦，对焦时间小于1s，焦距范围为 F = 4.9 ~ 49mm，105dB宽动态范</strong><br>\n<strong>围，温度工作范围-10 至 55 摄氏度。</strong></li>\n</ol>\n<center class=\"half\">\n    <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291515746.png\" alt=\"20220114-e0917cd4\" width=\"200\" height=\"200\"><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291518515.png\" alt=\"20220112-4b9a80b5\" width=\"200\" height=\"200\">\n</center>\n<hr>\n<h2 id=\"算法框架介绍\">算法框架介绍</h2>\n<p><strong>云台跟踪联动算法分为两大部分，第一部分是基于深度学习的目标检测算法和目标跟踪算法，<br>\n第二部分是基于PID调节的云台控制算法和底盘控制算法。</strong></p>\n<p><strong>流程图如下：</strong></p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291525682.png\" style=\"zoom: 50%;\">\n<p><strong>淡蓝色部分为检测跟踪算法的流程，深蓝色部分为云台和底盘联动控制算法部分</strong></p>\n<hr>\n<h2 id=\"云台ROS驱动介绍\">云台ROS驱动介绍</h2>\n<p><strong>云台ROS驱动，根据云台供应商给的串口指令集进行串口编码控制，在源码中serial_ros包主要用来<br>\n将云台的控制指令通过ROS进行转换为串口的指令下发给云台控制板。其中主要需要启动的ROS节点是<br>\nserial_node 和 moni。</strong></p>\n<p>启动方式为：<br>\n<code>$ rosrun serial_ros serial_node</code><br>\n<code>$ rosrun serial_ros moni</code></p>\n<p><strong>同时云台驱动中也包含了相机启动和KCF跟踪的fv_tracking包，主要启动的ROS节点为<br>\nweb_cam和tracker_kcf</strong>。</p>\n<p>启动方式为：<br>\n<code>$ rosrun fv_tracking web_cam</code><br>\n<code>$rosrun fv_tracking tracker_kcf</code></p>\n<p><strong>启动后，相机画面及跟踪窗口效果如下：</strong></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291526605.png\" alt=\"20220116-67901f3c\"></p>\n<hr>\n<h2 id=\"检测算法介绍\">检测算法介绍</h2>\n<p><strong>使用的目标检测算法为YOLOX，YOLOX的github<a href=\"https://github.com/Megvii-BaseDetection/YOLOX\" title=\"YOLOX\">链接</a></strong><br>\n<strong>将YOLOX的源码整合为ROS代码下，改为yolox_ros的ROS包</strong></p>\n<p>启动方式为：<br>\n<code>$roslaunch yolox_ros yolox_ros.launch</code><br>\n<strong>启动之后，yolox_ros节点将会订阅来自云台相机的原始图像数据，生成检测结果并展示在桌面上。</strong></p>\n<p><strong>yolo_ros</strong>包中发布的话题为：<strong>“/yolox/bounding_boxes”</strong> 和 <strong>&quot; /yolox/image_raw&quot;</strong>。<br>\n<strong>yolo_ros</strong>包中订阅的话题为：<strong>“/camera/rgb/image_raw”</strong>。</p>\n<p><strong>启动后，检测算法窗口效果如下：</strong></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291526682.png\" alt=\"20220116-76d60e91\"></p>\n<hr>\n<h2 id=\"跟踪算法介绍\">跟踪算法介绍</h2>\n<p><strong>使用的目标跟踪算法SiamFC++，SiamFC++的github<a href=\"https://github.com/MegviiDetection/video_analyst\" title=\"SiamFC++\">链接</a></strong>。<br>\n<strong>基于SiamFC++的源码整合为ROS代码下，开发了siamfc_ros的ROS包</strong>。</p>\n<p>启动方式为：<br>\n<code>$roslaunch siamfc_ros siamfc_ros.launch</code><br>\n<strong>启动之后，siamfc_ros节点将会订阅来自云台相机的原始图像数据，以及来自yolox_ros节</strong><br>\n<strong>点的检测到的物体选框，物体选框当前为自动选则模式，选择被检测到的第一个物体，下一步迭代</strong><br>\n<strong>会添加，优化一个ID选择器，让用户可以选择当前画面中被检测到物体的ID，然后输入到跟踪</strong><br>\n<strong>器中，会生成跟踪结果并展示在桌面上。</strong></p>\n<p><strong>siamfc_ros</strong>包中发布的话题为：<strong>“/siamfc/image_raw”</strong> 和 <strong>“/siamfc/bounding_box”</strong>。<br>\n<strong>siamfc_ros</strong>包中订阅的话题为：<strong>&quot;/yolox/bounding_boxes &quot;</strong> 和 <strong>&quot;/camera/rgb/image_raw &quot;</strong>。</p>\n<p><strong>启动后，跟踪算法窗口效果如下：</strong></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291527559.png\" alt=\"20220116-3b5f0f20\"></p>\n<hr>\n<h2 id=\"联动控制算法介绍\">联动控制算法介绍</h2>\n<p><strong>联动控制算法主要流程是依赖云台的PID调节控制输出的yaw值和底盘的yaw值进行关联，当</strong><br>\n<strong>云台跟随目标进行转动的时候，将云台的yaw值实时发出到ROS话题</strong>&quot;/gimbal_camera/rpy_data&quot;<strong>上，然后底盘根据云台的</strong><br>\n<strong>yaw值进行处理，转换为底盘的航向角。</strong></p>\n<h3 id=\"云台控制算法\">云台控制算法</h3>\n<p><strong>云台控制算法采用的PID进行调节，PID控制的error主要是根据图像中心十字靶心像素坐标和被跟踪物体的boundingbox的物体中心的像素坐标的误差。</strong></p>\n<h3 id=\"底盘控制算法\">底盘控制算法</h3>\n<p><strong>底盘的控制算法，其中一部分基于视觉的大致观测距离判断进行线速度的控制，另一部分基于云台的yaw值<br>\n实时对角速度进行控制。</strong></p>\n<hr>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h2 id=\"系统部署方式介绍\">系统部署方式介绍</h2>\n<p><strong>此项目还在迭代中，部署方式采用本地host环境结合Docker环境进行算法的部署。<br>\n以后将会优化镜像部署，不使用本地环境，这样部署效率更高。</strong></p>\n<p>源码将要分成两部分分别部署在主从机上。**</p>\n<hr>\n<h3 id=\"Host主机部署\">Host主机部署</h3>\n<p><strong>主机上需要部署的源码是：fv_tracking, gimbal_control, serial_ros, bboxes_ex_msgs进行部署。</strong><br>\n<strong>创建一个gimbal_camera_ws工作空间, 将刚才从gitlab上拉取的源码中以上提到的四个包放在</strong><br>\n<strong>gimbal_camera_ws的src文件夹中。</strong></p>\n<ol>\n<li>\n<p><strong>编译以上的gimbal_camera_ws的源码，在当前工作空间的根目录下执行命令：</strong><br>\n<code>$ catkin_make</code></p>\n</li>\n<li>\n<p><strong>编译完成后，链接好相机的USB和串口的USB转换头到工控机上，使用命令：</strong><br>\n<code>$ ls /dev/video* </code> <code>$ ls /dev/ttyUSB0</code><strong>去确认是否已经读取到云台相机和云台串口。</strong></p>\n</li>\n</ol>\n<p><em><strong>备注：在代码中，默认启动和绑定的设备是&quot; /dev/video0&quot; 和 “/dev/ttyUSB0”， 工控<br>\n机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException</strong></em></p>\n<hr>\n<h3 id=\"Docker从机部署\">Docker从机部署</h3>\n<p><strong>Docker镜像目前存储在公司的Harbor仓库中，当前维护版本镜像名为</strong><br>\n<strong>pix/gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1</strong></p>\n<ol>\n<li>\n<p><strong>实例化一个容器，镜像构建容器的启动命令：</strong><br>\n<code>$docker run -it --rm --gpus all --net host --privileged --ipc=host --volume /home/t/gimbal-camera-tracking/ gimbal_camera_ws/:/workspace/gimbal_camera_ws  xxx镜像名</code></p>\n</li>\n<li>\n<p><strong>启动之后会进入到容器的内部，然后会将本地的源码映射到docker环境中的/workspace</strong><br>\n<strong>文件夹下的/gimbal_camera_ws</strong></p>\n</li>\n</ol>\n<p><em><strong>备注：在镜像构建容器的启动命令时，需要加入自己将源码放置的路径，如上命令中，&quot;/home/t/gimbal-camera-tracking/gimbal_camera_ws&quot;就是工控机默认将源码</strong></em><br>\n<em><strong>放在/home/t/路径下，源码的拉取下来的文件夹名为&quot;gimbal-camera-tracking&quot;</strong></em></p>\n<hr>\n<h3 id=\"系统启动方式\">系统启动方式</h3>\n<p><strong>完成上面的主从机的部署后，确认主机的gimbal_camera_ws和从机docker中映射的</strong><br>\n<strong>gimbal_camera_ws中的源码都已经编译好了，然后将&quot;source /home/t/</strong><br>\n<strong>gimbal_camera_ws/devel/setup.bash&quot;和&quot;source /workspace/gimbal_camera_ws/devel/setup.bash&quot;</strong><br>\n<strong>分别添加到主机和docker从机的&quot;~/.bashrc&quot;环境中, 重新source下环境，以初始化环境变量的配置。</strong></p>\n<p><strong>接下来是启动步骤：</strong></p>\n<ol>\n<li><strong>检查是否可以读取到相机图像和串口信息</strong><br>\n使用<code>$ cheese</code>或者<code>$ rosrun  fv_tracking web_cam</code>,命令来读取相机图像，看是否能够打开web_cam节点并读到云台相机图像。<br>\n使用<code>$ rosrun serial_ros serial_node</code> 和<code>$ rosrun serial_ros moni</code>，看是否能够读取到云台的姿态信息输出。<br>\n确认都能读取到数据之后，将cheese关闭，<strong>serial_node和moni就不用关闭。</strong><br>\n如图：</li>\n</ol>\n<center class=\"half\">\n    <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291508803.png\" width=\"200\" height=\"200\"><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291510788.png\" alt=\"20220114-cddd0cdf\" width=\"200\" height=\"200\"><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291511050.png\" alt=\"20220114-923e402c\" width=\"200\" height=\"200\">\n</center>\n<ol start=\"2\">\n<li>\n<p><strong>启动相机节点</strong><br>\n在主机中，使用命令<code>$ rosrun  fv_tracking web_cam</code>，启动了相机视频流读取节点。</p>\n</li>\n<li>\n<p><strong>启动YOLOX的ROS节点</strong><br>\n在docker中，使用命令<code>$ roslaunch yolox_ros yolox_ros.launch</code>，启动了检测器，检测实时图像中的物体。</p>\n</li>\n<li>\n<p><strong>启动SiamFC++的ROS节点, 确认你需要跟踪的目标在当前相机的视野范围内</strong><br>\n在docker中，使用命令<code>$ roslaunch siamfc_ros siamfc_ros.launch</code>，跟踪器成功锁定需要被跟踪的目标。</p>\n</li>\n<li>\n<p><strong>启动主机中的fv_tracking包中的track_kcf的ROS节点, 确认被跟踪的物体是你想要的目标</strong><br>\n在主机中，使用命令<code>$ rosrun fv_tracking tracker_kcf</code>，云台开始转动跟随被锁定的目标。</p>\n</li>\n<li>\n<p><strong>启动底盘控制节点, 确认云台已经成功跟随需要被跟踪的目标</strong><br>\n在docker中，使用命令<code>$ roslaunch chassis_control chassis_control.launch</code>, 开启底盘控制节点，输出控制指令。</p>\n</li>\n<li>\n<p><strong>启动底盘驱动ROS节点</strong><br>\n在主机中，使用命令<code>$ roslaunch pix_driver pix_driver_read.launch</code><br>\n<code>$ roslaunch pix_driver pix_driver_write.launch</code></p>\n</li>\n<li>\n<p><strong>确定车辆状态，在遥控器上选择MOD为 self-driving模式，如图已经切换到自动驾驶模式，使用MOD按键进行切换</strong></p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291513665.png\" alt=\"20220114-8bd94d2e\" style=\"zoom: 25%;\">\n</li>\n</ol>\n<hr>\n<h2 id=\"硬件介绍\">硬件介绍</h2>\n<h3 id=\"云台硬件规格\">云台硬件规格</h3>\n<ol>\n<li><strong>吊舱云台是根据客户需要进行选型，选择支持变焦（10倍）的云台，具有400万有效像素，</strong><br>\n<strong>三自由度，最大航向角为±150度无极旋转，工作电流为240mA(@12V)，重量400g（含相机）。</strong></li>\n<li><strong>云台内部有两路视频流，一路1080P 30FPS本地H.264压缩，存储在设备内，另一路输</strong><br>\n<strong>出1080P 60FPS格式的HDMI信号，用于无线图传，支持PWM和串口控制。</strong></li>\n<li><strong>云台可以自动对焦，对焦时间小于1s，焦距范围为 F = 4.9 ~ 49mm，105dB宽动态范</strong><br>\n<strong>围，温度工作范围-10 至 55 摄氏度。</strong></li>\n</ol>\n<center class=\"half\">\n    <img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291515746.png\" alt=\"20220114-e0917cd4\" width=\"200\" height=\"200\"><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291518515.png\" alt=\"20220112-4b9a80b5\" width=\"200\" height=\"200\">\n</center>\n<hr>\n<h2 id=\"算法框架介绍\">算法框架介绍</h2>\n<p><strong>云台跟踪联动算法分为两大部分，第一部分是基于深度学习的目标检测算法和目标跟踪算法，<br>\n第二部分是基于PID调节的云台控制算法和底盘控制算法。</strong></p>\n<p><strong>流程图如下：</strong></p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291525682.png\" style=\"zoom: 50%;\">\n<p><strong>淡蓝色部分为检测跟踪算法的流程，深蓝色部分为云台和底盘联动控制算法部分</strong></p>\n<hr>\n<h2 id=\"云台ROS驱动介绍\">云台ROS驱动介绍</h2>\n<p><strong>云台ROS驱动，根据云台供应商给的串口指令集进行串口编码控制，在源码中serial_ros包主要用来<br>\n将云台的控制指令通过ROS进行转换为串口的指令下发给云台控制板。其中主要需要启动的ROS节点是<br>\nserial_node 和 moni。</strong></p>\n<p>启动方式为：<br>\n<code>$ rosrun serial_ros serial_node</code><br>\n<code>$ rosrun serial_ros moni</code></p>\n<p><strong>同时云台驱动中也包含了相机启动和KCF跟踪的fv_tracking包，主要启动的ROS节点为<br>\nweb_cam和tracker_kcf</strong>。</p>\n<p>启动方式为：<br>\n<code>$ rosrun fv_tracking web_cam</code><br>\n<code>$rosrun fv_tracking tracker_kcf</code></p>\n<p><strong>启动后，相机画面及跟踪窗口效果如下：</strong></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291526605.png\" alt=\"20220116-67901f3c\"></p>\n<hr>\n<h2 id=\"检测算法介绍\">检测算法介绍</h2>\n<p><strong>使用的目标检测算法为YOLOX，YOLOX的github<a href=\"https://github.com/Megvii-BaseDetection/YOLOX\" title=\"YOLOX\">链接</a></strong><br>\n<strong>将YOLOX的源码整合为ROS代码下，改为yolox_ros的ROS包</strong></p>\n<p>启动方式为：<br>\n<code>$roslaunch yolox_ros yolox_ros.launch</code><br>\n<strong>启动之后，yolox_ros节点将会订阅来自云台相机的原始图像数据，生成检测结果并展示在桌面上。</strong></p>\n<p><strong>yolo_ros</strong>包中发布的话题为：<strong>“/yolox/bounding_boxes”</strong> 和 <strong>&quot; /yolox/image_raw&quot;</strong>。<br>\n<strong>yolo_ros</strong>包中订阅的话题为：<strong>“/camera/rgb/image_raw”</strong>。</p>\n<p><strong>启动后，检测算法窗口效果如下：</strong></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291526682.png\" alt=\"20220116-76d60e91\"></p>\n<hr>\n<h2 id=\"跟踪算法介绍\">跟踪算法介绍</h2>\n<p><strong>使用的目标跟踪算法SiamFC++，SiamFC++的github<a href=\"https://github.com/MegviiDetection/video_analyst\" title=\"SiamFC++\">链接</a></strong>。<br>\n<strong>基于SiamFC++的源码整合为ROS代码下，开发了siamfc_ros的ROS包</strong>。</p>\n<p>启动方式为：<br>\n<code>$roslaunch siamfc_ros siamfc_ros.launch</code><br>\n<strong>启动之后，siamfc_ros节点将会订阅来自云台相机的原始图像数据，以及来自yolox_ros节</strong><br>\n<strong>点的检测到的物体选框，物体选框当前为自动选则模式，选择被检测到的第一个物体，下一步迭代</strong><br>\n<strong>会添加，优化一个ID选择器，让用户可以选择当前画面中被检测到物体的ID，然后输入到跟踪</strong><br>\n<strong>器中，会生成跟踪结果并展示在桌面上。</strong></p>\n<p><strong>siamfc_ros</strong>包中发布的话题为：<strong>“/siamfc/image_raw”</strong> 和 <strong>“/siamfc/bounding_box”</strong>。<br>\n<strong>siamfc_ros</strong>包中订阅的话题为：<strong>&quot;/yolox/bounding_boxes &quot;</strong> 和 <strong>&quot;/camera/rgb/image_raw &quot;</strong>。</p>\n<p><strong>启动后，跟踪算法窗口效果如下：</strong></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/29/202304291527559.png\" alt=\"20220116-3b5f0f20\"></p>\n<hr>\n<h2 id=\"联动控制算法介绍\">联动控制算法介绍</h2>\n<p><strong>联动控制算法主要流程是依赖云台的PID调节控制输出的yaw值和底盘的yaw值进行关联，当</strong><br>\n<strong>云台跟随目标进行转动的时候，将云台的yaw值实时发出到ROS话题</strong>&quot;/gimbal_camera/rpy_data&quot;<strong>上，然后底盘根据云台的</strong><br>\n<strong>yaw值进行处理，转换为底盘的航向角。</strong></p>\n<h3 id=\"云台控制算法\">云台控制算法</h3>\n<p><strong>云台控制算法采用的PID进行调节，PID控制的error主要是根据图像中心十字靶心像素坐标和被跟踪物体的boundingbox的物体中心的像素坐标的误差。</strong></p>\n<h3 id=\"底盘控制算法\">底盘控制算法</h3>\n<p><strong>底盘的控制算法，其中一部分基于视觉的大致观测距离判断进行线速度的控制，另一部分基于云台的yaw值<br>\n实时对角速度进行控制。</strong></p>\n<hr>\n"},{"title":"跟踪项目说明书","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":null,"date":"2023-03-22T13:24:47.000Z","updated":"2023-03-22T13:24:47.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/walle.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***视觉跟踪项目使用说明书***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n# 目录\n\n一、**硬件安装部署**\n\n- 云台相机安装及调试\n\n  - 1.云台相机HDMI转USB3.0接口安装调试\n\n  - 2.云台串口线接口安装调试\n\n  - 3.云台供电线接口安装调试\n\n- 底盘CAN接口安装调试\n\n  - 1.检查CAN口线路连接是否不松动\n\n  - 2.使用candump检查CAN通信\n\n二、**软件安装部署**\n\n- docker深度学习环境部署\n\n- 从机代码编译部署\n\n- 主机代码编译部署\n\n三、**测试部署情况**\n\n- 相机硬件\n\n- 相机软件\n\n四、**快速开始**\n\n- 使用launch方式启动\n\n***\n\n\n\n# 一、硬件安装部署\n\n\n\n## 云台相机安装及调试\n\n### 1. 云台相机HDMI转USB3.0接口安装调试\n\n云台默认提供HDMI的输出，所以我们需要使用HDMI转USB3.0的线进行数据接口转换。这根线的功能是将hdmi的输出转换为USB可读取识别的一个video设备。\n\n![image](https://www.synotech.top:5523/uploads/2023/04/12/202304122155832.png)\n\n```text\n$ ls /dev/video* \n正常的情况是输出有以下的设备\n“/dev/video0” “/dev/video1”\n一般来说，默认只插入了一个相机设备，只会有/dev/video0，插入多个设备的时候，\n会增加设备/dev/video1, 代码中，我们默认使用/dev/video0，所以需要让云台相机\n是作为第一个插入到工控机USB接口上的相机设备，否则会报错，找不到设备。\n```\n\n![HDMI转USB](https://www.synotech.top:5523/uploads/2023/04/12/202304122156854.png)\n\n\n\n### 2. 云台串口线接口安装调试\n\n云台的串口线需要将云台的IO口按照以下示意图的方式使用USB转TTL线进行连接，才可以将另一端的USB口接到工控机上。这根线的功能是tty转成USB可识别的一个tty设备。\n\n![串口链接](https://www.synotech.top:5523/uploads/2023/04/12/202304122156420.png)\n\n使用以下命令查看是否设备在Ubuntu系统上已经绑定：\n\n![查看串口设备](https://www.synotech.top:5523/uploads/2023/04/12/202304122157444.png)\n\n```text\n$ ls /dev/ttyUSB0\n正常情况下，会输出设备存在\n显示“/dev/ttyUSB0”的串口设备，同样，按正常情况我们只会插入一个串口设备，所以默认读取\n的设备是/dev/ttyUSB0，所以务必确认第一个插入的串口设备是云台的串口转USB设备接口。\n```\n\n**注意：在代码中，默认启动和绑定的设备是\" /dev/video0\" 和 \"/dev/ttyUSB0\"， 工控机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException**\n\n串口的接线图也可以参考如下与Pixhawk的连接方式：\n\n![串口接线示意图](https://www.synotech.top:5523/uploads/2023/04/12/202304122157870.png)\n\n\n\n\n\n\n\n### 3. 云台供电线接口安装调试\n\n云台电压的输入为12V，不要使用高于12V的电压输入，底盘默认给的供电端口是12V的。\n\n使用底盘给的12V输出的点源端口，当插入供电端口后，过几秒钟，云台相机会有一个“滴滴”的提示音，同时云台会转动到默认的初始位置。\n\n**注意：请使用底盘的12V电源供电线或者同等标准输出12V的电源适配器，高于12V的电压可能导致云台控制器内部的电子元件造成损伤。**\n\n\n\n## 底盘CAN接口安装调试\n\n### 1.检查CAN口线路连接是否不松动\n\n![CAN口连接状态](https://www.synotech.top:5523/uploads/2023/04/12/202304122158048.png)\n\n\n\n### 2. 使用candump检查CAN通信\n\n```text\n#CAN口数据调试\n$ candump can0 或者 candump can1 看你自己实际将CAN线接在哪个接口\n正常情况下，执行上面的命令之后，会输出串口的信息如下图所示。\n```\n\n![检查CAN通信](https://www.synotech.top:5523/uploads/2023/04/12/202304122158278.png)\n\n\n\n# 二、软件安装部署\n\n## 1. docker深度学习环境部署\n\n拉取镜像或者根据用我们提供的镜像压缩包直接导入环境（根据压缩包导入环境更快捷）\n\n```text\n# 使用load在本地进行docker虚拟环境的导入\ndocker load < gimbal-camera-tracking.tar\n```\n\n**注意：目前在本地已经有该镜像只需要创建一个容器，然后进入到该容器的环境中即可。**\n\n\n\n## 2. 从机代码编译部署(docker中的环境是从机)\n\n创建从机在本地的ROS包，如下所示\n\n```text\n# 创建从机的ROS包，包名可以自己定义，这里默认是压缩包解压之后的文件情况\ngimbal-camera-tracking/gimbal_camera_ws,\n\n$ cd ~/  #进入ubuntu系统的主目录\n$ unzip gimbal-camera-tracking.zip #在系统的主目录下解压\n$ mkdir -p /gimbal_camera_ws/src #创建一个同名的ROS工作空间在主目录下\n手动将解压的源码几个包放在刚才创建的 ~/gimbal_camera_ws/src下，需要放在\n这个src路径下的包有：chassis_control fv_tracking gimbal_control pix_driver \nserial_ros serial bboxes_ex_msgs\n\n# 因为”bboxes_ex_msgs”在内部也用到了，所以拷贝bboxes_ex_msgs这个包在刚才\n解压的 gimbal_camera_tracking/src路径下。\n\n# launch文件夹在主从机的工作空间下都需要有一份\n\n最后主机环境下使用的ROS工作空间gimbal_camera_ws文件结构如下所示\nsrc\n├── bboxes_ex_msgs\n├── chassis_control\n├── fv_tracking\n├── gimbal_control\n├── launch\n├── pix_driver\n├── serial\n└── serial_ros\n\n从机docker环境下ROS工作空间gimbal_camera_ws文件结构如下所示\nsrc\n├── bboxes_ex_msgs\n├── launch\n├── siamfc_ros\n└── yolox_ros\n```\n\n创建docker环境\n\n```text\n$ docker images  #这个命令可以列出本机上已经有的image。\n\n#镜像名是你在上面导入虚拟环境的时候，在本地会有一个image，可以使用上面的命令进行查找\n$ docker run -it --rm --gpus all --net host --privileged --ipc=host \n--volume /home/t/gimbal-camera-tracking/gimbal_camera_ws/:/root/\ngimbal_camera_ws xxx镜像名\n(gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1)\n\n上面这个指令执行完之后，就立马进入到docker虚拟环境中。\n$ cd /root/gimbal_camera_ws  #进入到gimbal_camera_ws的工作空间\n$ catkin_make  #使用catkin工具进行编译，这个指令会将工作空间下的src文件下所有的\n\t\t\t   #源码都进行编译。\n$ source /root/gimbal_camera_ws/devel/setup.bash #source环境变量\n```\n\n\n\n## 3. 主机代码编译部署\n\n进入到主机代码的根目录，比如，拉取的代码根目录在本机~/gimbal_camera_ws路径下\n\n```text\n# 需要把之前在本地创建的~/gimbal_camera_ws\n进入到主机环境进行源码的编译\n$ cd ~/gimbal_camera_ws\n$ catkin_make\n$ source ~/gimbal_camera_ws/devel/setup.bash\n```\n\n\n\n# 三、测试部署情况\n\n## 1. 相机硬件\n\n```text\n#使用Cheese软件在Ubuntu系统上打开,或者使用以下命令打开\n$ cheese  #使用cheese工具进行相机图像的可视化，如果相机的接口正常，那么可以在\n\t\t  #cheese的界面中看到相机当前的图像。\n```\n\n## 2.相机软件\n\n```text\n# 如果是新开的terminal终端，需要先执行以下命令\n$ cd ~/gimbal_camera_ws\n$ source devel/setup.bash\n#在主机环境下，使用以下命令检查云台的串口是否打开正确\n$ rosrun serial_ros serial_node 和\n$ rosrun serial_ros moni\n正常情况会看到如下类似输出：\n```\n\n![云台串口输出](https://www.synotech.top:5523/uploads/2023/04/12/202304122159949.png)\n\n有了如图所示的输出之后，说明云台串口解析是正常的。\n\n\n\n# 四、快速开始\n\n## 使用launch方式启动\n\n1. 进入主机环境, 进入到launch文件夹下，执行host_gimbal_camera.launch，这个launch文件可以启动云台相机、串口、底盘控制、云台控制等ROS节点。\n\n```text\n#主机环境下launch启动\n$ cd ~/gimbal_camera_ws/launch\n$ roslaunch host_gimbal_camera.launch\n```\n\n**注意：启动的时候，如果有报错信息，需要检查下相机的USB和串口线是否接好。正常启动的时候，界面会弹出相机当前的图像画面。**\n\n\n\n1. 进入从机docker环境，进入到launch文件夹下，执行slave_track_system.launch, 这个launch文件可以启动YOLOX检测和SiamFC++跟踪的ROS节点，因为这两个节点主要依赖的深度学习环境都在docker创建的这个虚拟环境中。\n\n```text\n#从机环境下launch启动\n$ cd ~/gimbal_camera_ws/launch\n$ roslaunch slave_track_system.launch\n```\n\n**注意：启动这个launch文件之前，比如需要跟踪人体，需要被跟踪的目标距离车身1-2米的距离，尽量让物体的全貌出现在相机的图像中，这样launch之后，检测和跟踪器就可以立马锁定你需要跟踪的目标。正常启动的时候，界面会弹出一个YOLOX的检测界面和一个SiamFC++的跟踪界面以及一个被跟踪物体的模板图像（一个缩略图）。**\n\n\n\n1. 在主机环境下新开一个terminal终端，执行以下命令，进入到launch文件夹下，执行track_and_control.launch\n\n```text\n#主机环境下launch启动\n$ cd ~/gimbal_camera_ws/launch\n$ roslaunch track_and_control.launch\n```\n\n**注意：确保自动驾驶模式在上面的launch文件启动之前，是没有打开的。这么做的原因是：防止前面检测跟踪的步骤没有做好，导致目标没有跟踪正确，底盘会立马跟随一个错误的目标。因此，在确认前面两个步骤执行正确的情况下，再launch这一步的。**\n\n\n\n1. 确认目标现在已经被成功跟随，目标移动的同时，云台也跟随着转动，那么可以使用遥控器切换到自动驾驶模式，如下图所示：\n\n![遥控器切换自动驾驶](https://www.synotech.top:5523/uploads/2023/04/12/202304122200914.png)\n\n**注意：当遥控器切换到自动驾驶模式的时候，一定有人专门拿着遥控器以防出现，跟踪丢失之后的安全问题，当感觉车已经丢失目标的时候，需要按下如上图中遥控的红色圆圈圈出的位置的”MOD”按键，这样车子会立马切换到手动遥控模式，如果车辆还是没有停止，需要立马按下上图中，中间位置醒目的红色带有旋转符号的按键，这个按键是紧急停止按钮。**\n","source":"_posts/003-Projects/01-视觉跟踪/跟踪项目说明书.md","raw":"---\ntitle: 跟踪项目说明书\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax:\ndate: 2023-03-22 21:24:47\nupdated: 2023-03-22 21:24:47\ntags: Gimbal Camera Tracking\ncategories: \n- Projects\n- 视觉跟踪\nkeywords:\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***视觉跟踪项目使用说明书***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n# 目录\n\n一、**硬件安装部署**\n\n- 云台相机安装及调试\n\n  - 1.云台相机HDMI转USB3.0接口安装调试\n\n  - 2.云台串口线接口安装调试\n\n  - 3.云台供电线接口安装调试\n\n- 底盘CAN接口安装调试\n\n  - 1.检查CAN口线路连接是否不松动\n\n  - 2.使用candump检查CAN通信\n\n二、**软件安装部署**\n\n- docker深度学习环境部署\n\n- 从机代码编译部署\n\n- 主机代码编译部署\n\n三、**测试部署情况**\n\n- 相机硬件\n\n- 相机软件\n\n四、**快速开始**\n\n- 使用launch方式启动\n\n***\n\n\n\n# 一、硬件安装部署\n\n\n\n## 云台相机安装及调试\n\n### 1. 云台相机HDMI转USB3.0接口安装调试\n\n云台默认提供HDMI的输出，所以我们需要使用HDMI转USB3.0的线进行数据接口转换。这根线的功能是将hdmi的输出转换为USB可读取识别的一个video设备。\n\n![image](https://www.synotech.top:5523/uploads/2023/04/12/202304122155832.png)\n\n```text\n$ ls /dev/video* \n正常的情况是输出有以下的设备\n“/dev/video0” “/dev/video1”\n一般来说，默认只插入了一个相机设备，只会有/dev/video0，插入多个设备的时候，\n会增加设备/dev/video1, 代码中，我们默认使用/dev/video0，所以需要让云台相机\n是作为第一个插入到工控机USB接口上的相机设备，否则会报错，找不到设备。\n```\n\n![HDMI转USB](https://www.synotech.top:5523/uploads/2023/04/12/202304122156854.png)\n\n\n\n### 2. 云台串口线接口安装调试\n\n云台的串口线需要将云台的IO口按照以下示意图的方式使用USB转TTL线进行连接，才可以将另一端的USB口接到工控机上。这根线的功能是tty转成USB可识别的一个tty设备。\n\n![串口链接](https://www.synotech.top:5523/uploads/2023/04/12/202304122156420.png)\n\n使用以下命令查看是否设备在Ubuntu系统上已经绑定：\n\n![查看串口设备](https://www.synotech.top:5523/uploads/2023/04/12/202304122157444.png)\n\n```text\n$ ls /dev/ttyUSB0\n正常情况下，会输出设备存在\n显示“/dev/ttyUSB0”的串口设备，同样，按正常情况我们只会插入一个串口设备，所以默认读取\n的设备是/dev/ttyUSB0，所以务必确认第一个插入的串口设备是云台的串口转USB设备接口。\n```\n\n**注意：在代码中，默认启动和绑定的设备是\" /dev/video0\" 和 \"/dev/ttyUSB0\"， 工控机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException**\n\n串口的接线图也可以参考如下与Pixhawk的连接方式：\n\n![串口接线示意图](https://www.synotech.top:5523/uploads/2023/04/12/202304122157870.png)\n\n\n\n\n\n\n\n### 3. 云台供电线接口安装调试\n\n云台电压的输入为12V，不要使用高于12V的电压输入，底盘默认给的供电端口是12V的。\n\n使用底盘给的12V输出的点源端口，当插入供电端口后，过几秒钟，云台相机会有一个“滴滴”的提示音，同时云台会转动到默认的初始位置。\n\n**注意：请使用底盘的12V电源供电线或者同等标准输出12V的电源适配器，高于12V的电压可能导致云台控制器内部的电子元件造成损伤。**\n\n\n\n## 底盘CAN接口安装调试\n\n### 1.检查CAN口线路连接是否不松动\n\n![CAN口连接状态](https://www.synotech.top:5523/uploads/2023/04/12/202304122158048.png)\n\n\n\n### 2. 使用candump检查CAN通信\n\n```text\n#CAN口数据调试\n$ candump can0 或者 candump can1 看你自己实际将CAN线接在哪个接口\n正常情况下，执行上面的命令之后，会输出串口的信息如下图所示。\n```\n\n![检查CAN通信](https://www.synotech.top:5523/uploads/2023/04/12/202304122158278.png)\n\n\n\n# 二、软件安装部署\n\n## 1. docker深度学习环境部署\n\n拉取镜像或者根据用我们提供的镜像压缩包直接导入环境（根据压缩包导入环境更快捷）\n\n```text\n# 使用load在本地进行docker虚拟环境的导入\ndocker load < gimbal-camera-tracking.tar\n```\n\n**注意：目前在本地已经有该镜像只需要创建一个容器，然后进入到该容器的环境中即可。**\n\n\n\n## 2. 从机代码编译部署(docker中的环境是从机)\n\n创建从机在本地的ROS包，如下所示\n\n```text\n# 创建从机的ROS包，包名可以自己定义，这里默认是压缩包解压之后的文件情况\ngimbal-camera-tracking/gimbal_camera_ws,\n\n$ cd ~/  #进入ubuntu系统的主目录\n$ unzip gimbal-camera-tracking.zip #在系统的主目录下解压\n$ mkdir -p /gimbal_camera_ws/src #创建一个同名的ROS工作空间在主目录下\n手动将解压的源码几个包放在刚才创建的 ~/gimbal_camera_ws/src下，需要放在\n这个src路径下的包有：chassis_control fv_tracking gimbal_control pix_driver \nserial_ros serial bboxes_ex_msgs\n\n# 因为”bboxes_ex_msgs”在内部也用到了，所以拷贝bboxes_ex_msgs这个包在刚才\n解压的 gimbal_camera_tracking/src路径下。\n\n# launch文件夹在主从机的工作空间下都需要有一份\n\n最后主机环境下使用的ROS工作空间gimbal_camera_ws文件结构如下所示\nsrc\n├── bboxes_ex_msgs\n├── chassis_control\n├── fv_tracking\n├── gimbal_control\n├── launch\n├── pix_driver\n├── serial\n└── serial_ros\n\n从机docker环境下ROS工作空间gimbal_camera_ws文件结构如下所示\nsrc\n├── bboxes_ex_msgs\n├── launch\n├── siamfc_ros\n└── yolox_ros\n```\n\n创建docker环境\n\n```text\n$ docker images  #这个命令可以列出本机上已经有的image。\n\n#镜像名是你在上面导入虚拟环境的时候，在本地会有一个image，可以使用上面的命令进行查找\n$ docker run -it --rm --gpus all --net host --privileged --ipc=host \n--volume /home/t/gimbal-camera-tracking/gimbal_camera_ws/:/root/\ngimbal_camera_ws xxx镜像名\n(gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1)\n\n上面这个指令执行完之后，就立马进入到docker虚拟环境中。\n$ cd /root/gimbal_camera_ws  #进入到gimbal_camera_ws的工作空间\n$ catkin_make  #使用catkin工具进行编译，这个指令会将工作空间下的src文件下所有的\n\t\t\t   #源码都进行编译。\n$ source /root/gimbal_camera_ws/devel/setup.bash #source环境变量\n```\n\n\n\n## 3. 主机代码编译部署\n\n进入到主机代码的根目录，比如，拉取的代码根目录在本机~/gimbal_camera_ws路径下\n\n```text\n# 需要把之前在本地创建的~/gimbal_camera_ws\n进入到主机环境进行源码的编译\n$ cd ~/gimbal_camera_ws\n$ catkin_make\n$ source ~/gimbal_camera_ws/devel/setup.bash\n```\n\n\n\n# 三、测试部署情况\n\n## 1. 相机硬件\n\n```text\n#使用Cheese软件在Ubuntu系统上打开,或者使用以下命令打开\n$ cheese  #使用cheese工具进行相机图像的可视化，如果相机的接口正常，那么可以在\n\t\t  #cheese的界面中看到相机当前的图像。\n```\n\n## 2.相机软件\n\n```text\n# 如果是新开的terminal终端，需要先执行以下命令\n$ cd ~/gimbal_camera_ws\n$ source devel/setup.bash\n#在主机环境下，使用以下命令检查云台的串口是否打开正确\n$ rosrun serial_ros serial_node 和\n$ rosrun serial_ros moni\n正常情况会看到如下类似输出：\n```\n\n![云台串口输出](https://www.synotech.top:5523/uploads/2023/04/12/202304122159949.png)\n\n有了如图所示的输出之后，说明云台串口解析是正常的。\n\n\n\n# 四、快速开始\n\n## 使用launch方式启动\n\n1. 进入主机环境, 进入到launch文件夹下，执行host_gimbal_camera.launch，这个launch文件可以启动云台相机、串口、底盘控制、云台控制等ROS节点。\n\n```text\n#主机环境下launch启动\n$ cd ~/gimbal_camera_ws/launch\n$ roslaunch host_gimbal_camera.launch\n```\n\n**注意：启动的时候，如果有报错信息，需要检查下相机的USB和串口线是否接好。正常启动的时候，界面会弹出相机当前的图像画面。**\n\n\n\n1. 进入从机docker环境，进入到launch文件夹下，执行slave_track_system.launch, 这个launch文件可以启动YOLOX检测和SiamFC++跟踪的ROS节点，因为这两个节点主要依赖的深度学习环境都在docker创建的这个虚拟环境中。\n\n```text\n#从机环境下launch启动\n$ cd ~/gimbal_camera_ws/launch\n$ roslaunch slave_track_system.launch\n```\n\n**注意：启动这个launch文件之前，比如需要跟踪人体，需要被跟踪的目标距离车身1-2米的距离，尽量让物体的全貌出现在相机的图像中，这样launch之后，检测和跟踪器就可以立马锁定你需要跟踪的目标。正常启动的时候，界面会弹出一个YOLOX的检测界面和一个SiamFC++的跟踪界面以及一个被跟踪物体的模板图像（一个缩略图）。**\n\n\n\n1. 在主机环境下新开一个terminal终端，执行以下命令，进入到launch文件夹下，执行track_and_control.launch\n\n```text\n#主机环境下launch启动\n$ cd ~/gimbal_camera_ws/launch\n$ roslaunch track_and_control.launch\n```\n\n**注意：确保自动驾驶模式在上面的launch文件启动之前，是没有打开的。这么做的原因是：防止前面检测跟踪的步骤没有做好，导致目标没有跟踪正确，底盘会立马跟随一个错误的目标。因此，在确认前面两个步骤执行正确的情况下，再launch这一步的。**\n\n\n\n1. 确认目标现在已经被成功跟随，目标移动的同时，云台也跟随着转动，那么可以使用遥控器切换到自动驾驶模式，如下图所示：\n\n![遥控器切换自动驾驶](https://www.synotech.top:5523/uploads/2023/04/12/202304122200914.png)\n\n**注意：当遥控器切换到自动驾驶模式的时候，一定有人专门拿着遥控器以防出现，跟踪丢失之后的安全问题，当感觉车已经丢失目标的时候，需要按下如上图中遥控的红色圆圈圈出的位置的”MOD”按键，这样车子会立马切换到手动遥控模式，如果车辆还是没有停止，需要立马按下上图中，中间位置醒目的红色带有旋转符号的按键，这个按键是紧急停止按钮。**\n","slug":"003-Projects/01-视觉跟踪/跟踪项目说明书","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483sz0010u9rlet7307bp","content":"<h1><em><strong>视觉跟踪项目使用说明书</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h1>目录</h1>\n<p>一、<strong>硬件安装部署</strong></p>\n<ul>\n<li>\n<p>云台相机安装及调试</p>\n<ul>\n<li>\n<p>1.云台相机HDMI转USB3.0接口安装调试</p>\n</li>\n<li>\n<p>2.云台串口线接口安装调试</p>\n</li>\n<li>\n<p>3.云台供电线接口安装调试</p>\n</li>\n</ul>\n</li>\n<li>\n<p>底盘CAN接口安装调试</p>\n<ul>\n<li>\n<p>1.检查CAN口线路连接是否不松动</p>\n</li>\n<li>\n<p>2.使用candump检查CAN通信</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>二、<strong>软件安装部署</strong></p>\n<ul>\n<li>\n<p>docker深度学习环境部署</p>\n</li>\n<li>\n<p>从机代码编译部署</p>\n</li>\n<li>\n<p>主机代码编译部署</p>\n</li>\n</ul>\n<p>三、<strong>测试部署情况</strong></p>\n<ul>\n<li>\n<p>相机硬件</p>\n</li>\n<li>\n<p>相机软件</p>\n</li>\n</ul>\n<p>四、<strong>快速开始</strong></p>\n<ul>\n<li>使用launch方式启动</li>\n</ul>\n<hr>\n<h1>一、硬件安装部署</h1>\n<h2 id=\"云台相机安装及调试\">云台相机安装及调试</h2>\n<h3 id=\"1-云台相机HDMI转USB3-0接口安装调试\">1. 云台相机HDMI转USB3.0接口安装调试</h3>\n<p>云台默认提供HDMI的输出，所以我们需要使用HDMI转USB3.0的线进行数据接口转换。这根线的功能是将hdmi的输出转换为USB可读取识别的一个video设备。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122155832.png\" alt=\"image\"></p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ls /dev/video* </span><br><span class=\"line\">正常的情况是输出有以下的设备</span><br><span class=\"line\">“/dev/video0” “/dev/video1”</span><br><span class=\"line\">一般来说，默认只插入了一个相机设备，只会有/dev/video0，插入多个设备的时候，</span><br><span class=\"line\">会增加设备/dev/video1, 代码中，我们默认使用/dev/video0，所以需要让云台相机</span><br><span class=\"line\">是作为第一个插入到工控机USB接口上的相机设备，否则会报错，找不到设备。</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122156854.png\" alt=\"HDMI转USB\"></p>\n<h3 id=\"2-云台串口线接口安装调试\">2. 云台串口线接口安装调试</h3>\n<p>云台的串口线需要将云台的IO口按照以下示意图的方式使用USB转TTL线进行连接，才可以将另一端的USB口接到工控机上。这根线的功能是tty转成USB可识别的一个tty设备。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122156420.png\" alt=\"串口链接\"></p>\n<p>使用以下命令查看是否设备在Ubuntu系统上已经绑定：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122157444.png\" alt=\"查看串口设备\"></p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ls /dev/ttyUSB0</span><br><span class=\"line\">正常情况下，会输出设备存在</span><br><span class=\"line\">显示“/dev/ttyUSB0”的串口设备，同样，按正常情况我们只会插入一个串口设备，所以默认读取</span><br><span class=\"line\">的设备是/dev/ttyUSB0，所以务必确认第一个插入的串口设备是云台的串口转USB设备接口。</span><br></pre></td></tr></table></figure>\n<p><strong>注意：在代码中，默认启动和绑定的设备是&quot; /dev/video0&quot; 和 “/dev/ttyUSB0”， 工控机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException</strong></p>\n<p>串口的接线图也可以参考如下与Pixhawk的连接方式：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122157870.png\" alt=\"串口接线示意图\"></p>\n<h3 id=\"3-云台供电线接口安装调试\">3. 云台供电线接口安装调试</h3>\n<p>云台电压的输入为12V，不要使用高于12V的电压输入，底盘默认给的供电端口是12V的。</p>\n<p>使用底盘给的12V输出的点源端口，当插入供电端口后，过几秒钟，云台相机会有一个“滴滴”的提示音，同时云台会转动到默认的初始位置。</p>\n<p><strong>注意：请使用底盘的12V电源供电线或者同等标准输出12V的电源适配器，高于12V的电压可能导致云台控制器内部的电子元件造成损伤。</strong></p>\n<h2 id=\"底盘CAN接口安装调试\">底盘CAN接口安装调试</h2>\n<h3 id=\"1-检查CAN口线路连接是否不松动\">1.检查CAN口线路连接是否不松动</h3>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122158048.png\" alt=\"CAN口连接状态\"></p>\n<h3 id=\"2-使用candump检查CAN通信\">2. 使用candump检查CAN通信</h3>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#CAN口数据调试</span><br><span class=\"line\">$ candump can0 或者 candump can1 看你自己实际将CAN线接在哪个接口</span><br><span class=\"line\">正常情况下，执行上面的命令之后，会输出串口的信息如下图所示。</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122158278.png\" alt=\"检查CAN通信\"></p>\n<h1>二、软件安装部署</h1>\n<h2 id=\"1-docker深度学习环境部署\">1. docker深度学习环境部署</h2>\n<p>拉取镜像或者根据用我们提供的镜像压缩包直接导入环境（根据压缩包导入环境更快捷）</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 使用load在本地进行docker虚拟环境的导入</span><br><span class=\"line\">docker load &lt; gimbal-camera-tracking.tar</span><br></pre></td></tr></table></figure>\n<p><strong>注意：目前在本地已经有该镜像只需要创建一个容器，然后进入到该容器的环境中即可。</strong></p>\n<h2 id=\"2-从机代码编译部署-docker中的环境是从机\">2. 从机代码编译部署(docker中的环境是从机)</h2>\n<p>创建从机在本地的ROS包，如下所示</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 创建从机的ROS包，包名可以自己定义，这里默认是压缩包解压之后的文件情况</span><br><span class=\"line\">gimbal-camera-tracking/gimbal_camera_ws,</span><br><span class=\"line\"></span><br><span class=\"line\">$ cd ~/  #进入ubuntu系统的主目录</span><br><span class=\"line\">$ unzip gimbal-camera-tracking.zip #在系统的主目录下解压</span><br><span class=\"line\">$ mkdir -p /gimbal_camera_ws/src #创建一个同名的ROS工作空间在主目录下</span><br><span class=\"line\">手动将解压的源码几个包放在刚才创建的 ~/gimbal_camera_ws/src下，需要放在</span><br><span class=\"line\">这个src路径下的包有：chassis_control fv_tracking gimbal_control pix_driver </span><br><span class=\"line\">serial_ros serial bboxes_ex_msgs</span><br><span class=\"line\"></span><br><span class=\"line\"># 因为”bboxes_ex_msgs”在内部也用到了，所以拷贝bboxes_ex_msgs这个包在刚才</span><br><span class=\"line\">解压的 gimbal_camera_tracking/src路径下。</span><br><span class=\"line\"></span><br><span class=\"line\"># launch文件夹在主从机的工作空间下都需要有一份</span><br><span class=\"line\"></span><br><span class=\"line\">最后主机环境下使用的ROS工作空间gimbal_camera_ws文件结构如下所示</span><br><span class=\"line\">src</span><br><span class=\"line\">├── bboxes_ex_msgs</span><br><span class=\"line\">├── chassis_control</span><br><span class=\"line\">├── fv_tracking</span><br><span class=\"line\">├── gimbal_control</span><br><span class=\"line\">├── launch</span><br><span class=\"line\">├── pix_driver</span><br><span class=\"line\">├── serial</span><br><span class=\"line\">└── serial_ros</span><br><span class=\"line\"></span><br><span class=\"line\">从机docker环境下ROS工作空间gimbal_camera_ws文件结构如下所示</span><br><span class=\"line\">src</span><br><span class=\"line\">├── bboxes_ex_msgs</span><br><span class=\"line\">├── launch</span><br><span class=\"line\">├── siamfc_ros</span><br><span class=\"line\">└── yolox_ros</span><br></pre></td></tr></table></figure>\n<p>创建docker环境</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker images  #这个命令可以列出本机上已经有的image。</span><br><span class=\"line\"></span><br><span class=\"line\">#镜像名是你在上面导入虚拟环境的时候，在本地会有一个image，可以使用上面的命令进行查找</span><br><span class=\"line\">$ docker run -it --rm --gpus all --net host --privileged --ipc=host </span><br><span class=\"line\">--volume /home/t/gimbal-camera-tracking/gimbal_camera_ws/:/root/</span><br><span class=\"line\">gimbal_camera_ws xxx镜像名</span><br><span class=\"line\">(gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1)</span><br><span class=\"line\"></span><br><span class=\"line\">上面这个指令执行完之后，就立马进入到docker虚拟环境中。</span><br><span class=\"line\">$ cd /root/gimbal_camera_ws  #进入到gimbal_camera_ws的工作空间</span><br><span class=\"line\">$ catkin_make  #使用catkin工具进行编译，这个指令会将工作空间下的src文件下所有的</span><br><span class=\"line\">\t\t\t   #源码都进行编译。</span><br><span class=\"line\">$ source /root/gimbal_camera_ws/devel/setup.bash #source环境变量</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-主机代码编译部署\">3. 主机代码编译部署</h2>\n<p>进入到主机代码的根目录，比如，拉取的代码根目录在本机~/gimbal_camera_ws路径下</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 需要把之前在本地创建的~/gimbal_camera_ws</span><br><span class=\"line\">进入到主机环境进行源码的编译</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws</span><br><span class=\"line\">$ catkin_make</span><br><span class=\"line\">$ source ~/gimbal_camera_ws/devel/setup.bash</span><br></pre></td></tr></table></figure>\n<h1>三、测试部署情况</h1>\n<h2 id=\"1-相机硬件\">1. 相机硬件</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#使用Cheese软件在Ubuntu系统上打开,或者使用以下命令打开</span><br><span class=\"line\">$ cheese  #使用cheese工具进行相机图像的可视化，如果相机的接口正常，那么可以在</span><br><span class=\"line\">\t\t  #cheese的界面中看到相机当前的图像。</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-相机软件\">2.相机软件</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 如果是新开的terminal终端，需要先执行以下命令</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws</span><br><span class=\"line\">$ source devel/setup.bash</span><br><span class=\"line\">#在主机环境下，使用以下命令检查云台的串口是否打开正确</span><br><span class=\"line\">$ rosrun serial_ros serial_node 和</span><br><span class=\"line\">$ rosrun serial_ros moni</span><br><span class=\"line\">正常情况会看到如下类似输出：</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122159949.png\" alt=\"云台串口输出\"></p>\n<p>有了如图所示的输出之后，说明云台串口解析是正常的。</p>\n<h1>四、快速开始</h1>\n<h2 id=\"使用launch方式启动\">使用launch方式启动</h2>\n<ol>\n<li>进入主机环境, 进入到launch文件夹下，执行host_gimbal_camera.launch，这个launch文件可以启动云台相机、串口、底盘控制、云台控制等ROS节点。</li>\n</ol>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#主机环境下launch启动</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws/launch</span><br><span class=\"line\">$ roslaunch host_gimbal_camera.launch</span><br></pre></td></tr></table></figure>\n<p><strong>注意：启动的时候，如果有报错信息，需要检查下相机的USB和串口线是否接好。正常启动的时候，界面会弹出相机当前的图像画面。</strong></p>\n<ol>\n<li>进入从机docker环境，进入到launch文件夹下，执行slave_track_system.launch, 这个launch文件可以启动YOLOX检测和SiamFC++跟踪的ROS节点，因为这两个节点主要依赖的深度学习环境都在docker创建的这个虚拟环境中。</li>\n</ol>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#从机环境下launch启动</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws/launch</span><br><span class=\"line\">$ roslaunch slave_track_system.launch</span><br></pre></td></tr></table></figure>\n<p><strong>注意：启动这个launch文件之前，比如需要跟踪人体，需要被跟踪的目标距离车身1-2米的距离，尽量让物体的全貌出现在相机的图像中，这样launch之后，检测和跟踪器就可以立马锁定你需要跟踪的目标。正常启动的时候，界面会弹出一个YOLOX的检测界面和一个SiamFC++的跟踪界面以及一个被跟踪物体的模板图像（一个缩略图）。</strong></p>\n<ol>\n<li>在主机环境下新开一个terminal终端，执行以下命令，进入到launch文件夹下，执行track_and_control.launch</li>\n</ol>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#主机环境下launch启动</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws/launch</span><br><span class=\"line\">$ roslaunch track_and_control.launch</span><br></pre></td></tr></table></figure>\n<p><strong>注意：确保自动驾驶模式在上面的launch文件启动之前，是没有打开的。这么做的原因是：防止前面检测跟踪的步骤没有做好，导致目标没有跟踪正确，底盘会立马跟随一个错误的目标。因此，在确认前面两个步骤执行正确的情况下，再launch这一步的。</strong></p>\n<ol>\n<li>确认目标现在已经被成功跟随，目标移动的同时，云台也跟随着转动，那么可以使用遥控器切换到自动驾驶模式，如下图所示：</li>\n</ol>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122200914.png\" alt=\"遥控器切换自动驾驶\"></p>\n<p><strong>注意：当遥控器切换到自动驾驶模式的时候，一定有人专门拿着遥控器以防出现，跟踪丢失之后的安全问题，当感觉车已经丢失目标的时候，需要按下如上图中遥控的红色圆圈圈出的位置的”MOD”按键，这样车子会立马切换到手动遥控模式，如果车辆还是没有停止，需要立马按下上图中，中间位置醒目的红色带有旋转符号的按键，这个按键是紧急停止按钮。</strong></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>视觉跟踪项目使用说明书</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h1>目录</h1>\n<p>一、<strong>硬件安装部署</strong></p>\n<ul>\n<li>\n<p>云台相机安装及调试</p>\n<ul>\n<li>\n<p>1.云台相机HDMI转USB3.0接口安装调试</p>\n</li>\n<li>\n<p>2.云台串口线接口安装调试</p>\n</li>\n<li>\n<p>3.云台供电线接口安装调试</p>\n</li>\n</ul>\n</li>\n<li>\n<p>底盘CAN接口安装调试</p>\n<ul>\n<li>\n<p>1.检查CAN口线路连接是否不松动</p>\n</li>\n<li>\n<p>2.使用candump检查CAN通信</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>二、<strong>软件安装部署</strong></p>\n<ul>\n<li>\n<p>docker深度学习环境部署</p>\n</li>\n<li>\n<p>从机代码编译部署</p>\n</li>\n<li>\n<p>主机代码编译部署</p>\n</li>\n</ul>\n<p>三、<strong>测试部署情况</strong></p>\n<ul>\n<li>\n<p>相机硬件</p>\n</li>\n<li>\n<p>相机软件</p>\n</li>\n</ul>\n<p>四、<strong>快速开始</strong></p>\n<ul>\n<li>使用launch方式启动</li>\n</ul>\n<hr>\n<h1>一、硬件安装部署</h1>\n<h2 id=\"云台相机安装及调试\">云台相机安装及调试</h2>\n<h3 id=\"1-云台相机HDMI转USB3-0接口安装调试\">1. 云台相机HDMI转USB3.0接口安装调试</h3>\n<p>云台默认提供HDMI的输出，所以我们需要使用HDMI转USB3.0的线进行数据接口转换。这根线的功能是将hdmi的输出转换为USB可读取识别的一个video设备。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122155832.png\" alt=\"image\"></p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ls /dev/video* </span><br><span class=\"line\">正常的情况是输出有以下的设备</span><br><span class=\"line\">“/dev/video0” “/dev/video1”</span><br><span class=\"line\">一般来说，默认只插入了一个相机设备，只会有/dev/video0，插入多个设备的时候，</span><br><span class=\"line\">会增加设备/dev/video1, 代码中，我们默认使用/dev/video0，所以需要让云台相机</span><br><span class=\"line\">是作为第一个插入到工控机USB接口上的相机设备，否则会报错，找不到设备。</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122156854.png\" alt=\"HDMI转USB\"></p>\n<h3 id=\"2-云台串口线接口安装调试\">2. 云台串口线接口安装调试</h3>\n<p>云台的串口线需要将云台的IO口按照以下示意图的方式使用USB转TTL线进行连接，才可以将另一端的USB口接到工控机上。这根线的功能是tty转成USB可识别的一个tty设备。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122156420.png\" alt=\"串口链接\"></p>\n<p>使用以下命令查看是否设备在Ubuntu系统上已经绑定：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122157444.png\" alt=\"查看串口设备\"></p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ls /dev/ttyUSB0</span><br><span class=\"line\">正常情况下，会输出设备存在</span><br><span class=\"line\">显示“/dev/ttyUSB0”的串口设备，同样，按正常情况我们只会插入一个串口设备，所以默认读取</span><br><span class=\"line\">的设备是/dev/ttyUSB0，所以务必确认第一个插入的串口设备是云台的串口转USB设备接口。</span><br></pre></td></tr></table></figure>\n<p><strong>注意：在代码中，默认启动和绑定的设备是&quot; /dev/video0&quot; 和 “/dev/ttyUSB0”， 工控机可能需要将/ttyUSB0的读写权限打开，否则可能会报错IOException</strong></p>\n<p>串口的接线图也可以参考如下与Pixhawk的连接方式：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122157870.png\" alt=\"串口接线示意图\"></p>\n<h3 id=\"3-云台供电线接口安装调试\">3. 云台供电线接口安装调试</h3>\n<p>云台电压的输入为12V，不要使用高于12V的电压输入，底盘默认给的供电端口是12V的。</p>\n<p>使用底盘给的12V输出的点源端口，当插入供电端口后，过几秒钟，云台相机会有一个“滴滴”的提示音，同时云台会转动到默认的初始位置。</p>\n<p><strong>注意：请使用底盘的12V电源供电线或者同等标准输出12V的电源适配器，高于12V的电压可能导致云台控制器内部的电子元件造成损伤。</strong></p>\n<h2 id=\"底盘CAN接口安装调试\">底盘CAN接口安装调试</h2>\n<h3 id=\"1-检查CAN口线路连接是否不松动\">1.检查CAN口线路连接是否不松动</h3>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122158048.png\" alt=\"CAN口连接状态\"></p>\n<h3 id=\"2-使用candump检查CAN通信\">2. 使用candump检查CAN通信</h3>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#CAN口数据调试</span><br><span class=\"line\">$ candump can0 或者 candump can1 看你自己实际将CAN线接在哪个接口</span><br><span class=\"line\">正常情况下，执行上面的命令之后，会输出串口的信息如下图所示。</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122158278.png\" alt=\"检查CAN通信\"></p>\n<h1>二、软件安装部署</h1>\n<h2 id=\"1-docker深度学习环境部署\">1. docker深度学习环境部署</h2>\n<p>拉取镜像或者根据用我们提供的镜像压缩包直接导入环境（根据压缩包导入环境更快捷）</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 使用load在本地进行docker虚拟环境的导入</span><br><span class=\"line\">docker load &lt; gimbal-camera-tracking.tar</span><br></pre></td></tr></table></figure>\n<p><strong>注意：目前在本地已经有该镜像只需要创建一个容器，然后进入到该容器的环境中即可。</strong></p>\n<h2 id=\"2-从机代码编译部署-docker中的环境是从机\">2. 从机代码编译部署(docker中的环境是从机)</h2>\n<p>创建从机在本地的ROS包，如下所示</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 创建从机的ROS包，包名可以自己定义，这里默认是压缩包解压之后的文件情况</span><br><span class=\"line\">gimbal-camera-tracking/gimbal_camera_ws,</span><br><span class=\"line\"></span><br><span class=\"line\">$ cd ~/  #进入ubuntu系统的主目录</span><br><span class=\"line\">$ unzip gimbal-camera-tracking.zip #在系统的主目录下解压</span><br><span class=\"line\">$ mkdir -p /gimbal_camera_ws/src #创建一个同名的ROS工作空间在主目录下</span><br><span class=\"line\">手动将解压的源码几个包放在刚才创建的 ~/gimbal_camera_ws/src下，需要放在</span><br><span class=\"line\">这个src路径下的包有：chassis_control fv_tracking gimbal_control pix_driver </span><br><span class=\"line\">serial_ros serial bboxes_ex_msgs</span><br><span class=\"line\"></span><br><span class=\"line\"># 因为”bboxes_ex_msgs”在内部也用到了，所以拷贝bboxes_ex_msgs这个包在刚才</span><br><span class=\"line\">解压的 gimbal_camera_tracking/src路径下。</span><br><span class=\"line\"></span><br><span class=\"line\"># launch文件夹在主从机的工作空间下都需要有一份</span><br><span class=\"line\"></span><br><span class=\"line\">最后主机环境下使用的ROS工作空间gimbal_camera_ws文件结构如下所示</span><br><span class=\"line\">src</span><br><span class=\"line\">├── bboxes_ex_msgs</span><br><span class=\"line\">├── chassis_control</span><br><span class=\"line\">├── fv_tracking</span><br><span class=\"line\">├── gimbal_control</span><br><span class=\"line\">├── launch</span><br><span class=\"line\">├── pix_driver</span><br><span class=\"line\">├── serial</span><br><span class=\"line\">└── serial_ros</span><br><span class=\"line\"></span><br><span class=\"line\">从机docker环境下ROS工作空间gimbal_camera_ws文件结构如下所示</span><br><span class=\"line\">src</span><br><span class=\"line\">├── bboxes_ex_msgs</span><br><span class=\"line\">├── launch</span><br><span class=\"line\">├── siamfc_ros</span><br><span class=\"line\">└── yolox_ros</span><br></pre></td></tr></table></figure>\n<p>创建docker环境</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker images  #这个命令可以列出本机上已经有的image。</span><br><span class=\"line\"></span><br><span class=\"line\">#镜像名是你在上面导入虚拟环境的时候，在本地会有一个image，可以使用上面的命令进行查找</span><br><span class=\"line\">$ docker run -it --rm --gpus all --net host --privileged --ipc=host </span><br><span class=\"line\">--volume /home/t/gimbal-camera-tracking/gimbal_camera_ws/:/root/</span><br><span class=\"line\">gimbal_camera_ws xxx镜像名</span><br><span class=\"line\">(gimbal-camera-tracking/pytorch-siamfc-yolox-ros-noetic/amd64:V1.1)</span><br><span class=\"line\"></span><br><span class=\"line\">上面这个指令执行完之后，就立马进入到docker虚拟环境中。</span><br><span class=\"line\">$ cd /root/gimbal_camera_ws  #进入到gimbal_camera_ws的工作空间</span><br><span class=\"line\">$ catkin_make  #使用catkin工具进行编译，这个指令会将工作空间下的src文件下所有的</span><br><span class=\"line\">\t\t\t   #源码都进行编译。</span><br><span class=\"line\">$ source /root/gimbal_camera_ws/devel/setup.bash #source环境变量</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-主机代码编译部署\">3. 主机代码编译部署</h2>\n<p>进入到主机代码的根目录，比如，拉取的代码根目录在本机~/gimbal_camera_ws路径下</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 需要把之前在本地创建的~/gimbal_camera_ws</span><br><span class=\"line\">进入到主机环境进行源码的编译</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws</span><br><span class=\"line\">$ catkin_make</span><br><span class=\"line\">$ source ~/gimbal_camera_ws/devel/setup.bash</span><br></pre></td></tr></table></figure>\n<h1>三、测试部署情况</h1>\n<h2 id=\"1-相机硬件\">1. 相机硬件</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#使用Cheese软件在Ubuntu系统上打开,或者使用以下命令打开</span><br><span class=\"line\">$ cheese  #使用cheese工具进行相机图像的可视化，如果相机的接口正常，那么可以在</span><br><span class=\"line\">\t\t  #cheese的界面中看到相机当前的图像。</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-相机软件\">2.相机软件</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 如果是新开的terminal终端，需要先执行以下命令</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws</span><br><span class=\"line\">$ source devel/setup.bash</span><br><span class=\"line\">#在主机环境下，使用以下命令检查云台的串口是否打开正确</span><br><span class=\"line\">$ rosrun serial_ros serial_node 和</span><br><span class=\"line\">$ rosrun serial_ros moni</span><br><span class=\"line\">正常情况会看到如下类似输出：</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122159949.png\" alt=\"云台串口输出\"></p>\n<p>有了如图所示的输出之后，说明云台串口解析是正常的。</p>\n<h1>四、快速开始</h1>\n<h2 id=\"使用launch方式启动\">使用launch方式启动</h2>\n<ol>\n<li>进入主机环境, 进入到launch文件夹下，执行host_gimbal_camera.launch，这个launch文件可以启动云台相机、串口、底盘控制、云台控制等ROS节点。</li>\n</ol>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#主机环境下launch启动</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws/launch</span><br><span class=\"line\">$ roslaunch host_gimbal_camera.launch</span><br></pre></td></tr></table></figure>\n<p><strong>注意：启动的时候，如果有报错信息，需要检查下相机的USB和串口线是否接好。正常启动的时候，界面会弹出相机当前的图像画面。</strong></p>\n<ol>\n<li>进入从机docker环境，进入到launch文件夹下，执行slave_track_system.launch, 这个launch文件可以启动YOLOX检测和SiamFC++跟踪的ROS节点，因为这两个节点主要依赖的深度学习环境都在docker创建的这个虚拟环境中。</li>\n</ol>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#从机环境下launch启动</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws/launch</span><br><span class=\"line\">$ roslaunch slave_track_system.launch</span><br></pre></td></tr></table></figure>\n<p><strong>注意：启动这个launch文件之前，比如需要跟踪人体，需要被跟踪的目标距离车身1-2米的距离，尽量让物体的全貌出现在相机的图像中，这样launch之后，检测和跟踪器就可以立马锁定你需要跟踪的目标。正常启动的时候，界面会弹出一个YOLOX的检测界面和一个SiamFC++的跟踪界面以及一个被跟踪物体的模板图像（一个缩略图）。</strong></p>\n<ol>\n<li>在主机环境下新开一个terminal终端，执行以下命令，进入到launch文件夹下，执行track_and_control.launch</li>\n</ol>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#主机环境下launch启动</span><br><span class=\"line\">$ cd ~/gimbal_camera_ws/launch</span><br><span class=\"line\">$ roslaunch track_and_control.launch</span><br></pre></td></tr></table></figure>\n<p><strong>注意：确保自动驾驶模式在上面的launch文件启动之前，是没有打开的。这么做的原因是：防止前面检测跟踪的步骤没有做好，导致目标没有跟踪正确，底盘会立马跟随一个错误的目标。因此，在确认前面两个步骤执行正确的情况下，再launch这一步的。</strong></p>\n<ol>\n<li>确认目标现在已经被成功跟随，目标移动的同时，云台也跟随着转动，那么可以使用遥控器切换到自动驾驶模式，如下图所示：</li>\n</ol>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122200914.png\" alt=\"遥控器切换自动驾驶\"></p>\n<p><strong>注意：当遥控器切换到自动驾驶模式的时候，一定有人专门拿着遥控器以防出现，跟踪丢失之后的安全问题，当感觉车已经丢失目标的时候，需要按下如上图中遥控的红色圆圈圈出的位置的”MOD”按键，这样车子会立马切换到手动遥控模式，如果车辆还是没有停止，需要立马按下上图中，中间位置醒目的红色带有旋转符号的按键，这个按键是紧急停止按钮。</strong></p>\n"},{"title":"CMake升级-Ubuntu","date":"2022-08-27T15:47:30.000Z","updated":"2022-08-27T15:47:30.000Z","keywords":null,"description":null,"top_img":null,"comments":1,"cover":"https://www.synotech.top:5523/wallpaper/5f056636a92cf_270_185.jpg","toc":true,"toc_number":true,"copyright_author":"Bluet","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"mathjax":null,"_content":"\n# ***CMake升级方法***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [线性建模](#线性建模 \"线性建模\")\n>     2.1. [什么是线性模型](#什么是线性模型 \"什么是线性模型\")\n>     2.2. [什么是损失函数](#什么是损失函数 \"什么是损失函数\")\n>     2.3. [对损失函数求偏导](#对损失函数求偏导 \"对损失函数求偏导\")\n>     2.4. [二阶导数的意义图解](#二阶导数的意义图解 \"二阶导数的意义图解\")\n>     2.5. [$w_0$和$w_1$的二阶导数](#$w_0$和$w_1$的二阶导数 \"$w_0$和$w_1$的二阶导数\")\n>3. [预测](#预测 \"预测\")\n>8. [正则化最小二乘法](#正则化最小二乘法 \"正则化最小二乘法\")\n> 9. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n>10. [引用文献](#引用文献 \"引用文献\")#下载图片接口 \"下载图片接口\")\n\n\n\n### 概述\n\nCmake升级是需要十分注意的，看了很多网上的博客，都是方式各异，也有不是很合理的方案。我们还是遵循自己作为一个开发者，全面了解Cmake具体的内容，再了解升级会有什么影响，再了解升级的不同种方式，最后让读者自己选择Cmake的升级策略。\n\n\n\n### Cmake介绍\n\nCmake是一个什么工具？Cmake开发开始于1999年，是为了解决需要一个跨平台构建环境的 [Insight Segmentation and Registration Toolkit](https://en.wikipedia.org/wiki/Insight_Segmentation_and_Registration_Toolkit) （ITK)而开发的。\n\n\n\n### Cmake升级的影响\n\n首先, 如果你只是在后续的一个项目中需要使用单一版本的cmake，那么你可以完全重新编译一个在本地，但是，如果你需要保留默认安装的Cmake版本，那么需要一些特殊的操作来管理多个cmake版本在你的主机上。在我自己的开发中，如果默认就把ROS安装好了或者其他的软件在老版本的Cmake下编译好的，那么当你安装了新的cmake, 并且覆盖了老版本的cmake链接，那么将很难找回。在之前的cmake下编译好的ROS驱动和包可以继续使用，只是当你需要安装编译新的包时候，可能会出现包之间由于不同版本cmake编译的原因，导致冲突或者依赖项不兼容的问题。总之，当你需要升级Ubuntu系统的cmake，需要使用update-alternatives进行管理。\n\n\n\n### Cmake的合理升级方法\n\n我这里以Ubuntu18为例，默认安装的是cmake-3.10，链接在/usr/bin/cmake下，使用命令：```$ cmake --version``` 可以查看版本。默认的cmake是通过apt-get install安装的，而我们需要使用源码编译的方式来升级会更方便定义它安装的位置在哪。\n\n\n\n1. 下载cmake源码\n\n   ``` shell\n   wget -O cmake-3.22.1.tar.gz https://cmake.org/files/v3.22/cmake-3.22.1.tar.gz\n   ```\n\n   根据自己需要的版本进行下载。\n\n   \n\n   ![image-20220830004817114](image-20220830004817114.png)\n\n   \n\n\n\n目前最新的版本是3.24，cmake是向后兼容的，但是编译好的包就无法进行修改。关于编译工具cmake的版本，个人觉得如果你项目是用来部署生产的，不需要考虑最新的安装包，选择发行stable的老一些的版本，可以避免后期加依赖的时候出现其他问题。\n\n2. 解压并进入到cmake文件内\n\n   ```shell\n   $ tar -xvzf cmake-3.22.1.tar.gz && cd cmake-3.22.1\n   ```\n\n3. 编译源码\n\n   ```shell\n   cmake -DCMAKE_INSTALL_PREFIX=/usr .\n   make\n   make install ##可能需要sudo权限\n   ##编译好的cmake将会把bin下的cmake可执行文件链接到 /usr/local/bin/cmake \n   ```\n\n4. 二进制包安装\n\n   ```shell\n   # 下载二进制包\n   wget https://github.com/Kitware/CMake/releases/download/v3.18.2/cmake-3.18.2-Linux-x86_64.tar.gz\n   tar zxvf cmake-3.18.2-Linux-x86_64.tar.gz\n   cd cmake-3.18.2\n   # 将文件夹内容拷贝到默认环境变量路径下，例如/usr\n   ```\n\n5. 快捷脚本安装\n\n   ```shell\n   $ wget -q -O cmake-linux.sh https://github.com/Kitware/CMake/releases/download/v3.17.0/cmake-3.17.0-Linux-x86_64.sh\n   $ sh cmake-linux.sh -- --skip-license --prefix=/usr\n   $ rm cmake-linux.sh\n   ```\n\n   \n\n6. 查看安装好的版本号\n\n   ```$ cmake --version```\n   正常的输出是对应你刚才下载的cmake版本号，因为你是用户安装，会默认放在'/usr/local/bin'路径下。\n\n7. 使用update-alternatives添加cmake的版本\n   ``` shell\n   \n   *# 第一个参数: --install 表示向update-alternatives注册服务名。* \n   \n   *# 第二个参数: 注册最终地址，成功后将会把命令在这个固定的目的地址做真实命令的软链，以后管理就是管理这个软链；*\n   \n    *# 第三个参数: 服务名，以后管理时以它为关联依据。*\n   \n    *# 第四个参数: 被管理的命令绝对路径。*\n   \n    *# 第五个参数: 优先级，数字越大优先级越高。*\n    \n   ##添加刚安装的cmake3.22.1，cmake 后面的路径是你刚才解压后文件的路径\n   sudo update-alternatives --install /usr/local/bin/cmake  cmake /home/t/Documents/cmake-3.22.1/bin/cmake  10  \n   \n   ##添加默认的cmake-3.10.2，默认的cmake链接在/usr/bin/cmake \n   sudo update-alternatives --install /usr/local/bin/cmake  cmake  /usr/bin/cmake  100 \n   \n   ##如果你重复上面的步骤，你可以再安装一个3.18.0\n   tar -xvzf cmake-3.18.0.tar.gz && cd cmake-3.18.0\n   #这个时候，不需要安装在/usr路径下，否则会覆盖之前安装的版本\n   ./bootstrap\n   make\n   #make编译之后，就已经把源码编译好了，会生成一个/bin目录，cmake的执行文件就在/bin下\n   \n   ##添加默认的cmake-3.18.0，默认的cmake链接在解压后的源码根目录的/bin下，比如我这里是 /home/t/Documents/cmake-3.18.0/bin/cmake\n   sudo update-alternatives --install /usr/local/bin/cmake  cmake /home/t/Documents/cmake-3.18.0/bin/cmake  1  \n   \n   这样我们就添加了三个cmake版本，两个是你刚才在用户下安装的，一个是默认的cmake deb包安装的。\n   \n   ```\n\n   如图所示：安装了三个版本的cmake，优先级可以自己定义，我这里定义默认的automode选择优先级最高的cmake-3.10.2，也就是/usr/bin/cmake链接的默认的cmake。\n\n   ![image-20220830010853110](image-20220830010853110.png)\n\n   可以自己使用--config 进行配置，选择你需要使用的cmake版本，当你项目需要使用高版本，你可以切换到高版本进行编译，需要低版本，就再切换到低版本进行编译，cmake之间的链接不会冲突干扰。\n\n   ![image-20220830010646685](image-20220830010646685.png)\n\n\n\n### 可能的误操作\n\n之前，不是很熟悉cmake管理的时候，会自己编译安装，但是发现安装之后，覆盖了原有deb包安装的cmake-3.10.2的链接，而且我还找不到，因为是deb安装的方式，没有源码就无法复制一个cmake执行文件，重新给个链接，导致需要卸载再重新安装，可能有其他方法，但是目前我还不知道怎么操作。因为需要卸载，\n\n卸载方式指令：```$ sudo apt remove cmake```, 在不清楚 ```$ sudo apt autoremove cmake```会产生什么效果之前，小心使用，一般都会在命令行提醒你，apt将会卸载哪些包。在我这里的情况是，因为ROS是在cmake-3.10下安装的，所以cmake卸载的话，会同步把ROS的安装包和驱动都一并的卸载，这也是很多小白最初不会操作的时候，经常犯错的。\n\n如果你确实没有办法恢复老版本的cmake，就像我这里一样，使用```$ sudo apt remove cmake```卸载cmake3.10，然后再使用 ```$ sudo apt autoremove cmake```看提示需要卸载什么ROS包，如果无关紧要，就执行autoremove的指令。最后，重新安装cmake，```$ sudo apt  install cmake```, 重新安装那些刚才你卸载的包，比如ROS。\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 参考引用文献：\n\nhttps://turbock79.cn/?p=2582 \n","source":"_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu.md","raw":"---\ntitle: CMake升级-Ubuntu\ndate: 2022-08-27 23:47:30\nupdated: 2022-08-27 23:47:30\ntags: \"CMake\"\ncategories: \n- Linux\n- Cmake\nkeywords: \ndescription:\ntop_img:\ncomments: true\ncover:\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\nmathjax: \n---\n\n# ***CMake升级方法***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [线性建模](#线性建模 \"线性建模\")\n>     2.1. [什么是线性模型](#什么是线性模型 \"什么是线性模型\")\n>     2.2. [什么是损失函数](#什么是损失函数 \"什么是损失函数\")\n>     2.3. [对损失函数求偏导](#对损失函数求偏导 \"对损失函数求偏导\")\n>     2.4. [二阶导数的意义图解](#二阶导数的意义图解 \"二阶导数的意义图解\")\n>     2.5. [$w_0$和$w_1$的二阶导数](#$w_0$和$w_1$的二阶导数 \"$w_0$和$w_1$的二阶导数\")\n>3. [预测](#预测 \"预测\")\n>8. [正则化最小二乘法](#正则化最小二乘法 \"正则化最小二乘法\")\n> 9. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n>10. [引用文献](#引用文献 \"引用文献\")#下载图片接口 \"下载图片接口\")\n\n\n\n### 概述\n\nCmake升级是需要十分注意的，看了很多网上的博客，都是方式各异，也有不是很合理的方案。我们还是遵循自己作为一个开发者，全面了解Cmake具体的内容，再了解升级会有什么影响，再了解升级的不同种方式，最后让读者自己选择Cmake的升级策略。\n\n\n\n### Cmake介绍\n\nCmake是一个什么工具？Cmake开发开始于1999年，是为了解决需要一个跨平台构建环境的 [Insight Segmentation and Registration Toolkit](https://en.wikipedia.org/wiki/Insight_Segmentation_and_Registration_Toolkit) （ITK)而开发的。\n\n\n\n### Cmake升级的影响\n\n首先, 如果你只是在后续的一个项目中需要使用单一版本的cmake，那么你可以完全重新编译一个在本地，但是，如果你需要保留默认安装的Cmake版本，那么需要一些特殊的操作来管理多个cmake版本在你的主机上。在我自己的开发中，如果默认就把ROS安装好了或者其他的软件在老版本的Cmake下编译好的，那么当你安装了新的cmake, 并且覆盖了老版本的cmake链接，那么将很难找回。在之前的cmake下编译好的ROS驱动和包可以继续使用，只是当你需要安装编译新的包时候，可能会出现包之间由于不同版本cmake编译的原因，导致冲突或者依赖项不兼容的问题。总之，当你需要升级Ubuntu系统的cmake，需要使用update-alternatives进行管理。\n\n\n\n### Cmake的合理升级方法\n\n我这里以Ubuntu18为例，默认安装的是cmake-3.10，链接在/usr/bin/cmake下，使用命令：```$ cmake --version``` 可以查看版本。默认的cmake是通过apt-get install安装的，而我们需要使用源码编译的方式来升级会更方便定义它安装的位置在哪。\n\n\n\n1. 下载cmake源码\n\n   ``` shell\n   wget -O cmake-3.22.1.tar.gz https://cmake.org/files/v3.22/cmake-3.22.1.tar.gz\n   ```\n\n   根据自己需要的版本进行下载。\n\n   \n\n   ![image-20220830004817114](image-20220830004817114.png)\n\n   \n\n\n\n目前最新的版本是3.24，cmake是向后兼容的，但是编译好的包就无法进行修改。关于编译工具cmake的版本，个人觉得如果你项目是用来部署生产的，不需要考虑最新的安装包，选择发行stable的老一些的版本，可以避免后期加依赖的时候出现其他问题。\n\n2. 解压并进入到cmake文件内\n\n   ```shell\n   $ tar -xvzf cmake-3.22.1.tar.gz && cd cmake-3.22.1\n   ```\n\n3. 编译源码\n\n   ```shell\n   cmake -DCMAKE_INSTALL_PREFIX=/usr .\n   make\n   make install ##可能需要sudo权限\n   ##编译好的cmake将会把bin下的cmake可执行文件链接到 /usr/local/bin/cmake \n   ```\n\n4. 二进制包安装\n\n   ```shell\n   # 下载二进制包\n   wget https://github.com/Kitware/CMake/releases/download/v3.18.2/cmake-3.18.2-Linux-x86_64.tar.gz\n   tar zxvf cmake-3.18.2-Linux-x86_64.tar.gz\n   cd cmake-3.18.2\n   # 将文件夹内容拷贝到默认环境变量路径下，例如/usr\n   ```\n\n5. 快捷脚本安装\n\n   ```shell\n   $ wget -q -O cmake-linux.sh https://github.com/Kitware/CMake/releases/download/v3.17.0/cmake-3.17.0-Linux-x86_64.sh\n   $ sh cmake-linux.sh -- --skip-license --prefix=/usr\n   $ rm cmake-linux.sh\n   ```\n\n   \n\n6. 查看安装好的版本号\n\n   ```$ cmake --version```\n   正常的输出是对应你刚才下载的cmake版本号，因为你是用户安装，会默认放在'/usr/local/bin'路径下。\n\n7. 使用update-alternatives添加cmake的版本\n   ``` shell\n   \n   *# 第一个参数: --install 表示向update-alternatives注册服务名。* \n   \n   *# 第二个参数: 注册最终地址，成功后将会把命令在这个固定的目的地址做真实命令的软链，以后管理就是管理这个软链；*\n   \n    *# 第三个参数: 服务名，以后管理时以它为关联依据。*\n   \n    *# 第四个参数: 被管理的命令绝对路径。*\n   \n    *# 第五个参数: 优先级，数字越大优先级越高。*\n    \n   ##添加刚安装的cmake3.22.1，cmake 后面的路径是你刚才解压后文件的路径\n   sudo update-alternatives --install /usr/local/bin/cmake  cmake /home/t/Documents/cmake-3.22.1/bin/cmake  10  \n   \n   ##添加默认的cmake-3.10.2，默认的cmake链接在/usr/bin/cmake \n   sudo update-alternatives --install /usr/local/bin/cmake  cmake  /usr/bin/cmake  100 \n   \n   ##如果你重复上面的步骤，你可以再安装一个3.18.0\n   tar -xvzf cmake-3.18.0.tar.gz && cd cmake-3.18.0\n   #这个时候，不需要安装在/usr路径下，否则会覆盖之前安装的版本\n   ./bootstrap\n   make\n   #make编译之后，就已经把源码编译好了，会生成一个/bin目录，cmake的执行文件就在/bin下\n   \n   ##添加默认的cmake-3.18.0，默认的cmake链接在解压后的源码根目录的/bin下，比如我这里是 /home/t/Documents/cmake-3.18.0/bin/cmake\n   sudo update-alternatives --install /usr/local/bin/cmake  cmake /home/t/Documents/cmake-3.18.0/bin/cmake  1  \n   \n   这样我们就添加了三个cmake版本，两个是你刚才在用户下安装的，一个是默认的cmake deb包安装的。\n   \n   ```\n\n   如图所示：安装了三个版本的cmake，优先级可以自己定义，我这里定义默认的automode选择优先级最高的cmake-3.10.2，也就是/usr/bin/cmake链接的默认的cmake。\n\n   ![image-20220830010853110](image-20220830010853110.png)\n\n   可以自己使用--config 进行配置，选择你需要使用的cmake版本，当你项目需要使用高版本，你可以切换到高版本进行编译，需要低版本，就再切换到低版本进行编译，cmake之间的链接不会冲突干扰。\n\n   ![image-20220830010646685](image-20220830010646685.png)\n\n\n\n### 可能的误操作\n\n之前，不是很熟悉cmake管理的时候，会自己编译安装，但是发现安装之后，覆盖了原有deb包安装的cmake-3.10.2的链接，而且我还找不到，因为是deb安装的方式，没有源码就无法复制一个cmake执行文件，重新给个链接，导致需要卸载再重新安装，可能有其他方法，但是目前我还不知道怎么操作。因为需要卸载，\n\n卸载方式指令：```$ sudo apt remove cmake```, 在不清楚 ```$ sudo apt autoremove cmake```会产生什么效果之前，小心使用，一般都会在命令行提醒你，apt将会卸载哪些包。在我这里的情况是，因为ROS是在cmake-3.10下安装的，所以cmake卸载的话，会同步把ROS的安装包和驱动都一并的卸载，这也是很多小白最初不会操作的时候，经常犯错的。\n\n如果你确实没有办法恢复老版本的cmake，就像我这里一样，使用```$ sudo apt remove cmake```卸载cmake3.10，然后再使用 ```$ sudo apt autoremove cmake```看提示需要卸载什么ROS包，如果无关紧要，就执行autoremove的指令。最后，重新安装cmake，```$ sudo apt  install cmake```, 重新安装那些刚才你卸载的包，比如ROS。\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 参考引用文献：\n\nhttps://turbock79.cn/?p=2582 \n","slug":"004-Linux/01-Cmake/Cmake升级-Ubuntu","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t00012u9rl503ucax8","content":"<h1><em><strong>CMake升级方法</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E5%BB%BA%E6%A8%A1\" title=\"线性建模\">线性建模</a><br>\n2.1. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\" title=\"什么是线性模型\">什么是线性模型</a><br>\n2.2. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\" title=\"什么是损失函数\">什么是损失函数</a><br>\n2.3. <a href=\"#%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC\" title=\"对损失函数求偏导\">对损失函数求偏导</a><br>\n2.4. <a href=\"#%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%9B%BE%E8%A7%A3\" title=\"二阶导数的意义图解\">二阶导数的意义图解</a><br>\n2.5. <a href=\"#$w_0$%E5%92%8C$w_1$%E7%9A%84%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0\" title=\"$w_0$和$w_1$的二阶导数\">$w_0$和$w_1$的二阶导数</a></li>\n<li><a href=\"#%E9%A2%84%E6%B5%8B\" title=\"预测\">预测</a></li>\n<li><a href=\"#%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95\" title=\"正则化最小二乘法\">正则化最小二乘法</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a>#下载图片接口 “下载图片接口”)</li>\n</ol>\n</blockquote>\n<h3 id=\"概述\">概述</h3>\n<p>Cmake升级是需要十分注意的，看了很多网上的博客，都是方式各异，也有不是很合理的方案。我们还是遵循自己作为一个开发者，全面了解Cmake具体的内容，再了解升级会有什么影响，再了解升级的不同种方式，最后让读者自己选择Cmake的升级策略。</p>\n<h3 id=\"Cmake介绍\">Cmake介绍</h3>\n<p>Cmake是一个什么工具？Cmake开发开始于1999年，是为了解决需要一个跨平台构建环境的 <a href=\"https://en.wikipedia.org/wiki/Insight_Segmentation_and_Registration_Toolkit\">Insight Segmentation and Registration Toolkit</a> （ITK)而开发的。</p>\n<h3 id=\"Cmake升级的影响\">Cmake升级的影响</h3>\n<p>首先, 如果你只是在后续的一个项目中需要使用单一版本的cmake，那么你可以完全重新编译一个在本地，但是，如果你需要保留默认安装的Cmake版本，那么需要一些特殊的操作来管理多个cmake版本在你的主机上。在我自己的开发中，如果默认就把ROS安装好了或者其他的软件在老版本的Cmake下编译好的，那么当你安装了新的cmake, 并且覆盖了老版本的cmake链接，那么将很难找回。在之前的cmake下编译好的ROS驱动和包可以继续使用，只是当你需要安装编译新的包时候，可能会出现包之间由于不同版本cmake编译的原因，导致冲突或者依赖项不兼容的问题。总之，当你需要升级Ubuntu系统的cmake，需要使用update-alternatives进行管理。</p>\n<h3 id=\"Cmake的合理升级方法\">Cmake的合理升级方法</h3>\n<p>我这里以Ubuntu18为例，默认安装的是cmake-3.10，链接在/usr/bin/cmake下，使用命令：<code>$ cmake --version</code> 可以查看版本。默认的cmake是通过apt-get install安装的，而我们需要使用源码编译的方式来升级会更方便定义它安装的位置在哪。</p>\n<ol>\n<li>\n<p>下载cmake源码</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget -O cmake-3.22.1.tar.gz https://cmake.org/files/v3.22/cmake-3.22.1.tar.gz</span><br></pre></td></tr></table></figure>\n<p>根据自己需要的版本进行下载。</p>\n<p><img src=\"/2022/08/27/004-Linux/01-Cmake/Cmake%E5%8D%87%E7%BA%A7-Ubuntu/image-20220830004817114.png\" alt=\"image-20220830004817114\"></p>\n</li>\n</ol>\n<p>目前最新的版本是3.24，cmake是向后兼容的，但是编译好的包就无法进行修改。关于编译工具cmake的版本，个人觉得如果你项目是用来部署生产的，不需要考虑最新的安装包，选择发行stable的老一些的版本，可以避免后期加依赖的时候出现其他问题。</p>\n<ol start=\"2\">\n<li>\n<p>解压并进入到cmake文件内</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">tar -xvzf cmake-3.22.1.tar.gz &amp;&amp; <span class=\"built_in\">cd</span> cmake-3.22.1</span></span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>编译源码</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake -DCMAKE_INSTALL_PREFIX=/usr .</span><br><span class=\"line\">make</span><br><span class=\"line\">make install ##可能需要sudo权限</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#编译好的cmake将会把bin下的cmake可执行文件链接到 /usr/local/bin/cmake</span></span> </span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>二进制包安装</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">下载二进制包</span></span><br><span class=\"line\">wget https://github.com/Kitware/CMake/releases/download/v3.18.2/cmake-3.18.2-Linux-x86_64.tar.gz</span><br><span class=\"line\">tar zxvf cmake-3.18.2-Linux-x86_64.tar.gz</span><br><span class=\"line\">cd cmake-3.18.2</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">将文件夹内容拷贝到默认环境变量路径下，例如/usr</span></span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>快捷脚本安装</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">wget -q -O cmake-linux.sh https://github.com/Kitware/CMake/releases/download/v3.17.0/cmake-3.17.0-Linux-x86_64.sh</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">sh cmake-linux.sh -- --skip-license --prefix=/usr</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">rm</span> cmake-linux.sh</span></span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>查看安装好的版本号</p>\n<p><code>$ cmake --version</code><br>\n正常的输出是对应你刚才下载的cmake版本号，因为你是用户安装，会默认放在’/usr/local/bin’路径下。</p>\n</li>\n<li>\n<p>使用update-alternatives添加cmake的版本</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">*# 第一个参数: --install 表示向update-alternatives注册服务名。* </span><br><span class=\"line\"></span><br><span class=\"line\">*# 第二个参数: 注册最终地址，成功后将会把命令在这个固定的目的地址做真实命令的软链，以后管理就是管理这个软链；*</span><br><span class=\"line\"></span><br><span class=\"line\"> *# 第三个参数: 服务名，以后管理时以它为关联依据。*</span><br><span class=\"line\"></span><br><span class=\"line\"> *# 第四个参数: 被管理的命令绝对路径。*</span><br><span class=\"line\"></span><br><span class=\"line\"> *# 第五个参数: 优先级，数字越大优先级越高。*</span><br><span class=\"line\"><span class=\"meta prompt_\"> </span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#添加刚安装的cmake3.22.1，cmake 后面的路径是你刚才解压后文件的路径</span></span></span><br><span class=\"line\">sudo update-alternatives --install /usr/local/bin/cmake  cmake /home/t/Documents/cmake-3.22.1/bin/cmake  10  </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#添加默认的cmake-3.10.2，默认的cmake链接在/usr/bin/cmake</span></span> </span><br><span class=\"line\">sudo update-alternatives --install /usr/local/bin/cmake  cmake  /usr/bin/cmake  100 </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#如果你重复上面的步骤，你可以再安装一个3.18.0</span></span></span><br><span class=\"line\">tar -xvzf cmake-3.18.0.tar.gz &amp;&amp; cd cmake-3.18.0</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">这个时候，不需要安装在/usr路径下，否则会覆盖之前安装的版本</span></span><br><span class=\"line\">./bootstrap</span><br><span class=\"line\">make</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">make编译之后，就已经把源码编译好了，会生成一个/bin目录，cmake的执行文件就在/bin下</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#添加默认的cmake-3.18.0，默认的cmake链接在解压后的源码根目录的/bin下，比如我这里是 /home/t/Documents/cmake-3.18.0/bin/cmake</span></span></span><br><span class=\"line\">sudo update-alternatives --install /usr/local/bin/cmake  cmake /home/t/Documents/cmake-3.18.0/bin/cmake  1  </span><br><span class=\"line\"></span><br><span class=\"line\">这样我们就添加了三个cmake版本，两个是你刚才在用户下安装的，一个是默认的cmake deb包安装的。</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>如图所示：安装了三个版本的cmake，优先级可以自己定义，我这里定义默认的automode选择优先级最高的cmake-3.10.2，也就是/usr/bin/cmake链接的默认的cmake。</p>\n<p><img src=\"/2022/08/27/004-Linux/01-Cmake/Cmake%E5%8D%87%E7%BA%A7-Ubuntu/image-20220830010853110.png\" alt=\"image-20220830010853110\"></p>\n<p>可以自己使用–config 进行配置，选择你需要使用的cmake版本，当你项目需要使用高版本，你可以切换到高版本进行编译，需要低版本，就再切换到低版本进行编译，cmake之间的链接不会冲突干扰。</p>\n<p><img src=\"/2022/08/27/004-Linux/01-Cmake/Cmake%E5%8D%87%E7%BA%A7-Ubuntu/image-20220830010646685.png\" alt=\"image-20220830010646685\"></p>\n</li>\n</ol>\n<h3 id=\"可能的误操作\">可能的误操作</h3>\n<p>之前，不是很熟悉cmake管理的时候，会自己编译安装，但是发现安装之后，覆盖了原有deb包安装的cmake-3.10.2的链接，而且我还找不到，因为是deb安装的方式，没有源码就无法复制一个cmake执行文件，重新给个链接，导致需要卸载再重新安装，可能有其他方法，但是目前我还不知道怎么操作。因为需要卸载，</p>\n<p>卸载方式指令：<code>$ sudo apt remove cmake</code>, 在不清楚 <code>$ sudo apt autoremove cmake</code>会产生什么效果之前，小心使用，一般都会在命令行提醒你，apt将会卸载哪些包。在我这里的情况是，因为ROS是在cmake-3.10下安装的，所以cmake卸载的话，会同步把ROS的安装包和驱动都一并的卸载，这也是很多小白最初不会操作的时候，经常犯错的。</p>\n<p>如果你确实没有办法恢复老版本的cmake，就像我这里一样，使用<code>$ sudo apt remove cmake</code>卸载cmake3.10，然后再使用 <code>$ sudo apt autoremove cmake</code>看提示需要卸载什么ROS包，如果无关紧要，就执行autoremove的指令。最后，重新安装cmake，<code>$ sudo apt  install cmake</code>, 重新安装那些刚才你卸载的包，比如ROS。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"参考引用文献：\">参考引用文献：</h3>\n<p><a href=\"https://turbock79.cn/?p=2582\">https://turbock79.cn/?p=2582</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>CMake升级方法</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%BA%BF%E6%80%A7%E5%BB%BA%E6%A8%A1\" title=\"线性建模\">线性建模</a><br>\n2.1. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\" title=\"什么是线性模型\">什么是线性模型</a><br>\n2.2. <a href=\"#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\" title=\"什么是损失函数\">什么是损失函数</a><br>\n2.3. <a href=\"#%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%81%8F%E5%AF%BC\" title=\"对损失函数求偏导\">对损失函数求偏导</a><br>\n2.4. <a href=\"#%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%9B%BE%E8%A7%A3\" title=\"二阶导数的意义图解\">二阶导数的意义图解</a><br>\n2.5. <a href=\"#$w_0$%E5%92%8C$w_1$%E7%9A%84%E4%BA%8C%E9%98%B6%E5%AF%BC%E6%95%B0\" title=\"$w_0$和$w_1$的二阶导数\">$w_0$和$w_1$的二阶导数</a></li>\n<li><a href=\"#%E9%A2%84%E6%B5%8B\" title=\"预测\">预测</a></li>\n<li><a href=\"#%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95\" title=\"正则化最小二乘法\">正则化最小二乘法</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a>#下载图片接口 “下载图片接口”)</li>\n</ol>\n</blockquote>\n<h3 id=\"概述\">概述</h3>\n<p>Cmake升级是需要十分注意的，看了很多网上的博客，都是方式各异，也有不是很合理的方案。我们还是遵循自己作为一个开发者，全面了解Cmake具体的内容，再了解升级会有什么影响，再了解升级的不同种方式，最后让读者自己选择Cmake的升级策略。</p>\n<h3 id=\"Cmake介绍\">Cmake介绍</h3>\n<p>Cmake是一个什么工具？Cmake开发开始于1999年，是为了解决需要一个跨平台构建环境的 <a href=\"https://en.wikipedia.org/wiki/Insight_Segmentation_and_Registration_Toolkit\">Insight Segmentation and Registration Toolkit</a> （ITK)而开发的。</p>\n<h3 id=\"Cmake升级的影响\">Cmake升级的影响</h3>\n<p>首先, 如果你只是在后续的一个项目中需要使用单一版本的cmake，那么你可以完全重新编译一个在本地，但是，如果你需要保留默认安装的Cmake版本，那么需要一些特殊的操作来管理多个cmake版本在你的主机上。在我自己的开发中，如果默认就把ROS安装好了或者其他的软件在老版本的Cmake下编译好的，那么当你安装了新的cmake, 并且覆盖了老版本的cmake链接，那么将很难找回。在之前的cmake下编译好的ROS驱动和包可以继续使用，只是当你需要安装编译新的包时候，可能会出现包之间由于不同版本cmake编译的原因，导致冲突或者依赖项不兼容的问题。总之，当你需要升级Ubuntu系统的cmake，需要使用update-alternatives进行管理。</p>\n<h3 id=\"Cmake的合理升级方法\">Cmake的合理升级方法</h3>\n<p>我这里以Ubuntu18为例，默认安装的是cmake-3.10，链接在/usr/bin/cmake下，使用命令：<code>$ cmake --version</code> 可以查看版本。默认的cmake是通过apt-get install安装的，而我们需要使用源码编译的方式来升级会更方便定义它安装的位置在哪。</p>\n<ol>\n<li>\n<p>下载cmake源码</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget -O cmake-3.22.1.tar.gz https://cmake.org/files/v3.22/cmake-3.22.1.tar.gz</span><br></pre></td></tr></table></figure>\n<p>根据自己需要的版本进行下载。</p>\n<p><img src=\"/2022/08/27/004-Linux/01-Cmake/Cmake%E5%8D%87%E7%BA%A7-Ubuntu/image-20220830004817114.png\" alt=\"image-20220830004817114\"></p>\n</li>\n</ol>\n<p>目前最新的版本是3.24，cmake是向后兼容的，但是编译好的包就无法进行修改。关于编译工具cmake的版本，个人觉得如果你项目是用来部署生产的，不需要考虑最新的安装包，选择发行stable的老一些的版本，可以避免后期加依赖的时候出现其他问题。</p>\n<ol start=\"2\">\n<li>\n<p>解压并进入到cmake文件内</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">tar -xvzf cmake-3.22.1.tar.gz &amp;&amp; <span class=\"built_in\">cd</span> cmake-3.22.1</span></span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>编译源码</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake -DCMAKE_INSTALL_PREFIX=/usr .</span><br><span class=\"line\">make</span><br><span class=\"line\">make install ##可能需要sudo权限</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#编译好的cmake将会把bin下的cmake可执行文件链接到 /usr/local/bin/cmake</span></span> </span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>二进制包安装</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">下载二进制包</span></span><br><span class=\"line\">wget https://github.com/Kitware/CMake/releases/download/v3.18.2/cmake-3.18.2-Linux-x86_64.tar.gz</span><br><span class=\"line\">tar zxvf cmake-3.18.2-Linux-x86_64.tar.gz</span><br><span class=\"line\">cd cmake-3.18.2</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">将文件夹内容拷贝到默认环境变量路径下，例如/usr</span></span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>快捷脚本安装</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">wget -q -O cmake-linux.sh https://github.com/Kitware/CMake/releases/download/v3.17.0/cmake-3.17.0-Linux-x86_64.sh</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">sh cmake-linux.sh -- --skip-license --prefix=/usr</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">rm</span> cmake-linux.sh</span></span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p>查看安装好的版本号</p>\n<p><code>$ cmake --version</code><br>\n正常的输出是对应你刚才下载的cmake版本号，因为你是用户安装，会默认放在’/usr/local/bin’路径下。</p>\n</li>\n<li>\n<p>使用update-alternatives添加cmake的版本</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">*# 第一个参数: --install 表示向update-alternatives注册服务名。* </span><br><span class=\"line\"></span><br><span class=\"line\">*# 第二个参数: 注册最终地址，成功后将会把命令在这个固定的目的地址做真实命令的软链，以后管理就是管理这个软链；*</span><br><span class=\"line\"></span><br><span class=\"line\"> *# 第三个参数: 服务名，以后管理时以它为关联依据。*</span><br><span class=\"line\"></span><br><span class=\"line\"> *# 第四个参数: 被管理的命令绝对路径。*</span><br><span class=\"line\"></span><br><span class=\"line\"> *# 第五个参数: 优先级，数字越大优先级越高。*</span><br><span class=\"line\"><span class=\"meta prompt_\"> </span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#添加刚安装的cmake3.22.1，cmake 后面的路径是你刚才解压后文件的路径</span></span></span><br><span class=\"line\">sudo update-alternatives --install /usr/local/bin/cmake  cmake /home/t/Documents/cmake-3.22.1/bin/cmake  10  </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#添加默认的cmake-3.10.2，默认的cmake链接在/usr/bin/cmake</span></span> </span><br><span class=\"line\">sudo update-alternatives --install /usr/local/bin/cmake  cmake  /usr/bin/cmake  100 </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#如果你重复上面的步骤，你可以再安装一个3.18.0</span></span></span><br><span class=\"line\">tar -xvzf cmake-3.18.0.tar.gz &amp;&amp; cd cmake-3.18.0</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">这个时候，不需要安装在/usr路径下，否则会覆盖之前安装的版本</span></span><br><span class=\"line\">./bootstrap</span><br><span class=\"line\">make</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">make编译之后，就已经把源码编译好了，会生成一个/bin目录，cmake的执行文件就在/bin下</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\"><span class=\"comment\">#添加默认的cmake-3.18.0，默认的cmake链接在解压后的源码根目录的/bin下，比如我这里是 /home/t/Documents/cmake-3.18.0/bin/cmake</span></span></span><br><span class=\"line\">sudo update-alternatives --install /usr/local/bin/cmake  cmake /home/t/Documents/cmake-3.18.0/bin/cmake  1  </span><br><span class=\"line\"></span><br><span class=\"line\">这样我们就添加了三个cmake版本，两个是你刚才在用户下安装的，一个是默认的cmake deb包安装的。</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>如图所示：安装了三个版本的cmake，优先级可以自己定义，我这里定义默认的automode选择优先级最高的cmake-3.10.2，也就是/usr/bin/cmake链接的默认的cmake。</p>\n<p><img src=\"/2022/08/27/004-Linux/01-Cmake/Cmake%E5%8D%87%E7%BA%A7-Ubuntu/image-20220830010853110.png\" alt=\"image-20220830010853110\"></p>\n<p>可以自己使用–config 进行配置，选择你需要使用的cmake版本，当你项目需要使用高版本，你可以切换到高版本进行编译，需要低版本，就再切换到低版本进行编译，cmake之间的链接不会冲突干扰。</p>\n<p><img src=\"/2022/08/27/004-Linux/01-Cmake/Cmake%E5%8D%87%E7%BA%A7-Ubuntu/image-20220830010646685.png\" alt=\"image-20220830010646685\"></p>\n</li>\n</ol>\n<h3 id=\"可能的误操作\">可能的误操作</h3>\n<p>之前，不是很熟悉cmake管理的时候，会自己编译安装，但是发现安装之后，覆盖了原有deb包安装的cmake-3.10.2的链接，而且我还找不到，因为是deb安装的方式，没有源码就无法复制一个cmake执行文件，重新给个链接，导致需要卸载再重新安装，可能有其他方法，但是目前我还不知道怎么操作。因为需要卸载，</p>\n<p>卸载方式指令：<code>$ sudo apt remove cmake</code>, 在不清楚 <code>$ sudo apt autoremove cmake</code>会产生什么效果之前，小心使用，一般都会在命令行提醒你，apt将会卸载哪些包。在我这里的情况是，因为ROS是在cmake-3.10下安装的，所以cmake卸载的话，会同步把ROS的安装包和驱动都一并的卸载，这也是很多小白最初不会操作的时候，经常犯错的。</p>\n<p>如果你确实没有办法恢复老版本的cmake，就像我这里一样，使用<code>$ sudo apt remove cmake</code>卸载cmake3.10，然后再使用 <code>$ sudo apt autoremove cmake</code>看提示需要卸载什么ROS包，如果无关紧要，就执行autoremove的指令。最后，重新安装cmake，<code>$ sudo apt  install cmake</code>, 重新安装那些刚才你卸载的包，比如ROS。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"参考引用文献：\">参考引用文献：</h3>\n<p><a href=\"https://turbock79.cn/?p=2582\">https://turbock79.cn/?p=2582</a></p>\n"},{"title":"【第一章】OpenCV入门","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":null,"date":"2023-03-23T10:11:03.000Z","updated":"2023-03-23T10:11:03.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/robotics-arm.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# 【第一章】OpenCV入门\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [环境配置](#环境配置 \"环境配置\")\n> 3. [图像处理基础操作](#图像处理基础操作 \"图像处理基础操作\")\n> 4. [OpenCV贡献库](#OpenCV贡献库 \"OpenCV贡献库\")\n> 5. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n> 6. [引用文献](#引用文献 \"引用文献\")\n\n\n\n# 概述\n\nOpenCV是一个开源的计算机视觉库，1999年由英特尔的Gary Bradski启动。同时，OpenCV库由C和C++语言编写，涵盖计算机视觉各个领域内的500多个函数，可以在多种操作系统上运行。它旨在提供一个简洁而又高效的接口，从而帮助开发人员快速地构建视觉应用。\n\n\n\n# 环境配置\n\n- Python环境，Python3.x\n\n- Ubuntu操作系统、Windows系统、macOS系统均可\n\n- Anaconda安装配置　[__下载链接__](https://www.anaconda.com/download/)（根据自己的系统环境进行选择）\n\n- 使用Pip进行OpenCV库的安装，当前仓库代码使用OpenCV3.4.3.18\n\n```text\npip install opencv-python\n```\n\n以上环境配置好后，就可以开始练习OpenCV的库函数使用。\n\n\n\n**注意：**本章节源码仓库可以从链接中自由获取，同时，本章节也有Jupyter Notebook的实现也在仓库中。Jupyter Notebook的使用和安装参考博客：[__https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f__](https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f)\n\n\n\n# 图像处理基础操作\n\n1. 读取图像\n\n1. 显示图像\n\n1. 使用waitKey函数\n\n1. waitKey函数实现程序暂停\n\n1. 销毁窗口\n\n1. 销毁所有的窗口\n\n1. 保存图像\n\n\n\n# OpenCV贡献库\n\n除了OpenCV的主库函数以外由OpenCV团队维护，OpenCV还有一个开源社区开发维护的贡献库，包含的视觉应用比OpenCV库更全面。\n\n包含的一部分扩展模块有如下：\n\n● bioinspired：生物视觉模块。\n\n● datasets：数据集读取模块。\n\n● dnn：深度神经网络模块。\n\n● face：人脸识别模块。\n\n● matlab:MATLAB接口模块。\n\n● stereo：双目立体匹配模块。\n\n● text：视觉文本匹配模块。\n\n● tracking：基于视觉的目标跟踪模块。\n\n● ximgpro：图像处理扩展模块。\n\n● xobjdetect：增强2D目标检测模块。\n\n● xphoto：计算摄影扩展模块。\n\n**安装指令：**\n\n```text\npip install opencv-contrib-python\n```\n\n**FAQ：**[__https://pypi.org/project/opencv-contrib-python/__](https://pypi.org/project/opencv-contrib-python/)\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n\nOpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5\n","source":"_posts/007-ComputerVision/01-OpenCV/【第一章】OpenCV入门.md","raw":"---\ntitle: 【第一章】OpenCV入门\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax:\ndate: 2023-03-23 18:11:03\nupdated: 2023-03-23 18:11:03\ntags: OpenCV\ncategories: \n- ComputerVision\n- OpenCV\nkeywords:\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# 【第一章】OpenCV入门\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [环境配置](#环境配置 \"环境配置\")\n> 3. [图像处理基础操作](#图像处理基础操作 \"图像处理基础操作\")\n> 4. [OpenCV贡献库](#OpenCV贡献库 \"OpenCV贡献库\")\n> 5. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n> 6. [引用文献](#引用文献 \"引用文献\")\n\n\n\n# 概述\n\nOpenCV是一个开源的计算机视觉库，1999年由英特尔的Gary Bradski启动。同时，OpenCV库由C和C++语言编写，涵盖计算机视觉各个领域内的500多个函数，可以在多种操作系统上运行。它旨在提供一个简洁而又高效的接口，从而帮助开发人员快速地构建视觉应用。\n\n\n\n# 环境配置\n\n- Python环境，Python3.x\n\n- Ubuntu操作系统、Windows系统、macOS系统均可\n\n- Anaconda安装配置　[__下载链接__](https://www.anaconda.com/download/)（根据自己的系统环境进行选择）\n\n- 使用Pip进行OpenCV库的安装，当前仓库代码使用OpenCV3.4.3.18\n\n```text\npip install opencv-python\n```\n\n以上环境配置好后，就可以开始练习OpenCV的库函数使用。\n\n\n\n**注意：**本章节源码仓库可以从链接中自由获取，同时，本章节也有Jupyter Notebook的实现也在仓库中。Jupyter Notebook的使用和安装参考博客：[__https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f__](https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f)\n\n\n\n# 图像处理基础操作\n\n1. 读取图像\n\n1. 显示图像\n\n1. 使用waitKey函数\n\n1. waitKey函数实现程序暂停\n\n1. 销毁窗口\n\n1. 销毁所有的窗口\n\n1. 保存图像\n\n\n\n# OpenCV贡献库\n\n除了OpenCV的主库函数以外由OpenCV团队维护，OpenCV还有一个开源社区开发维护的贡献库，包含的视觉应用比OpenCV库更全面。\n\n包含的一部分扩展模块有如下：\n\n● bioinspired：生物视觉模块。\n\n● datasets：数据集读取模块。\n\n● dnn：深度神经网络模块。\n\n● face：人脸识别模块。\n\n● matlab:MATLAB接口模块。\n\n● stereo：双目立体匹配模块。\n\n● text：视觉文本匹配模块。\n\n● tracking：基于视觉的目标跟踪模块。\n\n● ximgpro：图像处理扩展模块。\n\n● xobjdetect：增强2D目标检测模块。\n\n● xphoto：计算摄影扩展模块。\n\n**安装指令：**\n\n```text\npip install opencv-contrib-python\n```\n\n**FAQ：**[__https://pypi.org/project/opencv-contrib-python/__](https://pypi.org/project/opencv-contrib-python/)\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n\nOpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5\n","slug":"007-ComputerVision/01-OpenCV/【第一章】OpenCV入门","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t10014u9rl4njyfmw3","content":"<h1>【第一章】OpenCV入门</h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE\" title=\"环境配置\">环境配置</a></li>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C\" title=\"图像处理基础操作\">图像处理基础操作</a></li>\n<li><a href=\"#OpenCV%E8%B4%A1%E7%8C%AE%E5%BA%93\" title=\"OpenCV贡献库\">OpenCV贡献库</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a></li>\n</ol>\n</blockquote>\n<h1>概述</h1>\n<p>OpenCV是一个开源的计算机视觉库，1999年由英特尔的Gary Bradski启动。同时，OpenCV库由C和C++语言编写，涵盖计算机视觉各个领域内的500多个函数，可以在多种操作系统上运行。它旨在提供一个简洁而又高效的接口，从而帮助开发人员快速地构建视觉应用。</p>\n<h1>环境配置</h1>\n<ul>\n<li>\n<p>Python环境，Python3.x</p>\n</li>\n<li>\n<p>Ubuntu操作系统、Windows系统、macOS系统均可</p>\n</li>\n<li>\n<p>Anaconda安装配置　<a href=\"https://www.anaconda.com/download/\"><strong>下载链接</strong></a>（根据自己的系统环境进行选择）</p>\n</li>\n<li>\n<p>使用Pip进行OpenCV库的安装，当前仓库代码使用OpenCV3.4.3.18</p>\n</li>\n</ul>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install opencv-python</span><br></pre></td></tr></table></figure>\n<p>以上环境配置好后，就可以开始练习OpenCV的库函数使用。</p>\n<p>**注意：**本章节源码仓库可以从链接中自由获取，同时，本章节也有Jupyter Notebook的实现也在仓库中。Jupyter Notebook的使用和安装参考博客：<a href=\"https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f\"><strong>https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f</strong></a></p>\n<h1>图像处理基础操作</h1>\n<ol>\n<li>\n<p>读取图像</p>\n</li>\n<li>\n<p>显示图像</p>\n</li>\n<li>\n<p>使用waitKey函数</p>\n</li>\n<li>\n<p>waitKey函数实现程序暂停</p>\n</li>\n<li>\n<p>销毁窗口</p>\n</li>\n<li>\n<p>销毁所有的窗口</p>\n</li>\n<li>\n<p>保存图像</p>\n</li>\n</ol>\n<h1>OpenCV贡献库</h1>\n<p>除了OpenCV的主库函数以外由OpenCV团队维护，OpenCV还有一个开源社区开发维护的贡献库，包含的视觉应用比OpenCV库更全面。</p>\n<p>包含的一部分扩展模块有如下：</p>\n<p>● bioinspired：生物视觉模块。</p>\n<p>● datasets：数据集读取模块。</p>\n<p>● dnn：深度神经网络模块。</p>\n<p>● face：人脸识别模块。</p>\n<p>● matlab:MATLAB接口模块。</p>\n<p>● stereo：双目立体匹配模块。</p>\n<p>● text：视觉文本匹配模块。</p>\n<p>● tracking：基于视觉的目标跟踪模块。</p>\n<p>● ximgpro：图像处理扩展模块。</p>\n<p>● xobjdetect：增强2D目标检测模块。</p>\n<p>● xphoto：计算摄影扩展模块。</p>\n<p><strong>安装指令：</strong></p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install opencv-contrib-python</span><br></pre></td></tr></table></figure>\n<p><strong>FAQ：</strong><a href=\"https://pypi.org/project/opencv-contrib-python/\"><strong>https://pypi.org/project/opencv-contrib-python/</strong></a></p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p>OpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1>【第一章】OpenCV入门</h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE\" title=\"环境配置\">环境配置</a></li>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C\" title=\"图像处理基础操作\">图像处理基础操作</a></li>\n<li><a href=\"#OpenCV%E8%B4%A1%E7%8C%AE%E5%BA%93\" title=\"OpenCV贡献库\">OpenCV贡献库</a></li>\n<li><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></li>\n<li><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a></li>\n</ol>\n</blockquote>\n<h1>概述</h1>\n<p>OpenCV是一个开源的计算机视觉库，1999年由英特尔的Gary Bradski启动。同时，OpenCV库由C和C++语言编写，涵盖计算机视觉各个领域内的500多个函数，可以在多种操作系统上运行。它旨在提供一个简洁而又高效的接口，从而帮助开发人员快速地构建视觉应用。</p>\n<h1>环境配置</h1>\n<ul>\n<li>\n<p>Python环境，Python3.x</p>\n</li>\n<li>\n<p>Ubuntu操作系统、Windows系统、macOS系统均可</p>\n</li>\n<li>\n<p>Anaconda安装配置　<a href=\"https://www.anaconda.com/download/\"><strong>下载链接</strong></a>（根据自己的系统环境进行选择）</p>\n</li>\n<li>\n<p>使用Pip进行OpenCV库的安装，当前仓库代码使用OpenCV3.4.3.18</p>\n</li>\n</ul>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install opencv-python</span><br></pre></td></tr></table></figure>\n<p>以上环境配置好后，就可以开始练习OpenCV的库函数使用。</p>\n<p>**注意：**本章节源码仓库可以从链接中自由获取，同时，本章节也有Jupyter Notebook的实现也在仓库中。Jupyter Notebook的使用和安装参考博客：<a href=\"https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f\"><strong>https://medium.com/python4u/jupyter-notebook%E5%AE%8C%E6%95%B4%E4%BB%8B%E7%B4%B9%E5%8F%8A%E5%AE%89%E8%A3%9D%E8%AA%AA%E6%98%8E-b8fcadba15f</strong></a></p>\n<h1>图像处理基础操作</h1>\n<ol>\n<li>\n<p>读取图像</p>\n</li>\n<li>\n<p>显示图像</p>\n</li>\n<li>\n<p>使用waitKey函数</p>\n</li>\n<li>\n<p>waitKey函数实现程序暂停</p>\n</li>\n<li>\n<p>销毁窗口</p>\n</li>\n<li>\n<p>销毁所有的窗口</p>\n</li>\n<li>\n<p>保存图像</p>\n</li>\n</ol>\n<h1>OpenCV贡献库</h1>\n<p>除了OpenCV的主库函数以外由OpenCV团队维护，OpenCV还有一个开源社区开发维护的贡献库，包含的视觉应用比OpenCV库更全面。</p>\n<p>包含的一部分扩展模块有如下：</p>\n<p>● bioinspired：生物视觉模块。</p>\n<p>● datasets：数据集读取模块。</p>\n<p>● dnn：深度神经网络模块。</p>\n<p>● face：人脸识别模块。</p>\n<p>● matlab:MATLAB接口模块。</p>\n<p>● stereo：双目立体匹配模块。</p>\n<p>● text：视觉文本匹配模块。</p>\n<p>● tracking：基于视觉的目标跟踪模块。</p>\n<p>● ximgpro：图像处理扩展模块。</p>\n<p>● xobjdetect：增强2D目标检测模块。</p>\n<p>● xphoto：计算摄影扩展模块。</p>\n<p><strong>安装指令：</strong></p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install opencv-contrib-python</span><br></pre></td></tr></table></figure>\n<p><strong>FAQ：</strong><a href=\"https://pypi.org/project/opencv-contrib-python/\"><strong>https://pypi.org/project/opencv-contrib-python/</strong></a></p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p>OpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5</p>\n"},{"title":"【第三章】图像运算","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-03-26T16:44:21.000Z","updated":"2023-03-26T16:44:21.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/5f363d07a24be_270_185.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***【第三章】图像运算***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [加号运算](#加号运算 \"加号运算\")\n>3. [cv2.add函数](#cv2.add函数 \"cv2.add函数\")\n>4. [图像加权和](#图像加权和 \"图像加权和\")\n>5. [按位逻辑运算](#按位逻辑运算 \"按位逻辑运算\")\n>6. [掩模](#掩模 \"掩模\")\n>7. [图像与数值的运算](#图像与数值的运算 \"图像与数值的运算\")\n>8. [位平面分解](#位平面分解 \"位平面分解\")\n>9. [图像加密和解密](#图像加密和解密 \"图像加密和解密\")\n\n\n\n# 概述\n\n图像的运算包含基础的一些加法运算、位运算等。本章主要介绍位平面分解、图像异或加密、数字水印、脸部打码/解码等实例。\n\n\n\n# 加号运算\n\n使用\"+\"运算符对两张图像进行求和运算，准守以下规则：\n\n$$\na + b =  \\begin{cases}\n   a + b, &\\text{if } a+b \\leqslant 255  \\\\\n   mod(a + b, 256), &\\text{if } a+b > 255\n\\end{cases}\n$$\n相加之后对应的像素值如果大于255则对256取模的方式，计算，比如 255+58=313，大于255，则计算313%256=57，像素值则修改成57。\n\n\n\n# cv2.add()函数\n\n如果使用cv2.add函数，相加之后的规则如下：\n\n$$\na + b =  \\begin{cases}\n   a + b, &\\text{if } a+b \\leqslant 255  \\\\\n   255, &\\text{if } a+b > 255\n\\end{cases}\n$$\n大于255则，按照255赋值给像素点。\n\n# 图像加权和\n\n使用cv2.addWeighted()，两张图片可以进行不同权重的相加，公式如下：\n\n$$\ndst=saturate(src1×α+src2×β+γ)\n$$\n效果如下：\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135929.png\" alt=\"addWighted\" style=\"zoom:80%;\" />\n\n使用局部区域的像素加权和计算后效果：\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135055.png\" alt=\"addWeighted02\" style=\"zoom:80%;\" />\n\n函数语法格式：\n\ndst=cv2.addWeighted(src1,alpha,src2,beta,gamma)\n\n# 按位逻辑运算\n\n## 1. 按位与运算\n\n使用cv2.bitwise_and()函数实现按位与运算，语法格式：\n\ndst=cv2.bitwise_and(src1,src2[,mask]])\n\n- dst表示与输入值具有同样大小的array输出值。\n\n- src1表示第一个array或scalar类型的输入值。\n\n- src2表示第二个array或scalar类型的输入值。\n\n- mask表示可选操作掩码，8位单通道array。\n\n与操作的特点：\n\n- 将任何数值N与数值0进行按位与操作，都会得到数值0。\n\n- 将任何数值N（这里仅考虑8位值）与数值255（8位二进制数是1111 1111）进行按位与操作，都会得到数值N本身。\n\n实际用掩模图和原图按位与的效果：\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135384.png\" alt=\"bitWise_or\" style=\"zoom:80%;\" />\n\n## 2. 按位或运算\n\n使用cv2.bitwise_or（）函数来实现按位或运算，其语法格式为：dst=cv2.bitwise_or（src1,src2[,mask]]）\n\n- dst表示与输入值具有同样大小的array输出值。\n\n- src1表示第一个array或scalar类型的输入值。\n\n- src2表示第二个array或scalar类型的输入值。\n\n- mask表示可选操作掩码，8位单通道array值。\n\n\n\n## 3. 按位非运算\n\n非运算是取反操作，使用函数cv2.bitwise_not（）来实现按位取反操作，语法格式：dst=cv2.bitwise_not（src[,mask]]）\n\n- dst 表示与输入值具有同样大小的array输出值。\n\n- src表示array类型的输入值。\n\n- mask表示可选操作掩码，8位单通道array值。\n\n\n\n## 4. 按位与或运算\n\n异或运算也叫半加运算，其运算法则与不带进位的二进制加法类似，其英文为“exclusive OR”，因此其函数通常表示为xor。使用函数cv2.bitwise_xor（）来实现按位异或运算，其语法格式为：dst=cv2.bitwise_xor（src1,src2[,mask]]）\n\n- dst表示与输入值具有同样大小的array输出值。\n\n- src1表示第一个array或scalar类型的输入值。\n\n- src2表示第二个array或scalar类型的输入值。\n\n- mask表示可选操作掩码，8位单通道array值。\n\n\n\n# 掩模\n\nOpenCV中的很多函数都会指定一个掩模，也被称为掩码，例如：计算结果=cv2.add（参数1，参数2，掩模），实际上面按位与运算中的效果就是和掩模图进行的。\n\n\n\n# 图像与数值的运算\n\n加法运算中，参与运算的算子可以是两幅图像，也可以是一个图像一个数值。\n\n比如，如果想增加图像的整体亮度，可以将每一个像素值都加上一个特定值。在具体实现时，可以给图像加上一个统一像素值的图像，也可以给图像加上一个固定值。\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122136148.png\" alt=\"image_add_num\" style=\"zoom:80%;\" />\n\n\n\n# 位平面分解\n\n将灰度图像中处于同一比特位上的二进制像素值进行组合，得到一幅二进制值图像，该图像被称为灰度图像的一个位平面，这个过程被称为位平面分解。\n\n在8位灰度图中，每一个像素使用8位二进制值来表示，其值的范围在[0,255]之间。可以将其中的值表示为：\n\n$$\n value={a^7}×2^7+a^6×2^6+a^5×2^5+a^4×2^4+a^3×2^3+a^2×2^2+a^1×2^1+a^0×2^0\n$$\na7的权重最高，所构成的位平面与原图像相关性最高，该位平面看起来通常与原图像最类似。\n\na0权重最低，所构成的位平面与原图像相关性最低，该平面看起来通常是杂乱无章的。\n\n示意图分解如下所示，\n\n原始灰度图像素值：\n\n![bit_image](https://www.synotech.top:5523/uploads/2023/04/12/202304122137533.png)\n\n![bit_plane_img](https://www.synotech.top:5523/uploads/2023/04/12/202304122137420.png)\n\n![org_gray_image](https://www.synotech.top:5523/uploads/2023/04/12/202304122137382.png)\n\n\n\n为了提取位平面，我们需要构造提取矩阵，使用不同的提取因子来提取数值N中的特定位。\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137624.png\" alt=\"resolve_bit_plane_element\" style=\"zoom:80%;\" />\n\n然后将图像与提取矩阵/提取因子进行按位与运算，得到各个位平面。\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122138054.png\" alt=\"bit_plane_effect\" style=\"zoom:80%;\" />\n\n\n\n# 图像加密和解密\n\n通过按位异或运算可以实现图像的加密和解密。通过对原始图像与密钥图像进行按位异或，可以实现加密；将加密后的图像与密钥图像再次进行按位异或，可以实现解密。\n\n## 加密\n\n原始图像：\n\n![enctription_org](https://www.synotech.top:5523/uploads/2023/04/12/202304122138374.png)\n\n加密秘钥图像K：\n\n![enctryption_img](https://www.synotech.top:5523/uploads/2023/04/12/202304122138975.png)\n\n图像O所对应的二进制：\n\n![enctription_org_bit](https://www.synotech.top:5523/uploads/2023/04/12/202304122140379.png)\n\n秘钥图像对应的二进制：\n\n![enctryption_img_bit](https://www.synotech.top:5523/uploads/2023/04/12/202304122141288.png)\n\n异或运算结果图像二进制：\n\n![encryption_result](https://www.synotech.top:5523/uploads/2023/04/12/202304122141804.png)\n\n二进制转十进制图像：\n\n![encryption_result_img](https://www.synotech.top:5523/uploads/2023/04/12/202304122141965.png)\n\n## 解密\n\n解密过程需要将加密图像与密钥图像进行按位异或运算，得到原图像, 过程比较简单。\n\n\n\n\n\n# 数字水印\n\n最低有效位（Least Significant Bit,LSB）指的是一个二进制数中的第0位（即最低位）。最低有效位信息隐藏指的是，将一个需要隐藏的二值图像信息嵌入载体图像的最低有效位，即将载体图像的最低有效位层替换为当前需要隐藏的二值图像，从而实现将二值图像隐藏的目的。\n\n嵌入过程：将载体图像的第0个位平面替换为数字水印信息（一幅二值图像）。\n\n提取过程：将载体图像的最低有效位所构成的第0个位平面提取出来，得到数字水印信息。\n\n在实际中可以根据需要在多个通道内嵌入相同的水印（提高鲁棒性，即使部分水印丢失，也能提取出完整水印信息），或在各个不同的通道内嵌入不同的水印（提高嵌入容量）。在彩色图像的多个通道内嵌入水印的方法，与在灰度图像内嵌入水印的方法相同。基本都是按照按位与运算的方式，将含有水印的图像进行提取或者加入水印。\n\n\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n\nOpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5","source":"_posts/007-ComputerVision/01-OpenCV/【第三章】图像运算.md","raw":"---\ntitle: 【第三章】图像运算\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-03-27 00:44:21\nupdated: 2023-03-27 00:44:21\ntags: OpenCV\ncategories: \n- ComputerVision\n- OpenCV\nkeywords:\ndescription:\ntop_img: \ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***【第三章】图像运算***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>2. [加号运算](#加号运算 \"加号运算\")\n>3. [cv2.add函数](#cv2.add函数 \"cv2.add函数\")\n>4. [图像加权和](#图像加权和 \"图像加权和\")\n>5. [按位逻辑运算](#按位逻辑运算 \"按位逻辑运算\")\n>6. [掩模](#掩模 \"掩模\")\n>7. [图像与数值的运算](#图像与数值的运算 \"图像与数值的运算\")\n>8. [位平面分解](#位平面分解 \"位平面分解\")\n>9. [图像加密和解密](#图像加密和解密 \"图像加密和解密\")\n\n\n\n# 概述\n\n图像的运算包含基础的一些加法运算、位运算等。本章主要介绍位平面分解、图像异或加密、数字水印、脸部打码/解码等实例。\n\n\n\n# 加号运算\n\n使用\"+\"运算符对两张图像进行求和运算，准守以下规则：\n\n$$\na + b =  \\begin{cases}\n   a + b, &\\text{if } a+b \\leqslant 255  \\\\\n   mod(a + b, 256), &\\text{if } a+b > 255\n\\end{cases}\n$$\n相加之后对应的像素值如果大于255则对256取模的方式，计算，比如 255+58=313，大于255，则计算313%256=57，像素值则修改成57。\n\n\n\n# cv2.add()函数\n\n如果使用cv2.add函数，相加之后的规则如下：\n\n$$\na + b =  \\begin{cases}\n   a + b, &\\text{if } a+b \\leqslant 255  \\\\\n   255, &\\text{if } a+b > 255\n\\end{cases}\n$$\n大于255则，按照255赋值给像素点。\n\n# 图像加权和\n\n使用cv2.addWeighted()，两张图片可以进行不同权重的相加，公式如下：\n\n$$\ndst=saturate(src1×α+src2×β+γ)\n$$\n效果如下：\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135929.png\" alt=\"addWighted\" style=\"zoom:80%;\" />\n\n使用局部区域的像素加权和计算后效果：\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135055.png\" alt=\"addWeighted02\" style=\"zoom:80%;\" />\n\n函数语法格式：\n\ndst=cv2.addWeighted(src1,alpha,src2,beta,gamma)\n\n# 按位逻辑运算\n\n## 1. 按位与运算\n\n使用cv2.bitwise_and()函数实现按位与运算，语法格式：\n\ndst=cv2.bitwise_and(src1,src2[,mask]])\n\n- dst表示与输入值具有同样大小的array输出值。\n\n- src1表示第一个array或scalar类型的输入值。\n\n- src2表示第二个array或scalar类型的输入值。\n\n- mask表示可选操作掩码，8位单通道array。\n\n与操作的特点：\n\n- 将任何数值N与数值0进行按位与操作，都会得到数值0。\n\n- 将任何数值N（这里仅考虑8位值）与数值255（8位二进制数是1111 1111）进行按位与操作，都会得到数值N本身。\n\n实际用掩模图和原图按位与的效果：\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135384.png\" alt=\"bitWise_or\" style=\"zoom:80%;\" />\n\n## 2. 按位或运算\n\n使用cv2.bitwise_or（）函数来实现按位或运算，其语法格式为：dst=cv2.bitwise_or（src1,src2[,mask]]）\n\n- dst表示与输入值具有同样大小的array输出值。\n\n- src1表示第一个array或scalar类型的输入值。\n\n- src2表示第二个array或scalar类型的输入值。\n\n- mask表示可选操作掩码，8位单通道array值。\n\n\n\n## 3. 按位非运算\n\n非运算是取反操作，使用函数cv2.bitwise_not（）来实现按位取反操作，语法格式：dst=cv2.bitwise_not（src[,mask]]）\n\n- dst 表示与输入值具有同样大小的array输出值。\n\n- src表示array类型的输入值。\n\n- mask表示可选操作掩码，8位单通道array值。\n\n\n\n## 4. 按位与或运算\n\n异或运算也叫半加运算，其运算法则与不带进位的二进制加法类似，其英文为“exclusive OR”，因此其函数通常表示为xor。使用函数cv2.bitwise_xor（）来实现按位异或运算，其语法格式为：dst=cv2.bitwise_xor（src1,src2[,mask]]）\n\n- dst表示与输入值具有同样大小的array输出值。\n\n- src1表示第一个array或scalar类型的输入值。\n\n- src2表示第二个array或scalar类型的输入值。\n\n- mask表示可选操作掩码，8位单通道array值。\n\n\n\n# 掩模\n\nOpenCV中的很多函数都会指定一个掩模，也被称为掩码，例如：计算结果=cv2.add（参数1，参数2，掩模），实际上面按位与运算中的效果就是和掩模图进行的。\n\n\n\n# 图像与数值的运算\n\n加法运算中，参与运算的算子可以是两幅图像，也可以是一个图像一个数值。\n\n比如，如果想增加图像的整体亮度，可以将每一个像素值都加上一个特定值。在具体实现时，可以给图像加上一个统一像素值的图像，也可以给图像加上一个固定值。\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122136148.png\" alt=\"image_add_num\" style=\"zoom:80%;\" />\n\n\n\n# 位平面分解\n\n将灰度图像中处于同一比特位上的二进制像素值进行组合，得到一幅二进制值图像，该图像被称为灰度图像的一个位平面，这个过程被称为位平面分解。\n\n在8位灰度图中，每一个像素使用8位二进制值来表示，其值的范围在[0,255]之间。可以将其中的值表示为：\n\n$$\n value={a^7}×2^7+a^6×2^6+a^5×2^5+a^4×2^4+a^3×2^3+a^2×2^2+a^1×2^1+a^0×2^0\n$$\na7的权重最高，所构成的位平面与原图像相关性最高，该位平面看起来通常与原图像最类似。\n\na0权重最低，所构成的位平面与原图像相关性最低，该平面看起来通常是杂乱无章的。\n\n示意图分解如下所示，\n\n原始灰度图像素值：\n\n![bit_image](https://www.synotech.top:5523/uploads/2023/04/12/202304122137533.png)\n\n![bit_plane_img](https://www.synotech.top:5523/uploads/2023/04/12/202304122137420.png)\n\n![org_gray_image](https://www.synotech.top:5523/uploads/2023/04/12/202304122137382.png)\n\n\n\n为了提取位平面，我们需要构造提取矩阵，使用不同的提取因子来提取数值N中的特定位。\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137624.png\" alt=\"resolve_bit_plane_element\" style=\"zoom:80%;\" />\n\n然后将图像与提取矩阵/提取因子进行按位与运算，得到各个位平面。\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122138054.png\" alt=\"bit_plane_effect\" style=\"zoom:80%;\" />\n\n\n\n# 图像加密和解密\n\n通过按位异或运算可以实现图像的加密和解密。通过对原始图像与密钥图像进行按位异或，可以实现加密；将加密后的图像与密钥图像再次进行按位异或，可以实现解密。\n\n## 加密\n\n原始图像：\n\n![enctription_org](https://www.synotech.top:5523/uploads/2023/04/12/202304122138374.png)\n\n加密秘钥图像K：\n\n![enctryption_img](https://www.synotech.top:5523/uploads/2023/04/12/202304122138975.png)\n\n图像O所对应的二进制：\n\n![enctription_org_bit](https://www.synotech.top:5523/uploads/2023/04/12/202304122140379.png)\n\n秘钥图像对应的二进制：\n\n![enctryption_img_bit](https://www.synotech.top:5523/uploads/2023/04/12/202304122141288.png)\n\n异或运算结果图像二进制：\n\n![encryption_result](https://www.synotech.top:5523/uploads/2023/04/12/202304122141804.png)\n\n二进制转十进制图像：\n\n![encryption_result_img](https://www.synotech.top:5523/uploads/2023/04/12/202304122141965.png)\n\n## 解密\n\n解密过程需要将加密图像与密钥图像进行按位异或运算，得到原图像, 过程比较简单。\n\n\n\n\n\n# 数字水印\n\n最低有效位（Least Significant Bit,LSB）指的是一个二进制数中的第0位（即最低位）。最低有效位信息隐藏指的是，将一个需要隐藏的二值图像信息嵌入载体图像的最低有效位，即将载体图像的最低有效位层替换为当前需要隐藏的二值图像，从而实现将二值图像隐藏的目的。\n\n嵌入过程：将载体图像的第0个位平面替换为数字水印信息（一幅二值图像）。\n\n提取过程：将载体图像的最低有效位所构成的第0个位平面提取出来，得到数字水印信息。\n\n在实际中可以根据需要在多个通道内嵌入相同的水印（提高鲁棒性，即使部分水印丢失，也能提取出完整水印信息），或在各个不同的通道内嵌入不同的水印（提高嵌入容量）。在彩色图像的多个通道内嵌入水印的方法，与在灰度图像内嵌入水印的方法相同。基本都是按照按位与运算的方式，将含有水印的图像进行提取或者加入水印。\n\n\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n\nOpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5","slug":"007-ComputerVision/01-OpenCV/【第三章】图像运算","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t20017u9rl4a8ebfdg","content":"<h1><em><strong>【第三章】图像运算</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E5%8A%A0%E5%8F%B7%E8%BF%90%E7%AE%97\" title=\"加号运算\">加号运算</a></li>\n<li><a href=\"#cv2.add%E5%87%BD%E6%95%B0\" title=\"cv2.add函数\">cv2.add函数</a></li>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E5%8A%A0%E6%9D%83%E5%92%8C\" title=\"图像加权和\">图像加权和</a></li>\n<li><a href=\"#%E6%8C%89%E4%BD%8D%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97\" title=\"按位逻辑运算\">按位逻辑运算</a></li>\n<li><a href=\"#%E6%8E%A9%E6%A8%A1\" title=\"掩模\">掩模</a></li>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E4%B8%8E%E6%95%B0%E5%80%BC%E7%9A%84%E8%BF%90%E7%AE%97\" title=\"图像与数值的运算\">图像与数值的运算</a></li>\n<li><a href=\"#%E4%BD%8D%E5%B9%B3%E9%9D%A2%E5%88%86%E8%A7%A3\" title=\"位平面分解\">位平面分解</a></li>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E5%8A%A0%E5%AF%86%E5%92%8C%E8%A7%A3%E5%AF%86\" title=\"图像加密和解密\">图像加密和解密</a></li>\n</ol>\n</blockquote>\n<h1>概述</h1>\n<p>图像的运算包含基础的一些加法运算、位运算等。本章主要介绍位平面分解、图像异或加密、数字水印、脸部打码/解码等实例。</p>\n<h1>加号运算</h1>\n<p>使用&quot;+&quot;运算符对两张图像进行求和运算，准守以下规则：</p>\n<p>$$<br>\na + b =  \\begin{cases}<br>\na + b, &amp;\\text{if } a+b \\leqslant 255  \\<br>\nmod(a + b, 256), &amp;\\text{if } a+b &gt; 255<br>\n\\end{cases}<br>\n$$<br>\n相加之后对应的像素值如果大于255则对256取模的方式，计算，比如 255+58=313，大于255，则计算313%256=57，像素值则修改成57。</p>\n<h1>cv2.add()函数</h1>\n<p>如果使用cv2.add函数，相加之后的规则如下：</p>\n<p>$$<br>\na + b =  \\begin{cases}<br>\na + b, &amp;\\text{if } a+b \\leqslant 255  \\<br>\n255, &amp;\\text{if } a+b &gt; 255<br>\n\\end{cases}<br>\n$$<br>\n大于255则，按照255赋值给像素点。</p>\n<h1>图像加权和</h1>\n<p>使用cv2.addWeighted()，两张图片可以进行不同权重的相加，公式如下：</p>\n<p>$$<br>\ndst=saturate(src1×α+src2×β+γ)<br>\n$$<br>\n效果如下：</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135929.png\" alt=\"addWighted\" style=\"zoom:80%;\">\n<p>使用局部区域的像素加权和计算后效果：</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135055.png\" alt=\"addWeighted02\" style=\"zoom:80%;\">\n<p>函数语法格式：</p>\n<p>dst=cv2.addWeighted(src1,alpha,src2,beta,gamma)</p>\n<h1>按位逻辑运算</h1>\n<h2 id=\"1-按位与运算\">1. 按位与运算</h2>\n<p>使用cv2.bitwise_and()函数实现按位与运算，语法格式：</p>\n<p>dst=cv2.bitwise_and(src1,src2[,mask]])</p>\n<ul>\n<li>\n<p>dst表示与输入值具有同样大小的array输出值。</p>\n</li>\n<li>\n<p>src1表示第一个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>src2表示第二个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>mask表示可选操作掩码，8位单通道array。</p>\n</li>\n</ul>\n<p>与操作的特点：</p>\n<ul>\n<li>\n<p>将任何数值N与数值0进行按位与操作，都会得到数值0。</p>\n</li>\n<li>\n<p>将任何数值N（这里仅考虑8位值）与数值255（8位二进制数是1111 1111）进行按位与操作，都会得到数值N本身。</p>\n</li>\n</ul>\n<p>实际用掩模图和原图按位与的效果：</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135384.png\" alt=\"bitWise_or\" style=\"zoom:80%;\">\n<h2 id=\"2-按位或运算\">2. 按位或运算</h2>\n<p>使用cv2.bitwise_or（）函数来实现按位或运算，其语法格式为：dst=cv2.bitwise_or（src1,src2[,mask]]）</p>\n<ul>\n<li>\n<p>dst表示与输入值具有同样大小的array输出值。</p>\n</li>\n<li>\n<p>src1表示第一个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>src2表示第二个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>mask表示可选操作掩码，8位单通道array值。</p>\n</li>\n</ul>\n<h2 id=\"3-按位非运算\">3. 按位非运算</h2>\n<p>非运算是取反操作，使用函数cv2.bitwise_not（）来实现按位取反操作，语法格式：dst=cv2.bitwise_not（src[,mask]]）</p>\n<ul>\n<li>\n<p>dst 表示与输入值具有同样大小的array输出值。</p>\n</li>\n<li>\n<p>src表示array类型的输入值。</p>\n</li>\n<li>\n<p>mask表示可选操作掩码，8位单通道array值。</p>\n</li>\n</ul>\n<h2 id=\"4-按位与或运算\">4. 按位与或运算</h2>\n<p>异或运算也叫半加运算，其运算法则与不带进位的二进制加法类似，其英文为“exclusive OR”，因此其函数通常表示为xor。使用函数cv2.bitwise_xor（）来实现按位异或运算，其语法格式为：dst=cv2.bitwise_xor（src1,src2[,mask]]）</p>\n<ul>\n<li>\n<p>dst表示与输入值具有同样大小的array输出值。</p>\n</li>\n<li>\n<p>src1表示第一个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>src2表示第二个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>mask表示可选操作掩码，8位单通道array值。</p>\n</li>\n</ul>\n<h1>掩模</h1>\n<p>OpenCV中的很多函数都会指定一个掩模，也被称为掩码，例如：计算结果=cv2.add（参数1，参数2，掩模），实际上面按位与运算中的效果就是和掩模图进行的。</p>\n<h1>图像与数值的运算</h1>\n<p>加法运算中，参与运算的算子可以是两幅图像，也可以是一个图像一个数值。</p>\n<p>比如，如果想增加图像的整体亮度，可以将每一个像素值都加上一个特定值。在具体实现时，可以给图像加上一个统一像素值的图像，也可以给图像加上一个固定值。</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122136148.png\" alt=\"image_add_num\" style=\"zoom:80%;\">\n<h1>位平面分解</h1>\n<p>将灰度图像中处于同一比特位上的二进制像素值进行组合，得到一幅二进制值图像，该图像被称为灰度图像的一个位平面，这个过程被称为位平面分解。</p>\n<p>在8位灰度图中，每一个像素使用8位二进制值来表示，其值的范围在[0,255]之间。可以将其中的值表示为：</p>\n<p>$$<br>\nvalue={a^7}×2^7+a^6×2^6+a^5×2^5+a^4×2^4+a^3×2^3+a^2×2^2+a^1×2^1+a^0×2^0<br>\n$$<br>\na7的权重最高，所构成的位平面与原图像相关性最高，该位平面看起来通常与原图像最类似。</p>\n<p>a0权重最低，所构成的位平面与原图像相关性最低，该平面看起来通常是杂乱无章的。</p>\n<p>示意图分解如下所示，</p>\n<p>原始灰度图像素值：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137533.png\" alt=\"bit_image\"></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137420.png\" alt=\"bit_plane_img\"></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137382.png\" alt=\"org_gray_image\"></p>\n<p>为了提取位平面，我们需要构造提取矩阵，使用不同的提取因子来提取数值N中的特定位。</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137624.png\" alt=\"resolve_bit_plane_element\" style=\"zoom:80%;\">\n<p>然后将图像与提取矩阵/提取因子进行按位与运算，得到各个位平面。</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122138054.png\" alt=\"bit_plane_effect\" style=\"zoom:80%;\">\n<h1>图像加密和解密</h1>\n<p>通过按位异或运算可以实现图像的加密和解密。通过对原始图像与密钥图像进行按位异或，可以实现加密；将加密后的图像与密钥图像再次进行按位异或，可以实现解密。</p>\n<h2 id=\"加密\">加密</h2>\n<p>原始图像：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122138374.png\" alt=\"enctription_org\"></p>\n<p>加密秘钥图像K：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122138975.png\" alt=\"enctryption_img\"></p>\n<p>图像O所对应的二进制：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122140379.png\" alt=\"enctription_org_bit\"></p>\n<p>秘钥图像对应的二进制：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122141288.png\" alt=\"enctryption_img_bit\"></p>\n<p>异或运算结果图像二进制：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122141804.png\" alt=\"encryption_result\"></p>\n<p>二进制转十进制图像：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122141965.png\" alt=\"encryption_result_img\"></p>\n<h2 id=\"解密\">解密</h2>\n<p>解密过程需要将加密图像与密钥图像进行按位异或运算，得到原图像, 过程比较简单。</p>\n<h1>数字水印</h1>\n<p>最低有效位（Least Significant Bit,LSB）指的是一个二进制数中的第0位（即最低位）。最低有效位信息隐藏指的是，将一个需要隐藏的二值图像信息嵌入载体图像的最低有效位，即将载体图像的最低有效位层替换为当前需要隐藏的二值图像，从而实现将二值图像隐藏的目的。</p>\n<p>嵌入过程：将载体图像的第0个位平面替换为数字水印信息（一幅二值图像）。</p>\n<p>提取过程：将载体图像的最低有效位所构成的第0个位平面提取出来，得到数字水印信息。</p>\n<p>在实际中可以根据需要在多个通道内嵌入相同的水印（提高鲁棒性，即使部分水印丢失，也能提取出完整水印信息），或在各个不同的通道内嵌入不同的水印（提高嵌入容量）。在彩色图像的多个通道内嵌入水印的方法，与在灰度图像内嵌入水印的方法相同。基本都是按照按位与运算的方式，将含有水印的图像进行提取或者加入水印。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p>OpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>【第三章】图像运算</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></li>\n<li><a href=\"#%E5%8A%A0%E5%8F%B7%E8%BF%90%E7%AE%97\" title=\"加号运算\">加号运算</a></li>\n<li><a href=\"#cv2.add%E5%87%BD%E6%95%B0\" title=\"cv2.add函数\">cv2.add函数</a></li>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E5%8A%A0%E6%9D%83%E5%92%8C\" title=\"图像加权和\">图像加权和</a></li>\n<li><a href=\"#%E6%8C%89%E4%BD%8D%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97\" title=\"按位逻辑运算\">按位逻辑运算</a></li>\n<li><a href=\"#%E6%8E%A9%E6%A8%A1\" title=\"掩模\">掩模</a></li>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E4%B8%8E%E6%95%B0%E5%80%BC%E7%9A%84%E8%BF%90%E7%AE%97\" title=\"图像与数值的运算\">图像与数值的运算</a></li>\n<li><a href=\"#%E4%BD%8D%E5%B9%B3%E9%9D%A2%E5%88%86%E8%A7%A3\" title=\"位平面分解\">位平面分解</a></li>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E5%8A%A0%E5%AF%86%E5%92%8C%E8%A7%A3%E5%AF%86\" title=\"图像加密和解密\">图像加密和解密</a></li>\n</ol>\n</blockquote>\n<h1>概述</h1>\n<p>图像的运算包含基础的一些加法运算、位运算等。本章主要介绍位平面分解、图像异或加密、数字水印、脸部打码/解码等实例。</p>\n<h1>加号运算</h1>\n<p>使用&quot;+&quot;运算符对两张图像进行求和运算，准守以下规则：</p>\n<p>$$<br>\na + b =  \\begin{cases}<br>\na + b, &amp;\\text{if } a+b \\leqslant 255  \\<br>\nmod(a + b, 256), &amp;\\text{if } a+b &gt; 255<br>\n\\end{cases}<br>\n$$<br>\n相加之后对应的像素值如果大于255则对256取模的方式，计算，比如 255+58=313，大于255，则计算313%256=57，像素值则修改成57。</p>\n<h1>cv2.add()函数</h1>\n<p>如果使用cv2.add函数，相加之后的规则如下：</p>\n<p>$$<br>\na + b =  \\begin{cases}<br>\na + b, &amp;\\text{if } a+b \\leqslant 255  \\<br>\n255, &amp;\\text{if } a+b &gt; 255<br>\n\\end{cases}<br>\n$$<br>\n大于255则，按照255赋值给像素点。</p>\n<h1>图像加权和</h1>\n<p>使用cv2.addWeighted()，两张图片可以进行不同权重的相加，公式如下：</p>\n<p>$$<br>\ndst=saturate(src1×α+src2×β+γ)<br>\n$$<br>\n效果如下：</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135929.png\" alt=\"addWighted\" style=\"zoom:80%;\">\n<p>使用局部区域的像素加权和计算后效果：</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135055.png\" alt=\"addWeighted02\" style=\"zoom:80%;\">\n<p>函数语法格式：</p>\n<p>dst=cv2.addWeighted(src1,alpha,src2,beta,gamma)</p>\n<h1>按位逻辑运算</h1>\n<h2 id=\"1-按位与运算\">1. 按位与运算</h2>\n<p>使用cv2.bitwise_and()函数实现按位与运算，语法格式：</p>\n<p>dst=cv2.bitwise_and(src1,src2[,mask]])</p>\n<ul>\n<li>\n<p>dst表示与输入值具有同样大小的array输出值。</p>\n</li>\n<li>\n<p>src1表示第一个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>src2表示第二个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>mask表示可选操作掩码，8位单通道array。</p>\n</li>\n</ul>\n<p>与操作的特点：</p>\n<ul>\n<li>\n<p>将任何数值N与数值0进行按位与操作，都会得到数值0。</p>\n</li>\n<li>\n<p>将任何数值N（这里仅考虑8位值）与数值255（8位二进制数是1111 1111）进行按位与操作，都会得到数值N本身。</p>\n</li>\n</ul>\n<p>实际用掩模图和原图按位与的效果：</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122135384.png\" alt=\"bitWise_or\" style=\"zoom:80%;\">\n<h2 id=\"2-按位或运算\">2. 按位或运算</h2>\n<p>使用cv2.bitwise_or（）函数来实现按位或运算，其语法格式为：dst=cv2.bitwise_or（src1,src2[,mask]]）</p>\n<ul>\n<li>\n<p>dst表示与输入值具有同样大小的array输出值。</p>\n</li>\n<li>\n<p>src1表示第一个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>src2表示第二个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>mask表示可选操作掩码，8位单通道array值。</p>\n</li>\n</ul>\n<h2 id=\"3-按位非运算\">3. 按位非运算</h2>\n<p>非运算是取反操作，使用函数cv2.bitwise_not（）来实现按位取反操作，语法格式：dst=cv2.bitwise_not（src[,mask]]）</p>\n<ul>\n<li>\n<p>dst 表示与输入值具有同样大小的array输出值。</p>\n</li>\n<li>\n<p>src表示array类型的输入值。</p>\n</li>\n<li>\n<p>mask表示可选操作掩码，8位单通道array值。</p>\n</li>\n</ul>\n<h2 id=\"4-按位与或运算\">4. 按位与或运算</h2>\n<p>异或运算也叫半加运算，其运算法则与不带进位的二进制加法类似，其英文为“exclusive OR”，因此其函数通常表示为xor。使用函数cv2.bitwise_xor（）来实现按位异或运算，其语法格式为：dst=cv2.bitwise_xor（src1,src2[,mask]]）</p>\n<ul>\n<li>\n<p>dst表示与输入值具有同样大小的array输出值。</p>\n</li>\n<li>\n<p>src1表示第一个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>src2表示第二个array或scalar类型的输入值。</p>\n</li>\n<li>\n<p>mask表示可选操作掩码，8位单通道array值。</p>\n</li>\n</ul>\n<h1>掩模</h1>\n<p>OpenCV中的很多函数都会指定一个掩模，也被称为掩码，例如：计算结果=cv2.add（参数1，参数2，掩模），实际上面按位与运算中的效果就是和掩模图进行的。</p>\n<h1>图像与数值的运算</h1>\n<p>加法运算中，参与运算的算子可以是两幅图像，也可以是一个图像一个数值。</p>\n<p>比如，如果想增加图像的整体亮度，可以将每一个像素值都加上一个特定值。在具体实现时，可以给图像加上一个统一像素值的图像，也可以给图像加上一个固定值。</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122136148.png\" alt=\"image_add_num\" style=\"zoom:80%;\">\n<h1>位平面分解</h1>\n<p>将灰度图像中处于同一比特位上的二进制像素值进行组合，得到一幅二进制值图像，该图像被称为灰度图像的一个位平面，这个过程被称为位平面分解。</p>\n<p>在8位灰度图中，每一个像素使用8位二进制值来表示，其值的范围在[0,255]之间。可以将其中的值表示为：</p>\n<p>$$<br>\nvalue={a^7}×2^7+a^6×2^6+a^5×2^5+a^4×2^4+a^3×2^3+a^2×2^2+a^1×2^1+a^0×2^0<br>\n$$<br>\na7的权重最高，所构成的位平面与原图像相关性最高，该位平面看起来通常与原图像最类似。</p>\n<p>a0权重最低，所构成的位平面与原图像相关性最低，该平面看起来通常是杂乱无章的。</p>\n<p>示意图分解如下所示，</p>\n<p>原始灰度图像素值：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137533.png\" alt=\"bit_image\"></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137420.png\" alt=\"bit_plane_img\"></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137382.png\" alt=\"org_gray_image\"></p>\n<p>为了提取位平面，我们需要构造提取矩阵，使用不同的提取因子来提取数值N中的特定位。</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122137624.png\" alt=\"resolve_bit_plane_element\" style=\"zoom:80%;\">\n<p>然后将图像与提取矩阵/提取因子进行按位与运算，得到各个位平面。</p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122138054.png\" alt=\"bit_plane_effect\" style=\"zoom:80%;\">\n<h1>图像加密和解密</h1>\n<p>通过按位异或运算可以实现图像的加密和解密。通过对原始图像与密钥图像进行按位异或，可以实现加密；将加密后的图像与密钥图像再次进行按位异或，可以实现解密。</p>\n<h2 id=\"加密\">加密</h2>\n<p>原始图像：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122138374.png\" alt=\"enctription_org\"></p>\n<p>加密秘钥图像K：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122138975.png\" alt=\"enctryption_img\"></p>\n<p>图像O所对应的二进制：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122140379.png\" alt=\"enctription_org_bit\"></p>\n<p>秘钥图像对应的二进制：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122141288.png\" alt=\"enctryption_img_bit\"></p>\n<p>异或运算结果图像二进制：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122141804.png\" alt=\"encryption_result\"></p>\n<p>二进制转十进制图像：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122141965.png\" alt=\"encryption_result_img\"></p>\n<h2 id=\"解密\">解密</h2>\n<p>解密过程需要将加密图像与密钥图像进行按位异或运算，得到原图像, 过程比较简单。</p>\n<h1>数字水印</h1>\n<p>最低有效位（Least Significant Bit,LSB）指的是一个二进制数中的第0位（即最低位）。最低有效位信息隐藏指的是，将一个需要隐藏的二值图像信息嵌入载体图像的最低有效位，即将载体图像的最低有效位层替换为当前需要隐藏的二值图像，从而实现将二值图像隐藏的目的。</p>\n<p>嵌入过程：将载体图像的第0个位平面替换为数字水印信息（一幅二值图像）。</p>\n<p>提取过程：将载体图像的最低有效位所构成的第0个位平面提取出来，得到数字水印信息。</p>\n<p>在实际中可以根据需要在多个通道内嵌入相同的水印（提高鲁棒性，即使部分水印丢失，也能提取出完整水印信息），或在各个不同的通道内嵌入不同的水印（提高嵌入容量）。在彩色图像的多个通道内嵌入水印的方法，与在灰度图像内嵌入水印的方法相同。基本都是按照按位与运算的方式，将含有水印的图像进行提取或者加入水印。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p>OpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5</p>\n"},{"title":"【第二章】图像处理基础","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":null,"date":"2023-03-24T18:20:42.000Z","updated":"2023-03-24T18:20:42.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/walle.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***【第二章】图像处理基础***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>\n>2. [图像的基础表示方法](#图像的基础表示方法 \"图像的基础表示方法\")\n>\n>    2.1. [二值图像](#二值图像 \"二值图像\")\n>    \n>    2.2. [灰度图像](#灰度图像 \"灰度图像\")\n>    \n>    2.3. [彩色图像](#彩色图像 \"彩色图像\")\n>\n>3. [像素处理](#像素处理 \"像素处理\")\n>\n>4. [图像的属性](#图像的属性 \"图像的属性\")\n>\n>5. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n>\n>6. [引用文献](#引用文献 \"引用文献\")\n\n\n\n\n\n# 概述\n\n使用Python调用OpenCV函数库的功能，我们还需要了解Numpy函数库的用法，尤其是Numpy.array库，因为图像基本在处理的时候都是使用数组的方式的进行。\n\n\n\n# 图像的基础表示方法\n\n二值图像、灰度图像、彩色图像的表达方式。\n\n## 二值图像\n\n指的是仅仅包含了黑色和白色的两种颜色的图片。\n\n![二值图像](https://www.synotech.top:5523/uploads/2023/04/12/202304122131607.png)\n\n实际上的矩阵形式如下：\n\n![矩阵形式](https://www.synotech.top:5523/uploads/2023/04/12/202304122132313.png)\n\n\n\n## 灰度图像\n\n计算机会将灰度处理为256个灰度级，取值范围[0,255]。其中，数值“255”表示纯白色，数值“0”表示纯黑色。正好是一个字节（8位二进制值）来表示。\n\n![Lena_gray_image](https://www.synotech.top:5523/uploads/2023/04/12/202304122132236.png)\n\n实际的矩阵形式如下：\n\n![lena_gray_imageValue](https://www.synotech.top:5523/uploads/2023/04/12/202304122133725.png)\n\n\n\n## 彩色图像\n\n彩色图像是一个包含了三个颜色通道的图像，分别是红、绿、蓝，这三个通道共同用来表示颜色信息。\n\n![RGB_image](https://www.synotech.top:5523/uploads/2023/04/12/202304122133702.png)\n\n\n\n# 像素处理\n\n- Numpy处理图像数组矩阵\n\n- Numpy.array提供了item()和itemset()函数访问和修改像素值（函数是经过优化处理的，可以提高处理效率）\n\n- ROI的概念，Region of Interest感兴趣区域\n\n- 通道处理：通道拆分split、通道合并merge\n\n![split_channel](https://www.synotech.top:5523/uploads/2023/04/12/202304122133107.png)\n\nMerge通道之后的效果：\n\n![merge_channel](https://www.synotech.top:5523/uploads/2023/04/12/202304122133564.png)\n\n\n\n# 图像的属性\n\n- Shape属性，彩色图像的shape属性是包含行数、列数、通道数的数组，二值或灰度图像则返回行数和列数。\n\n- Size属性，返回值是图像的像素数目。\n\n- Dtype属性，返回图像的数据类型。\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n\nOpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5\n","source":"_posts/007-ComputerVision/01-OpenCV/【第二章】图像处理基础.md","raw":"---\ntitle: 【第二章】图像处理基础\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax:\ndate: 2023-03-25 02:20:42\nupdated: 2023-03-25 02:20:42\ntags: OpenCV\ncategories: \n- ComputerVision\n- OpenCV\nkeywords:\ndescription:\ntop_img: \ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***【第二章】图像处理基础***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n## 大纲：\n\n>1. [概述](#概述 \"概述\")\n>\n>2. [图像的基础表示方法](#图像的基础表示方法 \"图像的基础表示方法\")\n>\n>    2.1. [二值图像](#二值图像 \"二值图像\")\n>    \n>    2.2. [灰度图像](#灰度图像 \"灰度图像\")\n>    \n>    2.3. [彩色图像](#彩色图像 \"彩色图像\")\n>\n>3. [像素处理](#像素处理 \"像素处理\")\n>\n>4. [图像的属性](#图像的属性 \"图像的属性\")\n>\n>5. [后续章节内容预告](#后续章节内容预告 \"后续章节内容预告\")\n>\n>6. [引用文献](#引用文献 \"引用文献\")\n\n\n\n\n\n# 概述\n\n使用Python调用OpenCV函数库的功能，我们还需要了解Numpy函数库的用法，尤其是Numpy.array库，因为图像基本在处理的时候都是使用数组的方式的进行。\n\n\n\n# 图像的基础表示方法\n\n二值图像、灰度图像、彩色图像的表达方式。\n\n## 二值图像\n\n指的是仅仅包含了黑色和白色的两种颜色的图片。\n\n![二值图像](https://www.synotech.top:5523/uploads/2023/04/12/202304122131607.png)\n\n实际上的矩阵形式如下：\n\n![矩阵形式](https://www.synotech.top:5523/uploads/2023/04/12/202304122132313.png)\n\n\n\n## 灰度图像\n\n计算机会将灰度处理为256个灰度级，取值范围[0,255]。其中，数值“255”表示纯白色，数值“0”表示纯黑色。正好是一个字节（8位二进制值）来表示。\n\n![Lena_gray_image](https://www.synotech.top:5523/uploads/2023/04/12/202304122132236.png)\n\n实际的矩阵形式如下：\n\n![lena_gray_imageValue](https://www.synotech.top:5523/uploads/2023/04/12/202304122133725.png)\n\n\n\n## 彩色图像\n\n彩色图像是一个包含了三个颜色通道的图像，分别是红、绿、蓝，这三个通道共同用来表示颜色信息。\n\n![RGB_image](https://www.synotech.top:5523/uploads/2023/04/12/202304122133702.png)\n\n\n\n# 像素处理\n\n- Numpy处理图像数组矩阵\n\n- Numpy.array提供了item()和itemset()函数访问和修改像素值（函数是经过优化处理的，可以提高处理效率）\n\n- ROI的概念，Region of Interest感兴趣区域\n\n- 通道处理：通道拆分split、通道合并merge\n\n![split_channel](https://www.synotech.top:5523/uploads/2023/04/12/202304122133107.png)\n\nMerge通道之后的效果：\n\n![merge_channel](https://www.synotech.top:5523/uploads/2023/04/12/202304122133564.png)\n\n\n\n# 图像的属性\n\n- Shape属性，彩色图像的shape属性是包含行数、列数、通道数的数组，二值或灰度图像则返回行数和列数。\n\n- Size属性，返回值是图像的像素数目。\n\n- Dtype属性，返回图像的数据类型。\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n\nOpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5\n","slug":"007-ComputerVision/01-OpenCV/【第二章】图像处理基础","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t20018u9rlhuivh769","content":"<h1><em><strong>【第二章】图像处理基础</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li>\n<p><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%9B%BE%E5%83%8F%E7%9A%84%E5%9F%BA%E7%A1%80%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95\" title=\"图像的基础表示方法\">图像的基础表示方法</a></p>\n<p>2.1. <a href=\"#%E4%BA%8C%E5%80%BC%E5%9B%BE%E5%83%8F\" title=\"二值图像\">二值图像</a></p>\n<p>2.2. <a href=\"#%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F\" title=\"灰度图像\">灰度图像</a></p>\n<p>2.3. <a href=\"#%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F\" title=\"彩色图像\">彩色图像</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%83%8F%E7%B4%A0%E5%A4%84%E7%90%86\" title=\"像素处理\">像素处理</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%9B%BE%E5%83%8F%E7%9A%84%E5%B1%9E%E6%80%A7\" title=\"图像的属性\">图像的属性</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a></p>\n</li>\n</ol>\n</blockquote>\n<h1>概述</h1>\n<p>使用Python调用OpenCV函数库的功能，我们还需要了解Numpy函数库的用法，尤其是Numpy.array库，因为图像基本在处理的时候都是使用数组的方式的进行。</p>\n<h1>图像的基础表示方法</h1>\n<p>二值图像、灰度图像、彩色图像的表达方式。</p>\n<h2 id=\"二值图像\">二值图像</h2>\n<p>指的是仅仅包含了黑色和白色的两种颜色的图片。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122131607.png\" alt=\"二值图像\"></p>\n<p>实际上的矩阵形式如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122132313.png\" alt=\"矩阵形式\"></p>\n<h2 id=\"灰度图像\">灰度图像</h2>\n<p>计算机会将灰度处理为256个灰度级，取值范围[0,255]。其中，数值“255”表示纯白色，数值“0”表示纯黑色。正好是一个字节（8位二进制值）来表示。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122132236.png\" alt=\"Lena_gray_image\"></p>\n<p>实际的矩阵形式如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122133725.png\" alt=\"lena_gray_imageValue\"></p>\n<h2 id=\"彩色图像\">彩色图像</h2>\n<p>彩色图像是一个包含了三个颜色通道的图像，分别是红、绿、蓝，这三个通道共同用来表示颜色信息。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122133702.png\" alt=\"RGB_image\"></p>\n<h1>像素处理</h1>\n<ul>\n<li>\n<p>Numpy处理图像数组矩阵</p>\n</li>\n<li>\n<p>Numpy.array提供了item()和itemset()函数访问和修改像素值（函数是经过优化处理的，可以提高处理效率）</p>\n</li>\n<li>\n<p>ROI的概念，Region of Interest感兴趣区域</p>\n</li>\n<li>\n<p>通道处理：通道拆分split、通道合并merge</p>\n</li>\n</ul>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122133107.png\" alt=\"split_channel\"></p>\n<p>Merge通道之后的效果：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122133564.png\" alt=\"merge_channel\"></p>\n<h1>图像的属性</h1>\n<ul>\n<li>\n<p>Shape属性，彩色图像的shape属性是包含行数、列数、通道数的数组，二值或灰度图像则返回行数和列数。</p>\n</li>\n<li>\n<p>Size属性，返回值是图像的像素数目。</p>\n</li>\n<li>\n<p>Dtype属性，返回图像的数据类型。</p>\n</li>\n</ul>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p>OpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5</p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>【第二章】图像处理基础</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h2 id=\"大纲：\">大纲：</h2>\n<blockquote>\n<ol>\n<li>\n<p><a href=\"#%E6%A6%82%E8%BF%B0\" title=\"概述\">概述</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%9B%BE%E5%83%8F%E7%9A%84%E5%9F%BA%E7%A1%80%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95\" title=\"图像的基础表示方法\">图像的基础表示方法</a></p>\n<p>2.1. <a href=\"#%E4%BA%8C%E5%80%BC%E5%9B%BE%E5%83%8F\" title=\"二值图像\">二值图像</a></p>\n<p>2.2. <a href=\"#%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F\" title=\"灰度图像\">灰度图像</a></p>\n<p>2.3. <a href=\"#%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F\" title=\"彩色图像\">彩色图像</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%83%8F%E7%B4%A0%E5%A4%84%E7%90%86\" title=\"像素处理\">像素处理</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%9B%BE%E5%83%8F%E7%9A%84%E5%B1%9E%E6%80%A7\" title=\"图像的属性\">图像的属性</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9%E9%A2%84%E5%91%8A\" title=\"后续章节内容预告\">后续章节内容预告</a></p>\n</li>\n<li>\n<p><a href=\"#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE\" title=\"引用文献\">引用文献</a></p>\n</li>\n</ol>\n</blockquote>\n<h1>概述</h1>\n<p>使用Python调用OpenCV函数库的功能，我们还需要了解Numpy函数库的用法，尤其是Numpy.array库，因为图像基本在处理的时候都是使用数组的方式的进行。</p>\n<h1>图像的基础表示方法</h1>\n<p>二值图像、灰度图像、彩色图像的表达方式。</p>\n<h2 id=\"二值图像\">二值图像</h2>\n<p>指的是仅仅包含了黑色和白色的两种颜色的图片。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122131607.png\" alt=\"二值图像\"></p>\n<p>实际上的矩阵形式如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122132313.png\" alt=\"矩阵形式\"></p>\n<h2 id=\"灰度图像\">灰度图像</h2>\n<p>计算机会将灰度处理为256个灰度级，取值范围[0,255]。其中，数值“255”表示纯白色，数值“0”表示纯黑色。正好是一个字节（8位二进制值）来表示。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122132236.png\" alt=\"Lena_gray_image\"></p>\n<p>实际的矩阵形式如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122133725.png\" alt=\"lena_gray_imageValue\"></p>\n<h2 id=\"彩色图像\">彩色图像</h2>\n<p>彩色图像是一个包含了三个颜色通道的图像，分别是红、绿、蓝，这三个通道共同用来表示颜色信息。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122133702.png\" alt=\"RGB_image\"></p>\n<h1>像素处理</h1>\n<ul>\n<li>\n<p>Numpy处理图像数组矩阵</p>\n</li>\n<li>\n<p>Numpy.array提供了item()和itemset()函数访问和修改像素值（函数是经过优化处理的，可以提高处理效率）</p>\n</li>\n<li>\n<p>ROI的概念，Region of Interest感兴趣区域</p>\n</li>\n<li>\n<p>通道处理：通道拆分split、通道合并merge</p>\n</li>\n</ul>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122133107.png\" alt=\"split_channel\"></p>\n<p>Merge通道之后的效果：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122133564.png\" alt=\"merge_channel\"></p>\n<h1>图像的属性</h1>\n<ul>\n<li>\n<p>Shape属性，彩色图像的shape属性是包含行数、列数、通道数的数组，二值或灰度图像则返回行数和列数。</p>\n</li>\n<li>\n<p>Size属性，返回值是图像的像素数目。</p>\n</li>\n<li>\n<p>Dtype属性，返回图像的数据类型。</p>\n</li>\n</ul>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p>OpenCV轻松入门：面向Python/李立宗著.—北京：电子工业出版社，2019.5</p>\n"},{"title":"CLOCS后融合","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":null,"date":"2023-04-10T16:19:54.000Z","updated":"2023-04-10T16:19:54.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/gkihqEjXxJ5UZ1C.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n## ***CLOCS后融合***\n\n本笔记主要记录在学习阅读CLOCS论文时候，摘录了一些内容汇总。\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n------\n\n**CLOCS**将3D检测结果和2D检测结果通过一个CLOCS网络进行融合，能提高1-2个AP点，而且可以提高对遮挡物体的召回率。\n\n# 创新点：\n\n1. 使用任何一对预先训练好的2D/3D检测模型，不需要额外的训练，因此可以很容易和任何检测相关的模型适配起来。\n\n1. CLOCs的设计目的是利用二维和三维检测的几何和语义一致性，自动学习训练数据的概率依赖进行融合。\n\n1. CLOCs改进了单模态检测器的检测性能，以达到新的水平\n\n\n\n# 主要工作：\n\n将2D/3D的proposals在对应的模态中的confidence score进行关联，使用Geometric-consistency和Semantic-consistency两种约束方式。\n\n\n\n## Geometric-consistency\n\n如图所示，当2D/3D检测同时检测到这个物体的时候，都是TP的情况下，我们认为，二维和三维对应的焦点是大概率一致的，或者存在一些小的偏差。但是，如果检测结果是FP，那么两个检测器的结果上焦点是无法对应上的，因此作者任务这种几何结构的一致性可以作为检测结果的联系。\n\n![几何结构的联系](https://www.synotech.top:5523/uploads/2023/04/12/202304122045108.png)\n\n\n\n## Semantic consistency\n\n对于每一个模态的检测器，可能存在多种类别的输出，作者在融合阶段仅仅只是对同类别的objects进行融合。\n\n\n\n# **网络结构**：\n\n![网络结构](https://www.synotech.top:5523/uploads/2023/04/12/202304122051468.png)\n\n## 三个阶段：\n\n1. 2D和3D的目标检测器分别提出proposals\n\n1. 将两种模态的proposals编码成稀疏张量\n\n1. 对于非空的元素采用二维卷积做对应的特征融合\n\n\n\n### 1.张量编码\n\n- 在图像中的第i个检测结果和点云中的第j个的几何一致性（也就是前面提到的Geometric-consistency，这里用IOU表示）\n\n- 图像检测的第i个检测到的物体的置信度分数\n\n- 点云检测第j个检测到的物体的置信度分数\n\n- 点云场景下检测到的第j个物体到地面的归一化距离，**xy plane**\n\n最终将结果通过一个系数的四维张量进行表示，直接输入卷积网络融合。\n\n![四维张量](https://www.synotech.top:5523/uploads/2023/04/12/202304122053850.png)\n\n### 2.实验结果\n\n![实验结果](https://www.synotech.top:5523/uploads/2023/04/12/202304122054004.png)\n\n\n\n实际运行的测试情况如下（融合second+cascade-RCNN)：\n\n```shell\n2022-09-20 20:53:33,710   INFO  \nCar AP@0.70, 0.70, 0.70:\nbbox AP:95.6546, 89.7245, 88.7548\nbev  AP:89.7622, 87.5664, 85.5937\n3d   AP:88.2785, 77.6541, 75.9611\naos  AP:95.61, 89.55, 88.48\n\nCar AP_R40@0.70, 0.70, 0.70:\nbbox AP:97.4895, 93.7059, 91.4775\nbev  AP:92.5223, 88.3756, 87.3147\n3d   AP:88.7911, 79.0908, 76.0444\naos  AP:97.45, 93.48, 91.17\n\nCar AP@0.70, 0.50, 0.50:\nbbox AP:95.6546, 89.7245, 88.7548\nbev  AP:95.8589, 90.0293, 89.3180\n3d   AP:95.8132, 89.9309, 89.1922\naos  AP:95.61, 89.55, 88.48\n\nCar AP_R40@0.70, 0.50, 0.50:\nbbox AP:97.4895, 93.7059, 91.4775\nbev  AP:97.8720, 94.9669, 94.1804\n3d   AP:97.8428, 94.7969, 93.6374\naos  AP:97.45, 93.48, 91.17\n\n\n```\n\n\n\n## 思考\n\nLate-fusion中单模态检测器之间互不干扰的特性做了多种检测器的组合，这种方式可以忽略传感器进行early-fusion时候，找不到匹配的特征或者匹配的维度特征不丰富。\n\n未来可以实现deep-fusion的网络，原因主要是这种融合方式的自由度更大，在特征层的融合可以实现不同传感器信息之间的互补，不采用early-fusion的方式则是因为这种融合方式，在初期阶段会存在更多的视角，特征表示上的差距导致了融合困难。\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n\nhttps://arxiv.org/abs/2009.00784\n","source":"_posts/008-AutonomousDriving/01-感知/CLOCS后融合.md","raw":"---\ntitle: CLOCS后融合\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: \ndate: 2023-04-11 00:19:54\nupdated: 2023-04-11 00:19:54\ntags: 后融合\ncategories: \n- 自动驾驶\n- 后融合\nkeywords:\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n\n---\n\n## ***CLOCS后融合***\n\n本笔记主要记录在学习阅读CLOCS论文时候，摘录了一些内容汇总。\n\n### ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n------\n\n**CLOCS**将3D检测结果和2D检测结果通过一个CLOCS网络进行融合，能提高1-2个AP点，而且可以提高对遮挡物体的召回率。\n\n# 创新点：\n\n1. 使用任何一对预先训练好的2D/3D检测模型，不需要额外的训练，因此可以很容易和任何检测相关的模型适配起来。\n\n1. CLOCs的设计目的是利用二维和三维检测的几何和语义一致性，自动学习训练数据的概率依赖进行融合。\n\n1. CLOCs改进了单模态检测器的检测性能，以达到新的水平\n\n\n\n# 主要工作：\n\n将2D/3D的proposals在对应的模态中的confidence score进行关联，使用Geometric-consistency和Semantic-consistency两种约束方式。\n\n\n\n## Geometric-consistency\n\n如图所示，当2D/3D检测同时检测到这个物体的时候，都是TP的情况下，我们认为，二维和三维对应的焦点是大概率一致的，或者存在一些小的偏差。但是，如果检测结果是FP，那么两个检测器的结果上焦点是无法对应上的，因此作者任务这种几何结构的一致性可以作为检测结果的联系。\n\n![几何结构的联系](https://www.synotech.top:5523/uploads/2023/04/12/202304122045108.png)\n\n\n\n## Semantic consistency\n\n对于每一个模态的检测器，可能存在多种类别的输出，作者在融合阶段仅仅只是对同类别的objects进行融合。\n\n\n\n# **网络结构**：\n\n![网络结构](https://www.synotech.top:5523/uploads/2023/04/12/202304122051468.png)\n\n## 三个阶段：\n\n1. 2D和3D的目标检测器分别提出proposals\n\n1. 将两种模态的proposals编码成稀疏张量\n\n1. 对于非空的元素采用二维卷积做对应的特征融合\n\n\n\n### 1.张量编码\n\n- 在图像中的第i个检测结果和点云中的第j个的几何一致性（也就是前面提到的Geometric-consistency，这里用IOU表示）\n\n- 图像检测的第i个检测到的物体的置信度分数\n\n- 点云检测第j个检测到的物体的置信度分数\n\n- 点云场景下检测到的第j个物体到地面的归一化距离，**xy plane**\n\n最终将结果通过一个系数的四维张量进行表示，直接输入卷积网络融合。\n\n![四维张量](https://www.synotech.top:5523/uploads/2023/04/12/202304122053850.png)\n\n### 2.实验结果\n\n![实验结果](https://www.synotech.top:5523/uploads/2023/04/12/202304122054004.png)\n\n\n\n实际运行的测试情况如下（融合second+cascade-RCNN)：\n\n```shell\n2022-09-20 20:53:33,710   INFO  \nCar AP@0.70, 0.70, 0.70:\nbbox AP:95.6546, 89.7245, 88.7548\nbev  AP:89.7622, 87.5664, 85.5937\n3d   AP:88.2785, 77.6541, 75.9611\naos  AP:95.61, 89.55, 88.48\n\nCar AP_R40@0.70, 0.70, 0.70:\nbbox AP:97.4895, 93.7059, 91.4775\nbev  AP:92.5223, 88.3756, 87.3147\n3d   AP:88.7911, 79.0908, 76.0444\naos  AP:97.45, 93.48, 91.17\n\nCar AP@0.70, 0.50, 0.50:\nbbox AP:95.6546, 89.7245, 88.7548\nbev  AP:95.8589, 90.0293, 89.3180\n3d   AP:95.8132, 89.9309, 89.1922\naos  AP:95.61, 89.55, 88.48\n\nCar AP_R40@0.70, 0.50, 0.50:\nbbox AP:97.4895, 93.7059, 91.4775\nbev  AP:97.8720, 94.9669, 94.1804\n3d   AP:97.8428, 94.7969, 93.6374\naos  AP:97.45, 93.48, 91.17\n\n\n```\n\n\n\n## 思考\n\nLate-fusion中单模态检测器之间互不干扰的特性做了多种检测器的组合，这种方式可以忽略传感器进行early-fusion时候，找不到匹配的特征或者匹配的维度特征不丰富。\n\n未来可以实现deep-fusion的网络，原因主要是这种融合方式的自由度更大，在特征层的融合可以实现不同传感器信息之间的互补，不采用early-fusion的方式则是因为这种融合方式，在初期阶段会存在更多的视角，特征表示上的差距导致了融合困难。\n\n\n\n\n***\n\n### ***后续章节内容预告***：\n\n***\n\n### 引用文献：\n\nhttps://arxiv.org/abs/2009.00784\n","slug":"008-AutonomousDriving/01-感知/CLOCS后融合","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t3001bu9rl95628i0x","content":"<h2 id=\"CLOCS后融合\"><em><strong>CLOCS后融合</strong></em></h2>\n<p>本笔记主要记录在学习阅读CLOCS论文时候，摘录了一些内容汇总。</p>\n<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<p><strong>CLOCS</strong>将3D检测结果和2D检测结果通过一个CLOCS网络进行融合，能提高1-2个AP点，而且可以提高对遮挡物体的召回率。</p>\n<h1>创新点：</h1>\n<ol>\n<li>\n<p>使用任何一对预先训练好的2D/3D检测模型，不需要额外的训练，因此可以很容易和任何检测相关的模型适配起来。</p>\n</li>\n<li>\n<p>CLOCs的设计目的是利用二维和三维检测的几何和语义一致性，自动学习训练数据的概率依赖进行融合。</p>\n</li>\n<li>\n<p>CLOCs改进了单模态检测器的检测性能，以达到新的水平</p>\n</li>\n</ol>\n<h1>主要工作：</h1>\n<p>将2D/3D的proposals在对应的模态中的confidence score进行关联，使用Geometric-consistency和Semantic-consistency两种约束方式。</p>\n<h2 id=\"Geometric-consistency\">Geometric-consistency</h2>\n<p>如图所示，当2D/3D检测同时检测到这个物体的时候，都是TP的情况下，我们认为，二维和三维对应的焦点是大概率一致的，或者存在一些小的偏差。但是，如果检测结果是FP，那么两个检测器的结果上焦点是无法对应上的，因此作者任务这种几何结构的一致性可以作为检测结果的联系。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122045108.png\" alt=\"几何结构的联系\"></p>\n<h2 id=\"Semantic-consistency\">Semantic consistency</h2>\n<p>对于每一个模态的检测器，可能存在多种类别的输出，作者在融合阶段仅仅只是对同类别的objects进行融合。</p>\n<h1><strong>网络结构</strong>：</h1>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122051468.png\" alt=\"网络结构\"></p>\n<h2 id=\"三个阶段：\">三个阶段：</h2>\n<ol>\n<li>\n<p>2D和3D的目标检测器分别提出proposals</p>\n</li>\n<li>\n<p>将两种模态的proposals编码成稀疏张量</p>\n</li>\n<li>\n<p>对于非空的元素采用二维卷积做对应的特征融合</p>\n</li>\n</ol>\n<h3 id=\"1-张量编码\">1.张量编码</h3>\n<ul>\n<li>\n<p>在图像中的第i个检测结果和点云中的第j个的几何一致性（也就是前面提到的Geometric-consistency，这里用IOU表示）</p>\n</li>\n<li>\n<p>图像检测的第i个检测到的物体的置信度分数</p>\n</li>\n<li>\n<p>点云检测第j个检测到的物体的置信度分数</p>\n</li>\n<li>\n<p>点云场景下检测到的第j个物体到地面的归一化距离，<strong>xy plane</strong></p>\n</li>\n</ul>\n<p>最终将结果通过一个系数的四维张量进行表示，直接输入卷积网络融合。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122053850.png\" alt=\"四维张量\"></p>\n<h3 id=\"2-实验结果\">2.实验结果</h3>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122054004.png\" alt=\"实验结果\"></p>\n<p>实际运行的测试情况如下（融合second+cascade-RCNN)：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2022-09-20 20:53:33,710   INFO  </span><br><span class=\"line\">Car AP@0.70, 0.70, 0.70:</span><br><span class=\"line\">bbox AP:95.6546, 89.7245, 88.7548</span><br><span class=\"line\">bev  AP:89.7622, 87.5664, 85.5937</span><br><span class=\"line\">3d   AP:88.2785, 77.6541, 75.9611</span><br><span class=\"line\">aos  AP:95.61, 89.55, 88.48</span><br><span class=\"line\"></span><br><span class=\"line\">Car AP_R40@0.70, 0.70, 0.70:</span><br><span class=\"line\">bbox AP:97.4895, 93.7059, 91.4775</span><br><span class=\"line\">bev  AP:92.5223, 88.3756, 87.3147</span><br><span class=\"line\">3d   AP:88.7911, 79.0908, 76.0444</span><br><span class=\"line\">aos  AP:97.45, 93.48, 91.17</span><br><span class=\"line\"></span><br><span class=\"line\">Car AP@0.70, 0.50, 0.50:</span><br><span class=\"line\">bbox AP:95.6546, 89.7245, 88.7548</span><br><span class=\"line\">bev  AP:95.8589, 90.0293, 89.3180</span><br><span class=\"line\">3d   AP:95.8132, 89.9309, 89.1922</span><br><span class=\"line\">aos  AP:95.61, 89.55, 88.48</span><br><span class=\"line\"></span><br><span class=\"line\">Car AP_R40@0.70, 0.50, 0.50:</span><br><span class=\"line\">bbox AP:97.4895, 93.7059, 91.4775</span><br><span class=\"line\">bev  AP:97.8720, 94.9669, 94.1804</span><br><span class=\"line\">3d   AP:97.8428, 94.7969, 93.6374</span><br><span class=\"line\">aos  AP:97.45, 93.48, 91.17</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"思考\">思考</h2>\n<p>Late-fusion中单模态检测器之间互不干扰的特性做了多种检测器的组合，这种方式可以忽略传感器进行early-fusion时候，找不到匹配的特征或者匹配的维度特征不丰富。</p>\n<p>未来可以实现deep-fusion的网络，原因主要是这种融合方式的自由度更大，在特征层的融合可以实现不同传感器信息之间的互补，不采用early-fusion的方式则是因为这种融合方式，在初期阶段会存在更多的视角，特征表示上的差距导致了融合困难。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p><a href=\"https://arxiv.org/abs/2009.00784\">https://arxiv.org/abs/2009.00784</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h2 id=\"CLOCS后融合\"><em><strong>CLOCS后融合</strong></em></h2>\n<p>本笔记主要记录在学习阅读CLOCS论文时候，摘录了一些内容汇总。</p>\n<h3 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h3>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<p><strong>CLOCS</strong>将3D检测结果和2D检测结果通过一个CLOCS网络进行融合，能提高1-2个AP点，而且可以提高对遮挡物体的召回率。</p>\n<h1>创新点：</h1>\n<ol>\n<li>\n<p>使用任何一对预先训练好的2D/3D检测模型，不需要额外的训练，因此可以很容易和任何检测相关的模型适配起来。</p>\n</li>\n<li>\n<p>CLOCs的设计目的是利用二维和三维检测的几何和语义一致性，自动学习训练数据的概率依赖进行融合。</p>\n</li>\n<li>\n<p>CLOCs改进了单模态检测器的检测性能，以达到新的水平</p>\n</li>\n</ol>\n<h1>主要工作：</h1>\n<p>将2D/3D的proposals在对应的模态中的confidence score进行关联，使用Geometric-consistency和Semantic-consistency两种约束方式。</p>\n<h2 id=\"Geometric-consistency\">Geometric-consistency</h2>\n<p>如图所示，当2D/3D检测同时检测到这个物体的时候，都是TP的情况下，我们认为，二维和三维对应的焦点是大概率一致的，或者存在一些小的偏差。但是，如果检测结果是FP，那么两个检测器的结果上焦点是无法对应上的，因此作者任务这种几何结构的一致性可以作为检测结果的联系。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122045108.png\" alt=\"几何结构的联系\"></p>\n<h2 id=\"Semantic-consistency\">Semantic consistency</h2>\n<p>对于每一个模态的检测器，可能存在多种类别的输出，作者在融合阶段仅仅只是对同类别的objects进行融合。</p>\n<h1><strong>网络结构</strong>：</h1>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122051468.png\" alt=\"网络结构\"></p>\n<h2 id=\"三个阶段：\">三个阶段：</h2>\n<ol>\n<li>\n<p>2D和3D的目标检测器分别提出proposals</p>\n</li>\n<li>\n<p>将两种模态的proposals编码成稀疏张量</p>\n</li>\n<li>\n<p>对于非空的元素采用二维卷积做对应的特征融合</p>\n</li>\n</ol>\n<h3 id=\"1-张量编码\">1.张量编码</h3>\n<ul>\n<li>\n<p>在图像中的第i个检测结果和点云中的第j个的几何一致性（也就是前面提到的Geometric-consistency，这里用IOU表示）</p>\n</li>\n<li>\n<p>图像检测的第i个检测到的物体的置信度分数</p>\n</li>\n<li>\n<p>点云检测第j个检测到的物体的置信度分数</p>\n</li>\n<li>\n<p>点云场景下检测到的第j个物体到地面的归一化距离，<strong>xy plane</strong></p>\n</li>\n</ul>\n<p>最终将结果通过一个系数的四维张量进行表示，直接输入卷积网络融合。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122053850.png\" alt=\"四维张量\"></p>\n<h3 id=\"2-实验结果\">2.实验结果</h3>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/04/12/202304122054004.png\" alt=\"实验结果\"></p>\n<p>实际运行的测试情况如下（融合second+cascade-RCNN)：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2022-09-20 20:53:33,710   INFO  </span><br><span class=\"line\">Car AP@0.70, 0.70, 0.70:</span><br><span class=\"line\">bbox AP:95.6546, 89.7245, 88.7548</span><br><span class=\"line\">bev  AP:89.7622, 87.5664, 85.5937</span><br><span class=\"line\">3d   AP:88.2785, 77.6541, 75.9611</span><br><span class=\"line\">aos  AP:95.61, 89.55, 88.48</span><br><span class=\"line\"></span><br><span class=\"line\">Car AP_R40@0.70, 0.70, 0.70:</span><br><span class=\"line\">bbox AP:97.4895, 93.7059, 91.4775</span><br><span class=\"line\">bev  AP:92.5223, 88.3756, 87.3147</span><br><span class=\"line\">3d   AP:88.7911, 79.0908, 76.0444</span><br><span class=\"line\">aos  AP:97.45, 93.48, 91.17</span><br><span class=\"line\"></span><br><span class=\"line\">Car AP@0.70, 0.50, 0.50:</span><br><span class=\"line\">bbox AP:95.6546, 89.7245, 88.7548</span><br><span class=\"line\">bev  AP:95.8589, 90.0293, 89.3180</span><br><span class=\"line\">3d   AP:95.8132, 89.9309, 89.1922</span><br><span class=\"line\">aos  AP:95.61, 89.55, 88.48</span><br><span class=\"line\"></span><br><span class=\"line\">Car AP_R40@0.70, 0.50, 0.50:</span><br><span class=\"line\">bbox AP:97.4895, 93.7059, 91.4775</span><br><span class=\"line\">bev  AP:97.8720, 94.9669, 94.1804</span><br><span class=\"line\">3d   AP:97.8428, 94.7969, 93.6374</span><br><span class=\"line\">aos  AP:97.45, 93.48, 91.17</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"思考\">思考</h2>\n<p>Late-fusion中单模态检测器之间互不干扰的特性做了多种检测器的组合，这种方式可以忽略传感器进行early-fusion时候，找不到匹配的特征或者匹配的维度特征不丰富。</p>\n<p>未来可以实现deep-fusion的网络，原因主要是这种融合方式的自由度更大，在特征层的融合可以实现不同传感器信息之间的互补，不采用early-fusion的方式则是因为这种融合方式，在初期阶段会存在更多的视角，特征表示上的差距导致了融合困难。</p>\n<hr>\n<h3 id=\"后续章节内容预告：\"><em><strong>后续章节内容预告</strong></em>：</h3>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p><a href=\"https://arxiv.org/abs/2009.00784\">https://arxiv.org/abs/2009.00784</a></p>\n"},{"title":"3步实现在Jetson Orin上联调Autoware.Universe和Carla-0.9.15","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2024-03-14T17:49:34.000Z","updated":"2024-03-14T17:49:34.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/5f363d07a24be_270_185.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***如何3步实现在Jetson Orin上联调Autoware.Universe和Carla-0.9.15***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n## 准备工作\n\n### 软件\n\n1. Carla 0.9.15 (the newest version)\n2. Autoware.universe (V1.0 Branch, will be changed to other version)\n3. Autoware-Carla-Bridge (Main Branch)\n4. ROS2 (Humble version)\n5. Ubuntu 22.04 (LTS Released version)\n6. Docker\n\n### 硬件\n\nJetson Orin开发者套件在机器人和自动驾驶领域都是非常好的一个开发套件\n\n1. 一个全千兆的交换机或者路由器（局域网内主机和Jetson Orin通信需要）\n2. 一个Jetson Orin开发套件 32G/64G，带外置NVME的500G的SSD硬盘\n3. 一个Ubuntu22.04的主机\n4. 两个或以上的超五类或者六类网线\n\n所有的设备必须在局域网内相互连接能够通信，全千兆的规格会更好。\n\n------\n\n***Note:*** 为了说明方便，我使用“主机”来表示带有GPU显卡(2070或者30/40系列的显卡均可)的机器。这个主机将会用来部署Carla-0.9.15仿真模拟器和Autoware-Carla-Bridge ROS2所有包。同时，使用Jetson Orin来表示嵌入式系统，这个嵌入式开发套件将会用来部署Autoware.Universe的技术栈软件。\n\n------\n\n> 在你开始下面的步骤之前，我建议你使用下面的指令先拉取一些必要的docker镜像，因此在你阅读完这个教程之后，立马可以开始部署:\n\n```bash\n $ docker pull carlasim/carla:0.9.15\n $ docker pull tumgeka/carla-autoware-bridge:latest\n $ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n```\n\n\n\n## 3步快速部署\n\n### 步骤一：部署carla-0.9.15仿真器\n\n这个步骤时基于carla官方发布的docker镜像，你需要拉取这个镜像然后在你的主机上运行起来。必须检查下rpc-port端口是否被占用。\n\n```bash\n$ docker pull carlasim/carla:0.9.15\n$ docker run --privileged --gpus all --net=host -e DISPLAY=$DISPLAY carlasim/carla:0.9.15 /bin/bash ./CarlaUE4.sh -carla-rpc-port=1403\n```\n\n### 步骤二：部署autoware-carla-bridge\n\n这一步主要是基于TUM团队发布的镜像，在我的笔记本上需要设置timeout为一个更大的值，比如10000才可以，默认值是5000ms。\n\n```bash\n$ docker pull tumgeka/carla-autoware-bridge:latest\n$ docker run -it -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --network host tumgeka/carla-autoware-bridge:latest\n$ ros2 launch carla_autoware_bridge carla_aw_bridge.launch.py port:=1403 town:=Town10HD timeout:=10000\n```\n\n### 步骤三：部署autowarea.unvierse技术栈到Jetson Orin上\n\n编译autoware.universe有时候很困难，因为有很多不同的依赖项。所以，我建议使用我发布的预编译的docker镜像来的更简单。下面的命令必须在Jetson Orin上运行。主要是因为我们想使用一个Jetson Orin设备来部署autoware.universe技术栈。\n\n***注意：***这个镜像有点大，并且只能在ARM64的平台上使用，所以Jetson系列的开发套件都能适用。\n\n```bash\n$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n$ sudo apt-get install python3-rocker\n$ rocker --user --nvidia --privileged --network host --x11 --volume $HOME/Documents  --volume $HOME/autoware -- 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n$ ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=carla_t2_vehicle sensor_model:=carla_t2_sensor_kit map_path:=/autoware1.0_ws/Town10/\n```\n\n> ***注意***：当你使用rocker命令的时候，可能会出现如下消息：***invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported***.\n>\n> 你只需要更改\"--gpus all\" 成 \"--runtime=nvidia\"，如下面的样例指令，你的指令将会和我的不一样，你应该在运行 ***rocker --user --nvidia xxxx** 命令后，粘贴终端反馈的指令信息来运行镜像。\n\n```bash\n$ docker run  --rm -it --network host   --runtime=nvidia --privileged  -e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   -e XAUTHORITY=/tmp/.dockerafc7hfmf.xauth -v /tmp/.dockerafc7hfmf.xauth:/tmp/.dockerafc7hfmf.xauth   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /etc/localtime:/etc/localtime:ro  8ea8cd5cadfe\n```\n\n \n\n### 可选项\n\n#### 生成NPC\n\n如果你想添加一些NPC到你的carla仿真器中，你需要使用一个在autoware-carla-bridge docker容器中的Python脚本来启动。同时，注意要和之前一样使用相同的端口。这个**generate_traffic.py**应该步骤二启动的docker容器中的终端中运行。\n\n```bash\n$ python3 src/carla_autoware_bridge/utils/generate_traffic.py -p 1403\n```\n\n#### 部署autoware.universe技术栈在主机上\n\n在你的主机本地终端上编译autoware的源码，如果你不想部署在jetson嵌入式平台的话。\n\n```bash\n$ colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n```\n\n如果你想在有GPU显卡的主机上编译源码，你可能需要解决一些依赖问题。\n\n\n\n\n\n## 本地部署工作流\n\n本地部署工作流将会后来添加，添加一个Dockerfile，发布修改之后的源码。\n\n\n\n## TODO List\n\n- [ ] 添加另一个shuttle-bus的车辆模型到carla仿真器中\n- [ ] 简化Dockerfile给用户进行本地Autoware.universe镜像的编译\n- [ ] 上传修改的autoware.universe代码，解决感知模块无法启动的一些问题\n\n\n\n## 解决Bugs\n\ndocker运行时的错误，错误信息可能和如下的相似：\n\n```bash\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'csv'\ninvoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported.Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.: unknown.\n```\n\n你必须使用\"--runtime=nvidia \"标志莱欧启动一个容器。\n\n\n\n## 引用\n\nTUM论文：https://arxiv.org/abs/2402.11239\n\n","source":"_posts/008-AutonomousDriving/02-Autoware.universe/3步实现在Jetson-Orin上联调Autoware-Universe和Carla-0-9-15.md","raw":"---\ntitle: 3步实现在Jetson Orin上联调Autoware.Universe和Carla-0.9.15\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2024-03-15 01:49:34\nupdated: 2024-03-15 01:49:34\ntags: [Autoware.Universe, Ubuntu22, Carla]\ncategories:\n- 自动驾驶\n- Autoware.Universe\nkeywords:\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***如何3步实现在Jetson Orin上联调Autoware.Universe和Carla-0.9.15***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n## 准备工作\n\n### 软件\n\n1. Carla 0.9.15 (the newest version)\n2. Autoware.universe (V1.0 Branch, will be changed to other version)\n3. Autoware-Carla-Bridge (Main Branch)\n4. ROS2 (Humble version)\n5. Ubuntu 22.04 (LTS Released version)\n6. Docker\n\n### 硬件\n\nJetson Orin开发者套件在机器人和自动驾驶领域都是非常好的一个开发套件\n\n1. 一个全千兆的交换机或者路由器（局域网内主机和Jetson Orin通信需要）\n2. 一个Jetson Orin开发套件 32G/64G，带外置NVME的500G的SSD硬盘\n3. 一个Ubuntu22.04的主机\n4. 两个或以上的超五类或者六类网线\n\n所有的设备必须在局域网内相互连接能够通信，全千兆的规格会更好。\n\n------\n\n***Note:*** 为了说明方便，我使用“主机”来表示带有GPU显卡(2070或者30/40系列的显卡均可)的机器。这个主机将会用来部署Carla-0.9.15仿真模拟器和Autoware-Carla-Bridge ROS2所有包。同时，使用Jetson Orin来表示嵌入式系统，这个嵌入式开发套件将会用来部署Autoware.Universe的技术栈软件。\n\n------\n\n> 在你开始下面的步骤之前，我建议你使用下面的指令先拉取一些必要的docker镜像，因此在你阅读完这个教程之后，立马可以开始部署:\n\n```bash\n $ docker pull carlasim/carla:0.9.15\n $ docker pull tumgeka/carla-autoware-bridge:latest\n $ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n```\n\n\n\n## 3步快速部署\n\n### 步骤一：部署carla-0.9.15仿真器\n\n这个步骤时基于carla官方发布的docker镜像，你需要拉取这个镜像然后在你的主机上运行起来。必须检查下rpc-port端口是否被占用。\n\n```bash\n$ docker pull carlasim/carla:0.9.15\n$ docker run --privileged --gpus all --net=host -e DISPLAY=$DISPLAY carlasim/carla:0.9.15 /bin/bash ./CarlaUE4.sh -carla-rpc-port=1403\n```\n\n### 步骤二：部署autoware-carla-bridge\n\n这一步主要是基于TUM团队发布的镜像，在我的笔记本上需要设置timeout为一个更大的值，比如10000才可以，默认值是5000ms。\n\n```bash\n$ docker pull tumgeka/carla-autoware-bridge:latest\n$ docker run -it -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --network host tumgeka/carla-autoware-bridge:latest\n$ ros2 launch carla_autoware_bridge carla_aw_bridge.launch.py port:=1403 town:=Town10HD timeout:=10000\n```\n\n### 步骤三：部署autowarea.unvierse技术栈到Jetson Orin上\n\n编译autoware.universe有时候很困难，因为有很多不同的依赖项。所以，我建议使用我发布的预编译的docker镜像来的更简单。下面的命令必须在Jetson Orin上运行。主要是因为我们想使用一个Jetson Orin设备来部署autoware.universe技术栈。\n\n***注意：***这个镜像有点大，并且只能在ARM64的平台上使用，所以Jetson系列的开发套件都能适用。\n\n```bash\n$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n$ sudo apt-get install python3-rocker\n$ rocker --user --nvidia --privileged --network host --x11 --volume $HOME/Documents  --volume $HOME/autoware -- 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n$ ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=carla_t2_vehicle sensor_model:=carla_t2_sensor_kit map_path:=/autoware1.0_ws/Town10/\n```\n\n> ***注意***：当你使用rocker命令的时候，可能会出现如下消息：***invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported***.\n>\n> 你只需要更改\"--gpus all\" 成 \"--runtime=nvidia\"，如下面的样例指令，你的指令将会和我的不一样，你应该在运行 ***rocker --user --nvidia xxxx** 命令后，粘贴终端反馈的指令信息来运行镜像。\n\n```bash\n$ docker run  --rm -it --network host   --runtime=nvidia --privileged  -e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   -e XAUTHORITY=/tmp/.dockerafc7hfmf.xauth -v /tmp/.dockerafc7hfmf.xauth:/tmp/.dockerafc7hfmf.xauth   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /etc/localtime:/etc/localtime:ro  8ea8cd5cadfe\n```\n\n \n\n### 可选项\n\n#### 生成NPC\n\n如果你想添加一些NPC到你的carla仿真器中，你需要使用一个在autoware-carla-bridge docker容器中的Python脚本来启动。同时，注意要和之前一样使用相同的端口。这个**generate_traffic.py**应该步骤二启动的docker容器中的终端中运行。\n\n```bash\n$ python3 src/carla_autoware_bridge/utils/generate_traffic.py -p 1403\n```\n\n#### 部署autoware.universe技术栈在主机上\n\n在你的主机本地终端上编译autoware的源码，如果你不想部署在jetson嵌入式平台的话。\n\n```bash\n$ colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n```\n\n如果你想在有GPU显卡的主机上编译源码，你可能需要解决一些依赖问题。\n\n\n\n\n\n## 本地部署工作流\n\n本地部署工作流将会后来添加，添加一个Dockerfile，发布修改之后的源码。\n\n\n\n## TODO List\n\n- [ ] 添加另一个shuttle-bus的车辆模型到carla仿真器中\n- [ ] 简化Dockerfile给用户进行本地Autoware.universe镜像的编译\n- [ ] 上传修改的autoware.universe代码，解决感知模块无法启动的一些问题\n\n\n\n## 解决Bugs\n\ndocker运行时的错误，错误信息可能和如下的相似：\n\n```bash\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'csv'\ninvoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported.Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.: unknown.\n```\n\n你必须使用\"--runtime=nvidia \"标志莱欧启动一个容器。\n\n\n\n## 引用\n\nTUM论文：https://arxiv.org/abs/2402.11239\n\n","slug":"008-AutonomousDriving/02-Autoware.universe/3步实现在Jetson-Orin上联调Autoware-Universe和Carla-0-9-15","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t4001cu9rl1ed91mm3","content":"<h1><em><strong>如何3步实现在Jetson Orin上联调Autoware.Universe和Carla-0.9.15</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h2 id=\"准备工作\">准备工作</h2>\n<h3 id=\"软件\">软件</h3>\n<ol>\n<li>Carla 0.9.15 (the newest version)</li>\n<li>Autoware.universe (V1.0 Branch, will be changed to other version)</li>\n<li>Autoware-Carla-Bridge (Main Branch)</li>\n<li>ROS2 (Humble version)</li>\n<li>Ubuntu 22.04 (LTS Released version)</li>\n<li>Docker</li>\n</ol>\n<h3 id=\"硬件\">硬件</h3>\n<p>Jetson Orin开发者套件在机器人和自动驾驶领域都是非常好的一个开发套件</p>\n<ol>\n<li>一个全千兆的交换机或者路由器（局域网内主机和Jetson Orin通信需要）</li>\n<li>一个Jetson Orin开发套件 32G/64G，带外置NVME的500G的SSD硬盘</li>\n<li>一个Ubuntu22.04的主机</li>\n<li>两个或以上的超五类或者六类网线</li>\n</ol>\n<p>所有的设备必须在局域网内相互连接能够通信，全千兆的规格会更好。</p>\n<hr>\n<p><em><strong>Note:</strong></em> 为了说明方便，我使用“主机”来表示带有GPU显卡(2070或者30/40系列的显卡均可)的机器。这个主机将会用来部署Carla-0.9.15仿真模拟器和Autoware-Carla-Bridge ROS2所有包。同时，使用Jetson Orin来表示嵌入式系统，这个嵌入式开发套件将会用来部署Autoware.Universe的技术栈软件。</p>\n<hr>\n<blockquote>\n<p>在你开始下面的步骤之前，我建议你使用下面的指令先拉取一些必要的docker镜像，因此在你阅读完这个教程之后，立马可以开始部署:</p>\n</blockquote>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull carlasim/carla:0.9.15</span><br><span class=\"line\">$ docker pull tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"3步快速部署\">3步快速部署</h2>\n<h3 id=\"步骤一：部署carla-0-9-15仿真器\">步骤一：部署carla-0.9.15仿真器</h3>\n<p>这个步骤时基于carla官方发布的docker镜像，你需要拉取这个镜像然后在你的主机上运行起来。必须检查下rpc-port端口是否被占用。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull carlasim/carla:0.9.15</span><br><span class=\"line\">$ docker run --privileged --gpus all --net=host -e DISPLAY=<span class=\"variable\">$DISPLAY</span> carlasim/carla:0.9.15 /bin/bash ./CarlaUE4.sh -carla-rpc-port=1403</span><br></pre></td></tr></table></figure>\n<h3 id=\"步骤二：部署autoware-carla-bridge\">步骤二：部署autoware-carla-bridge</h3>\n<p>这一步主要是基于TUM团队发布的镜像，在我的笔记本上需要设置timeout为一个更大的值，比如10000才可以，默认值是5000ms。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ docker run -it -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --network host tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ ros2 launch carla_autoware_bridge carla_aw_bridge.launch.py port:=1403 town:=Town10HD <span class=\"built_in\">timeout</span>:=10000</span><br></pre></td></tr></table></figure>\n<h3 id=\"步骤三：部署autowarea-unvierse技术栈到Jetson-Orin上\">步骤三：部署autowarea.unvierse技术栈到Jetson Orin上</h3>\n<p>编译autoware.universe有时候很困难，因为有很多不同的依赖项。所以，我建议使用我发布的预编译的docker镜像来的更简单。下面的命令必须在Jetson Orin上运行。主要是因为我们想使用一个Jetson Orin设备来部署autoware.universe技术栈。</p>\n<p>***注意：***这个镜像有点大，并且只能在ARM64的平台上使用，所以Jetson系列的开发套件都能适用。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br><span class=\"line\">$ sudo apt-get install python3-rocker</span><br><span class=\"line\">$ rocker --user --nvidia --privileged --network host --x11 --volume <span class=\"variable\">$HOME</span>/Documents  --volume <span class=\"variable\">$HOME</span>/autoware -- 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br><span class=\"line\">$ ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=carla_t2_vehicle sensor_model:=carla_t2_sensor_kit map_path:=/autoware1.0_ws/Town10/</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><em><strong>注意</strong></em>：当你使用rocker命令的时候，可能会出现如下消息：<em><strong>invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported</strong></em>.</p>\n<p>你只需要更改&quot;–gpus all&quot; 成 “–runtime=nvidia”，如下面的样例指令，你的指令将会和我的不一样，你应该在运行 *<strong>rocker --user --nvidia xxxx</strong> 命令后，粘贴终端反馈的指令信息来运行镜像。</p>\n</blockquote>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run  --<span class=\"built_in\">rm</span> -it --network host   --runtime=nvidia --privileged  -e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   -e XAUTHORITY=/tmp/.dockerafc7hfmf.xauth -v /tmp/.dockerafc7hfmf.xauth:/tmp/.dockerafc7hfmf.xauth   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /etc/localtime:/etc/localtime:ro  8ea8cd5cadfe</span><br></pre></td></tr></table></figure>\n<h3 id=\"可选项\">可选项</h3>\n<h4 id=\"生成NPC\">生成NPC</h4>\n<p>如果你想添加一些NPC到你的carla仿真器中，你需要使用一个在autoware-carla-bridge docker容器中的Python脚本来启动。同时，注意要和之前一样使用相同的端口。这个<strong>generate_traffic.py</strong>应该步骤二启动的docker容器中的终端中运行。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ python3 src/carla_autoware_bridge/utils/generate_traffic.py -p 1403</span><br></pre></td></tr></table></figure>\n<h4 id=\"部署autoware-universe技术栈在主机上\">部署autoware.universe技术栈在主机上</h4>\n<p>在你的主机本地终端上编译autoware的源码，如果你不想部署在jetson嵌入式平台的话。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br></pre></td></tr></table></figure>\n<p>如果你想在有GPU显卡的主机上编译源码，你可能需要解决一些依赖问题。</p>\n<h2 id=\"本地部署工作流\">本地部署工作流</h2>\n<p>本地部署工作流将会后来添加，添加一个Dockerfile，发布修改之后的源码。</p>\n<h2 id=\"TODO-List\">TODO List</h2>\n<ul>\n<li>[ ] 添加另一个shuttle-bus的车辆模型到carla仿真器中</li>\n<li>[ ] 简化Dockerfile给用户进行本地Autoware.universe镜像的编译</li>\n<li>[ ] 上传修改的autoware.universe代码，解决感知模块无法启动的一些问题</li>\n</ul>\n<h2 id=\"解决Bugs\">解决Bugs</h2>\n<p>docker运行时的错误，错误信息可能和如下的相似：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker: Error response from daemon: failed to create task <span class=\"keyword\">for</span> container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook <span class=\"comment\">#0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as &#x27;csv&#x27;</span></span><br><span class=\"line\">invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported.Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.: unknown.</span><br></pre></td></tr></table></figure>\n<p>你必须使用&quot;–runtime=nvidia &quot;标志莱欧启动一个容器。</p>\n<h2 id=\"引用\">引用</h2>\n<p>TUM论文：<a href=\"https://arxiv.org/abs/2402.11239\">https://arxiv.org/abs/2402.11239</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>如何3步实现在Jetson Orin上联调Autoware.Universe和Carla-0.9.15</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h2 id=\"准备工作\">准备工作</h2>\n<h3 id=\"软件\">软件</h3>\n<ol>\n<li>Carla 0.9.15 (the newest version)</li>\n<li>Autoware.universe (V1.0 Branch, will be changed to other version)</li>\n<li>Autoware-Carla-Bridge (Main Branch)</li>\n<li>ROS2 (Humble version)</li>\n<li>Ubuntu 22.04 (LTS Released version)</li>\n<li>Docker</li>\n</ol>\n<h3 id=\"硬件\">硬件</h3>\n<p>Jetson Orin开发者套件在机器人和自动驾驶领域都是非常好的一个开发套件</p>\n<ol>\n<li>一个全千兆的交换机或者路由器（局域网内主机和Jetson Orin通信需要）</li>\n<li>一个Jetson Orin开发套件 32G/64G，带外置NVME的500G的SSD硬盘</li>\n<li>一个Ubuntu22.04的主机</li>\n<li>两个或以上的超五类或者六类网线</li>\n</ol>\n<p>所有的设备必须在局域网内相互连接能够通信，全千兆的规格会更好。</p>\n<hr>\n<p><em><strong>Note:</strong></em> 为了说明方便，我使用“主机”来表示带有GPU显卡(2070或者30/40系列的显卡均可)的机器。这个主机将会用来部署Carla-0.9.15仿真模拟器和Autoware-Carla-Bridge ROS2所有包。同时，使用Jetson Orin来表示嵌入式系统，这个嵌入式开发套件将会用来部署Autoware.Universe的技术栈软件。</p>\n<hr>\n<blockquote>\n<p>在你开始下面的步骤之前，我建议你使用下面的指令先拉取一些必要的docker镜像，因此在你阅读完这个教程之后，立马可以开始部署:</p>\n</blockquote>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull carlasim/carla:0.9.15</span><br><span class=\"line\">$ docker pull tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"3步快速部署\">3步快速部署</h2>\n<h3 id=\"步骤一：部署carla-0-9-15仿真器\">步骤一：部署carla-0.9.15仿真器</h3>\n<p>这个步骤时基于carla官方发布的docker镜像，你需要拉取这个镜像然后在你的主机上运行起来。必须检查下rpc-port端口是否被占用。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull carlasim/carla:0.9.15</span><br><span class=\"line\">$ docker run --privileged --gpus all --net=host -e DISPLAY=<span class=\"variable\">$DISPLAY</span> carlasim/carla:0.9.15 /bin/bash ./CarlaUE4.sh -carla-rpc-port=1403</span><br></pre></td></tr></table></figure>\n<h3 id=\"步骤二：部署autoware-carla-bridge\">步骤二：部署autoware-carla-bridge</h3>\n<p>这一步主要是基于TUM团队发布的镜像，在我的笔记本上需要设置timeout为一个更大的值，比如10000才可以，默认值是5000ms。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ docker run -it -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --network host tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ ros2 launch carla_autoware_bridge carla_aw_bridge.launch.py port:=1403 town:=Town10HD <span class=\"built_in\">timeout</span>:=10000</span><br></pre></td></tr></table></figure>\n<h3 id=\"步骤三：部署autowarea-unvierse技术栈到Jetson-Orin上\">步骤三：部署autowarea.unvierse技术栈到Jetson Orin上</h3>\n<p>编译autoware.universe有时候很困难，因为有很多不同的依赖项。所以，我建议使用我发布的预编译的docker镜像来的更简单。下面的命令必须在Jetson Orin上运行。主要是因为我们想使用一个Jetson Orin设备来部署autoware.universe技术栈。</p>\n<p>***注意：***这个镜像有点大，并且只能在ARM64的平台上使用，所以Jetson系列的开发套件都能适用。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br><span class=\"line\">$ sudo apt-get install python3-rocker</span><br><span class=\"line\">$ rocker --user --nvidia --privileged --network host --x11 --volume <span class=\"variable\">$HOME</span>/Documents  --volume <span class=\"variable\">$HOME</span>/autoware -- 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br><span class=\"line\">$ ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=carla_t2_vehicle sensor_model:=carla_t2_sensor_kit map_path:=/autoware1.0_ws/Town10/</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><em><strong>注意</strong></em>：当你使用rocker命令的时候，可能会出现如下消息：<em><strong>invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported</strong></em>.</p>\n<p>你只需要更改&quot;–gpus all&quot; 成 “–runtime=nvidia”，如下面的样例指令，你的指令将会和我的不一样，你应该在运行 *<strong>rocker --user --nvidia xxxx</strong> 命令后，粘贴终端反馈的指令信息来运行镜像。</p>\n</blockquote>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run  --<span class=\"built_in\">rm</span> -it --network host   --runtime=nvidia --privileged  -e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   -e XAUTHORITY=/tmp/.dockerafc7hfmf.xauth -v /tmp/.dockerafc7hfmf.xauth:/tmp/.dockerafc7hfmf.xauth   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /etc/localtime:/etc/localtime:ro  8ea8cd5cadfe</span><br></pre></td></tr></table></figure>\n<h3 id=\"可选项\">可选项</h3>\n<h4 id=\"生成NPC\">生成NPC</h4>\n<p>如果你想添加一些NPC到你的carla仿真器中，你需要使用一个在autoware-carla-bridge docker容器中的Python脚本来启动。同时，注意要和之前一样使用相同的端口。这个<strong>generate_traffic.py</strong>应该步骤二启动的docker容器中的终端中运行。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ python3 src/carla_autoware_bridge/utils/generate_traffic.py -p 1403</span><br></pre></td></tr></table></figure>\n<h4 id=\"部署autoware-universe技术栈在主机上\">部署autoware.universe技术栈在主机上</h4>\n<p>在你的主机本地终端上编译autoware的源码，如果你不想部署在jetson嵌入式平台的话。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br></pre></td></tr></table></figure>\n<p>如果你想在有GPU显卡的主机上编译源码，你可能需要解决一些依赖问题。</p>\n<h2 id=\"本地部署工作流\">本地部署工作流</h2>\n<p>本地部署工作流将会后来添加，添加一个Dockerfile，发布修改之后的源码。</p>\n<h2 id=\"TODO-List\">TODO List</h2>\n<ul>\n<li>[ ] 添加另一个shuttle-bus的车辆模型到carla仿真器中</li>\n<li>[ ] 简化Dockerfile给用户进行本地Autoware.universe镜像的编译</li>\n<li>[ ] 上传修改的autoware.universe代码，解决感知模块无法启动的一些问题</li>\n</ul>\n<h2 id=\"解决Bugs\">解决Bugs</h2>\n<p>docker运行时的错误，错误信息可能和如下的相似：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker: Error response from daemon: failed to create task <span class=\"keyword\">for</span> container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook <span class=\"comment\">#0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as &#x27;csv&#x27;</span></span><br><span class=\"line\">invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported.Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.: unknown.</span><br></pre></td></tr></table></figure>\n<p>你必须使用&quot;–runtime=nvidia &quot;标志莱欧启动一个容器。</p>\n<h2 id=\"引用\">引用</h2>\n<p>TUM论文：<a href=\"https://arxiv.org/abs/2402.11239\">https://arxiv.org/abs/2402.11239</a></p>\n"},{"title":"Autoware.Universe环境搭建","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-06-08T08:17:05.000Z","updated":"2023-06-08T08:17:05.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/robotics-arm.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***Autoware.Universe环境搭建***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n# 1. Docker镜像安装\n\n两种镜像：**预编译（prebuilt）**和**开发版(devlopment)**\n\n**预编译**中的版本，可以适合小白迅速上手测试，无需关心代码的编译，驱动依赖等问题，镜像内部已经有编译好的代码及所有的依赖项环境。\n\n**开发版**，适合开发者部署自己的代码，进行开发工作，需要关心编译问题，以及其他的依赖项可能缺失的问题，镜像内部没有编译好的代码，好处就是，镜像相对较小，但是代码需要自己编译和处理可能有的bugs。\n\n\n\n## 拉取源码\n\n```text\ngit clone https://github.com/autowarefoundation/autoware.git\ncd autoware\n```\n\n## 创建工作空间\n\n需要创建一个autoware_map的文件夹来保存后面将会使用到的地图。这样我们就有了一个工作空间，其中包含了autoware的源码和autoware_map的地图信息。\n\n```text\nmkdir ~/autoware_map\n```\n\n## 拉取镜像\n\n```text\ndocker pull ghcr.io/autowarefoundation/autoware-universe:latest-cuda\n```\n\n根据自己的情况选择合适的镜像，这里默认的latest-cuda镜像是基于ubuntu22上构建的支持amd64硬件的镜像。\n\n## 启动容器\n\n```text\nrocker --nvidia --x11 --user --volume $HOME/autoware --volume $HOME/autoware_map -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda\n```\n\nrocker的具体使用方法可以参考[__这里__](https://github.com/autowarefoundation/autoware/tree/main/docker/README.md)\n\n如果是使用arm64的架构平台需要使用不同的启动方式：\n\n```text\nrocker -e LIBGL_ALWAYS_SOFTWARE=1 --x11 --user --volume $HOME/autoware -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda\n```\n\n## docker的启动指令\n\n```text\ndocker run --rm -it  --gpus all \n-v /home/userroot/autoware:/home/userroot/autoware\n-v /home/userroot/autoware_map:/home/userroot/autoware_map  \n-e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   \n-e XAUTHORITY=/tmp/.dockercnqq0upg.xauth \n-v /tmp/.dockercnqq0upg.xauth:/tmp/.dockercnqq0upg.xauth   \n-v /tmp/.X11-unix:/tmp/.X11-unix   \n-v /etc/localtime:/etc/localtime:ro  b3ea2527886b\n```\n\n启动之后，我们就创建了一个容器并进入到容器中的终端进行交互。\n\n## 拉取源码并安装依赖\n\n```text\ncd autoware\nmkdir src\nvcs import src < autoware.repos\nsudo apt update\nrosdep update\nrosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO\n```\n\n## 编译工作空间\n\n```text\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"-w\"\n```\n\n我这里编译报错的信息\n\n```text\n11 packages had stderr output: behavior_path_planner elevation_map_loader lidar_apollo_segmentation_tvm lidar_apollo_segmentation_tvm_nodes lidar_centerpoint_tvm map_tf_generator obstacle_velocity_limiter pacmod_interface system_monitor tensorrt_yolox velodyne_pointcloud\n```\n\n编译完成就可以启动autoware\n\n## 启动autoware\n\n```text\nsource install/setup.bash\nros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=<your mapfile location>\n```\n\n需要关联仿真环境才能够完成启动上面的指令，需要先启动仿真环境。\n\n## 启动仿真\n\n```text\nros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=/home/userroot/autoware_map/nishishinjuku_autoware_map\n```\n\n参考AWSIM的部分\n\n[Ubuntu18安装AWSIM运行autoware.universe](https://www.bluenote.top/2023/06/08/008-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/02-Autoware.universe/Ubuntu18%E5%AE%89%E8%A3%85AWSIM%E8%BF%90%E8%A1%8CAutoware-Universe/)\n\n\n\n\n\n\n\n# 2. 源码编译安装\n\n挖坑代填。。\n\n\n\n\n\n\n\n\n\n# 参考引用\n\nDocker镜像仓库\n\n[Build software better, together](https://github.com/autowarefoundation/autoware/pkgs/container/autoware-universe)\n\nrocker使用介绍\n\n[GitHub - osrf/rocker: A tool to run docker containers with overlays and convenient options for things like GUIs etc.](https://github.com/osrf/rocker)\n","source":"_posts/008-AutonomousDriving/02-Autoware.universe/Autoware-Universe环境搭建.md","raw":"---\ntitle: Autoware.Universe环境搭建\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-06-08 16:17:05\nupdated: 2023-06-08 16:17:05\ntags: [Autoware.Universe, Docker]\ncategories:\n- 自动驾驶\n- Autoware.Universe\nkeywords:\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***Autoware.Universe环境搭建***\n\n\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n# 1. Docker镜像安装\n\n两种镜像：**预编译（prebuilt）**和**开发版(devlopment)**\n\n**预编译**中的版本，可以适合小白迅速上手测试，无需关心代码的编译，驱动依赖等问题，镜像内部已经有编译好的代码及所有的依赖项环境。\n\n**开发版**，适合开发者部署自己的代码，进行开发工作，需要关心编译问题，以及其他的依赖项可能缺失的问题，镜像内部没有编译好的代码，好处就是，镜像相对较小，但是代码需要自己编译和处理可能有的bugs。\n\n\n\n## 拉取源码\n\n```text\ngit clone https://github.com/autowarefoundation/autoware.git\ncd autoware\n```\n\n## 创建工作空间\n\n需要创建一个autoware_map的文件夹来保存后面将会使用到的地图。这样我们就有了一个工作空间，其中包含了autoware的源码和autoware_map的地图信息。\n\n```text\nmkdir ~/autoware_map\n```\n\n## 拉取镜像\n\n```text\ndocker pull ghcr.io/autowarefoundation/autoware-universe:latest-cuda\n```\n\n根据自己的情况选择合适的镜像，这里默认的latest-cuda镜像是基于ubuntu22上构建的支持amd64硬件的镜像。\n\n## 启动容器\n\n```text\nrocker --nvidia --x11 --user --volume $HOME/autoware --volume $HOME/autoware_map -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda\n```\n\nrocker的具体使用方法可以参考[__这里__](https://github.com/autowarefoundation/autoware/tree/main/docker/README.md)\n\n如果是使用arm64的架构平台需要使用不同的启动方式：\n\n```text\nrocker -e LIBGL_ALWAYS_SOFTWARE=1 --x11 --user --volume $HOME/autoware -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda\n```\n\n## docker的启动指令\n\n```text\ndocker run --rm -it  --gpus all \n-v /home/userroot/autoware:/home/userroot/autoware\n-v /home/userroot/autoware_map:/home/userroot/autoware_map  \n-e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   \n-e XAUTHORITY=/tmp/.dockercnqq0upg.xauth \n-v /tmp/.dockercnqq0upg.xauth:/tmp/.dockercnqq0upg.xauth   \n-v /tmp/.X11-unix:/tmp/.X11-unix   \n-v /etc/localtime:/etc/localtime:ro  b3ea2527886b\n```\n\n启动之后，我们就创建了一个容器并进入到容器中的终端进行交互。\n\n## 拉取源码并安装依赖\n\n```text\ncd autoware\nmkdir src\nvcs import src < autoware.repos\nsudo apt update\nrosdep update\nrosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO\n```\n\n## 编译工作空间\n\n```text\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"-w\"\n```\n\n我这里编译报错的信息\n\n```text\n11 packages had stderr output: behavior_path_planner elevation_map_loader lidar_apollo_segmentation_tvm lidar_apollo_segmentation_tvm_nodes lidar_centerpoint_tvm map_tf_generator obstacle_velocity_limiter pacmod_interface system_monitor tensorrt_yolox velodyne_pointcloud\n```\n\n编译完成就可以启动autoware\n\n## 启动autoware\n\n```text\nsource install/setup.bash\nros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=<your mapfile location>\n```\n\n需要关联仿真环境才能够完成启动上面的指令，需要先启动仿真环境。\n\n## 启动仿真\n\n```text\nros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=/home/userroot/autoware_map/nishishinjuku_autoware_map\n```\n\n参考AWSIM的部分\n\n[Ubuntu18安装AWSIM运行autoware.universe](https://www.bluenote.top/2023/06/08/008-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/02-Autoware.universe/Ubuntu18%E5%AE%89%E8%A3%85AWSIM%E8%BF%90%E8%A1%8CAutoware-Universe/)\n\n\n\n\n\n\n\n# 2. 源码编译安装\n\n挖坑代填。。\n\n\n\n\n\n\n\n\n\n# 参考引用\n\nDocker镜像仓库\n\n[Build software better, together](https://github.com/autowarefoundation/autoware/pkgs/container/autoware-universe)\n\nrocker使用介绍\n\n[GitHub - osrf/rocker: A tool to run docker containers with overlays and convenient options for things like GUIs etc.](https://github.com/osrf/rocker)\n","slug":"008-AutonomousDriving/02-Autoware.universe/Autoware-Universe环境搭建","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t5001fu9rl7pbre1kt","content":"<h1><em><strong>Autoware.Universe环境搭建</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>1. Docker镜像安装</h1>\n<p>两种镜像：<strong>预编译（prebuilt）<strong>和</strong>开发版(devlopment)</strong></p>\n<p><strong>预编译</strong>中的版本，可以适合小白迅速上手测试，无需关心代码的编译，驱动依赖等问题，镜像内部已经有编译好的代码及所有的依赖项环境。</p>\n<p><strong>开发版</strong>，适合开发者部署自己的代码，进行开发工作，需要关心编译问题，以及其他的依赖项可能缺失的问题，镜像内部没有编译好的代码，好处就是，镜像相对较小，但是代码需要自己编译和处理可能有的bugs。</p>\n<h2 id=\"拉取源码\">拉取源码</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/autowarefoundation/autoware.git</span><br><span class=\"line\">cd autoware</span><br></pre></td></tr></table></figure>\n<h2 id=\"创建工作空间\">创建工作空间</h2>\n<p>需要创建一个autoware_map的文件夹来保存后面将会使用到的地图。这样我们就有了一个工作空间，其中包含了autoware的源码和autoware_map的地图信息。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/autoware_map</span><br></pre></td></tr></table></figure>\n<h2 id=\"拉取镜像\">拉取镜像</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br></pre></td></tr></table></figure>\n<p>根据自己的情况选择合适的镜像，这里默认的latest-cuda镜像是基于ubuntu22上构建的支持amd64硬件的镜像。</p>\n<h2 id=\"启动容器\">启动容器</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rocker --nvidia --x11 --user --volume $HOME/autoware --volume $HOME/autoware_map -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br></pre></td></tr></table></figure>\n<p>rocker的具体使用方法可以参考<a href=\"https://github.com/autowarefoundation/autoware/tree/main/docker/README.md\"><strong>这里</strong></a></p>\n<p>如果是使用arm64的架构平台需要使用不同的启动方式：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rocker -e LIBGL_ALWAYS_SOFTWARE=1 --x11 --user --volume $HOME/autoware -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br></pre></td></tr></table></figure>\n<h2 id=\"docker的启动指令\">docker的启动指令</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --rm -it  --gpus all </span><br><span class=\"line\">-v /home/userroot/autoware:/home/userroot/autoware</span><br><span class=\"line\">-v /home/userroot/autoware_map:/home/userroot/autoware_map  </span><br><span class=\"line\">-e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   </span><br><span class=\"line\">-e XAUTHORITY=/tmp/.dockercnqq0upg.xauth </span><br><span class=\"line\">-v /tmp/.dockercnqq0upg.xauth:/tmp/.dockercnqq0upg.xauth   </span><br><span class=\"line\">-v /tmp/.X11-unix:/tmp/.X11-unix   </span><br><span class=\"line\">-v /etc/localtime:/etc/localtime:ro  b3ea2527886b</span><br></pre></td></tr></table></figure>\n<p>启动之后，我们就创建了一个容器并进入到容器中的终端进行交互。</p>\n<h2 id=\"拉取源码并安装依赖\">拉取源码并安装依赖</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd autoware</span><br><span class=\"line\">mkdir src</span><br><span class=\"line\">vcs import src &lt; autoware.repos</span><br><span class=\"line\">sudo apt update</span><br><span class=\"line\">rosdep update</span><br><span class=\"line\">rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO</span><br></pre></td></tr></table></figure>\n<h2 id=\"编译工作空间\">编译工作空间</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=&quot;-w&quot;</span><br></pre></td></tr></table></figure>\n<p>我这里编译报错的信息</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">11 packages had stderr output: behavior_path_planner elevation_map_loader lidar_apollo_segmentation_tvm lidar_apollo_segmentation_tvm_nodes lidar_centerpoint_tvm map_tf_generator obstacle_velocity_limiter pacmod_interface system_monitor tensorrt_yolox velodyne_pointcloud</span><br></pre></td></tr></table></figure>\n<p>编译完成就可以启动autoware</p>\n<h2 id=\"启动autoware\">启动autoware</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=&lt;your mapfile location&gt;</span><br></pre></td></tr></table></figure>\n<p>需要关联仿真环境才能够完成启动上面的指令，需要先启动仿真环境。</p>\n<h2 id=\"启动仿真\">启动仿真</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=/home/userroot/autoware_map/nishishinjuku_autoware_map</span><br></pre></td></tr></table></figure>\n<p>参考AWSIM的部分</p>\n<p><a href=\"https://www.bluenote.top/2023/06/08/008-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/02-Autoware.universe/Ubuntu18%E5%AE%89%E8%A3%85AWSIM%E8%BF%90%E8%A1%8CAutoware-Universe/\">Ubuntu18安装AWSIM运行autoware.universe</a></p>\n<h1>2. 源码编译安装</h1>\n<p>挖坑代填。。</p>\n<h1>参考引用</h1>\n<p>Docker镜像仓库</p>\n<p><a href=\"https://github.com/autowarefoundation/autoware/pkgs/container/autoware-universe\">Build software better, together</a></p>\n<p>rocker使用介绍</p>\n<p><a href=\"https://github.com/osrf/rocker\">GitHub - osrf/rocker: A tool to run docker containers with overlays and convenient options for things like GUIs etc.</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>Autoware.Universe环境搭建</strong></em></h1>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>1. Docker镜像安装</h1>\n<p>两种镜像：<strong>预编译（prebuilt）<strong>和</strong>开发版(devlopment)</strong></p>\n<p><strong>预编译</strong>中的版本，可以适合小白迅速上手测试，无需关心代码的编译，驱动依赖等问题，镜像内部已经有编译好的代码及所有的依赖项环境。</p>\n<p><strong>开发版</strong>，适合开发者部署自己的代码，进行开发工作，需要关心编译问题，以及其他的依赖项可能缺失的问题，镜像内部没有编译好的代码，好处就是，镜像相对较小，但是代码需要自己编译和处理可能有的bugs。</p>\n<h2 id=\"拉取源码\">拉取源码</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/autowarefoundation/autoware.git</span><br><span class=\"line\">cd autoware</span><br></pre></td></tr></table></figure>\n<h2 id=\"创建工作空间\">创建工作空间</h2>\n<p>需要创建一个autoware_map的文件夹来保存后面将会使用到的地图。这样我们就有了一个工作空间，其中包含了autoware的源码和autoware_map的地图信息。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/autoware_map</span><br></pre></td></tr></table></figure>\n<h2 id=\"拉取镜像\">拉取镜像</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br></pre></td></tr></table></figure>\n<p>根据自己的情况选择合适的镜像，这里默认的latest-cuda镜像是基于ubuntu22上构建的支持amd64硬件的镜像。</p>\n<h2 id=\"启动容器\">启动容器</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rocker --nvidia --x11 --user --volume $HOME/autoware --volume $HOME/autoware_map -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br></pre></td></tr></table></figure>\n<p>rocker的具体使用方法可以参考<a href=\"https://github.com/autowarefoundation/autoware/tree/main/docker/README.md\"><strong>这里</strong></a></p>\n<p>如果是使用arm64的架构平台需要使用不同的启动方式：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rocker -e LIBGL_ALWAYS_SOFTWARE=1 --x11 --user --volume $HOME/autoware -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br></pre></td></tr></table></figure>\n<h2 id=\"docker的启动指令\">docker的启动指令</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --rm -it  --gpus all </span><br><span class=\"line\">-v /home/userroot/autoware:/home/userroot/autoware</span><br><span class=\"line\">-v /home/userroot/autoware_map:/home/userroot/autoware_map  </span><br><span class=\"line\">-e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   </span><br><span class=\"line\">-e XAUTHORITY=/tmp/.dockercnqq0upg.xauth </span><br><span class=\"line\">-v /tmp/.dockercnqq0upg.xauth:/tmp/.dockercnqq0upg.xauth   </span><br><span class=\"line\">-v /tmp/.X11-unix:/tmp/.X11-unix   </span><br><span class=\"line\">-v /etc/localtime:/etc/localtime:ro  b3ea2527886b</span><br></pre></td></tr></table></figure>\n<p>启动之后，我们就创建了一个容器并进入到容器中的终端进行交互。</p>\n<h2 id=\"拉取源码并安装依赖\">拉取源码并安装依赖</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd autoware</span><br><span class=\"line\">mkdir src</span><br><span class=\"line\">vcs import src &lt; autoware.repos</span><br><span class=\"line\">sudo apt update</span><br><span class=\"line\">rosdep update</span><br><span class=\"line\">rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO</span><br></pre></td></tr></table></figure>\n<h2 id=\"编译工作空间\">编译工作空间</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=&quot;-w&quot;</span><br></pre></td></tr></table></figure>\n<p>我这里编译报错的信息</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">11 packages had stderr output: behavior_path_planner elevation_map_loader lidar_apollo_segmentation_tvm lidar_apollo_segmentation_tvm_nodes lidar_centerpoint_tvm map_tf_generator obstacle_velocity_limiter pacmod_interface system_monitor tensorrt_yolox velodyne_pointcloud</span><br></pre></td></tr></table></figure>\n<p>编译完成就可以启动autoware</p>\n<h2 id=\"启动autoware\">启动autoware</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=&lt;your mapfile location&gt;</span><br></pre></td></tr></table></figure>\n<p>需要关联仿真环境才能够完成启动上面的指令，需要先启动仿真环境。</p>\n<h2 id=\"启动仿真\">启动仿真</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=/home/userroot/autoware_map/nishishinjuku_autoware_map</span><br></pre></td></tr></table></figure>\n<p>参考AWSIM的部分</p>\n<p><a href=\"https://www.bluenote.top/2023/06/08/008-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/02-Autoware.universe/Ubuntu18%E5%AE%89%E8%A3%85AWSIM%E8%BF%90%E8%A1%8CAutoware-Universe/\">Ubuntu18安装AWSIM运行autoware.universe</a></p>\n<h1>2. 源码编译安装</h1>\n<p>挖坑代填。。</p>\n<h1>参考引用</h1>\n<p>Docker镜像仓库</p>\n<p><a href=\"https://github.com/autowarefoundation/autoware/pkgs/container/autoware-universe\">Build software better, together</a></p>\n<p>rocker使用介绍</p>\n<p><a href=\"https://github.com/osrf/rocker\">GitHub - osrf/rocker: A tool to run docker containers with overlays and convenient options for things like GUIs etc.</a></p>\n"},{"title":"How to Joint testing between Autoware-Universe and Carla-0-9-15 on Jetson Orin?","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2024-03-12T12:08:57.000Z","updated":"2024-03-12T12:08:57.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/gkihqEjXxJ5UZ1C.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# 3 Steps to Deploy Autoware.universe with Using Carla_0.9.15 on Jetson Orin\n\nThis repository is based on the github repository from TUM. [**LINK**](https://github.com/TUMFTM/Carla-Autoware-Bridge.git) \n\nThanks a lot for their brilliant work, if you are interested in their paper, you can refer to this link [[PDF](https://arxiv.org/abs/2402.11239)]\n\nAutoware.universe source codes can be found from their github repository. [**LINK**](https://github.com/autowarefoundation/autoware/tree/v1.0)\n\nIn this repository, we did some revisions to use **REV-shuttle-bus** in the **Carla simulator**, which has different sensors, also the **autoware.universe** version we used will be changed to our version later.\n\n## **Requirements**\n\n### Software\n\n1. Carla 0.9.15 (the newest version)\n2. Autoware.universe (V1.0 Branch, will be changed to other version)\n3. Autoware-Carla-Bridge (Main Branch)\n4. ROS2 (Humble version)\n5. Ubuntu 22.04 (LTS Released version)\n6. Docker\n\n### Hardware\n\nJetson Orin Developer kit is an awesome platform for robots and Autonomous Vehicle.\n\n1. One all 1 Gigabyte Switcher or Router (for communication between your host Ubuntu machine with Jetson Orin).\n\n2. One Jetson Orin Develop kit 32GB/64GB with nvme ssd 500G external storage.\n3. One Ubuntu 22.04 Host Machine.\n4. Two or more Cat 5e/6 or above level cables \n\nAll devices are required to connect with each other in a local network. All 1 Gigabyte specs are better. \n\n------\n\n***Note:*** To illustrate the following steps easier, I use \"***Host Machine***\" stands for the machine which has a standard **GPU** device like **RTX2070** or 30/40 series and it 's also the one which will deploy a **Carla 0.9.15** simulator and **Autoware-Carla-Bridge ROS2** packages, and use \"***Jetson Orin***\" stands for the embedded system which will be used for deploying the **Autoware.universe** stack.\n\n------\n\n> Before you go through the following steps, I suggest you pull some necessary docker images using following commands, so after you finish reading this document, you can start deploying right away:\n\n```bash\n $ docker pull carlasim/carla:0.9.15\n $ docker pull tumgeka/carla-autoware-bridge:latest\n $ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n```\n\n\n\n## 3 Steps to deploy\n\n### STEP-1: Deploy the carla-0.9.15 simulator\n\nThis step is based on the carla official docker image, you have to pull the image and then run it on your Host machine.\n\nYou have to check the rpc-port is not used on your Host machine.\n\n```bash\n$ docker pull carlasim/carla:0.9.15\n$ docker run --privileged --gpus all --net=host -e DISPLAY=$DISPLAY carlasim/carla:0.9.15 /bin/bash ./CarlaUE4.sh -carla-rpc-port=1403\n```\n\n### STEP-2: Deploy the autoware-carla-bridge\n\nThis step is also based on the docker image issued by TUM team. Setting the timeout to a larger volume is necessary on my own laptop, the default value is 5000. \n\n```bash\n$ docker pull tumgeka/carla-autoware-bridge:latest\n$ docker run -it -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --network host tumgeka/carla-autoware-bridge:latest\n$ ros2 launch carla_autoware_bridge carla_aw_bridge.launch.py port:=1403 town:=Town10HD timeout:=10000\n```\n\n### STEP-3: Deploy autoware.universe stack on Jetson Orin\n\nCompile the autoware.universe is sometimes difficult, as there are various dependencies. So, I suggest that use our pre-built docker images is easier. These following commands need to run on the Jetson Orin terminals. As we want to use a Jetson Orin device to deploy the autoware.universe stack. \n\n***Note**:* *This docker image is a little bit large and only supported on the ARM64 platform, so Jetson series are all suitable.*\n\n```bash\n$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n$ sudo apt-get install python3-rocker\n$ rocker --user --nvidia --privileged --network host --x11 --volume $HOME/Documents  --volume $HOME/autoware -- 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n$ ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=carla_t2_vehicle sensor_model:=carla_t2_sensor_kit map_path:=/autoware1.0_ws/Town10/\n```\n\n> ***Note:*** When you run the rocker commands, and it may shows the error like the following messages: ***invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported***.\n>\n> You merely need to change the \"--gpus all\" to \"--runtime=nvidia\". like the example commands below: \"Your commands will be different with mine, you should copy the commands, after you run the ***rocker --user --nvidia xxxx** command*\"\n\n```bash\n$ docker run  --rm -it --network host   --runtime=nvidia --privileged  -e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   -e XAUTHORITY=/tmp/.dockerafc7hfmf.xauth -v /tmp/.dockerafc7hfmf.xauth:/tmp/.dockerafc7hfmf.xauth   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /etc/localtime:/etc/localtime:ro  8ea8cd5cadfe\n```\n\n\n\n### Optional\n\n#### Spawn NPCs\n\nIf you want to add some NPCs in the carla you can use the python script in the autoware-carla-brdige docker container. Must set the port to the same with the front steps. This ***generate_traffic.py*** should be used in the docker container's terminal of the STEP-2(tumgeka/carla-autoware-bridge:latest).\n\n```bash\n$ python3 src/carla_autoware_bridge/utils/generate_traffic.py -p 1403\n```\n\n#### Deploy autoware.universe stack on Host Machine\n\nCompile the autoware source codes on your host machine locally, if you do not want to deploy on the jetson embedded systems.\n\n```bash\n$ colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n```\n\nIf you want to compile the source codes on the Host Machine with a GPU card, you may need to solve some dependencies problems.\n\n\n\n\n\n\n\n## Local workflow\n\nLocal workflow will be added later, add a Dockerfile, released and revised source codes. \n\n\n\nTODO List\n\n- [ ] Add another shuttle-bus model in carla simulator to use\n- [ ] Simplified a Dockerfile for users to build the Autoware.universe docker image locally.\n- [ ] Uploads the revised autoware.universe codes for solving the problems of starting the perception module in the autoware.universe stack\n\n\n\n## Bugs Shooting\n\ndocker run error, the error info is like the followings:\n\n```bash\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'csv'\ninvoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported.Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.: unknown.\n```\n\nYou have to use the \"--runtime=nvidia \" flag to run using docker run commands.\n\n\n\n\n\n## Reference\n\n```\n@inproceedings{carla_aw_bridge24,\n\n  title = {CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research\n\nwith a Unified Framework for Simulation and Module Development,\n\n  author = {Kaljavesi, Gemb and Kerbl, Tobias and Betz, Tobias and Mitkovskii, Kirill and Diermeyer, Frank},\n\n  year = {2024}\n\n}\n```\n\nBlog link: https://www.bluenote.top/2024/03/12/008-AutonomousDriving/02-Autoware.universe/%E5%A6%82%E4%BD%953%E6%AD%A5%E5%AE%9E%E7%8E%B0%E5%9C%A8Jetson-Orin%E4%B8%8A%E8%81%94%E8%B0%83Autoware-Universe%E5%92%8CCarla-0-9-15%EF%BC%9F/index.html\n\n\n\n","source":"_posts/008-AutonomousDriving/02-Autoware.universe/How to Joint testing between Autoware-Universe and Carla-0-9-15 on Jetson Orin?.md","raw":"---\ntitle: How to Joint testing between Autoware-Universe and Carla-0-9-15 on Jetson Orin?\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2024-03-12 20:08:57\nupdated: 2024-03-12 20:08:57\ntags: [Autoware.Universe, Ubuntu22, Carla]\ncategories:\n- 自动驾驶\n- Autoware.Universe\nkeywords:\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# 3 Steps to Deploy Autoware.universe with Using Carla_0.9.15 on Jetson Orin\n\nThis repository is based on the github repository from TUM. [**LINK**](https://github.com/TUMFTM/Carla-Autoware-Bridge.git) \n\nThanks a lot for their brilliant work, if you are interested in their paper, you can refer to this link [[PDF](https://arxiv.org/abs/2402.11239)]\n\nAutoware.universe source codes can be found from their github repository. [**LINK**](https://github.com/autowarefoundation/autoware/tree/v1.0)\n\nIn this repository, we did some revisions to use **REV-shuttle-bus** in the **Carla simulator**, which has different sensors, also the **autoware.universe** version we used will be changed to our version later.\n\n## **Requirements**\n\n### Software\n\n1. Carla 0.9.15 (the newest version)\n2. Autoware.universe (V1.0 Branch, will be changed to other version)\n3. Autoware-Carla-Bridge (Main Branch)\n4. ROS2 (Humble version)\n5. Ubuntu 22.04 (LTS Released version)\n6. Docker\n\n### Hardware\n\nJetson Orin Developer kit is an awesome platform for robots and Autonomous Vehicle.\n\n1. One all 1 Gigabyte Switcher or Router (for communication between your host Ubuntu machine with Jetson Orin).\n\n2. One Jetson Orin Develop kit 32GB/64GB with nvme ssd 500G external storage.\n3. One Ubuntu 22.04 Host Machine.\n4. Two or more Cat 5e/6 or above level cables \n\nAll devices are required to connect with each other in a local network. All 1 Gigabyte specs are better. \n\n------\n\n***Note:*** To illustrate the following steps easier, I use \"***Host Machine***\" stands for the machine which has a standard **GPU** device like **RTX2070** or 30/40 series and it 's also the one which will deploy a **Carla 0.9.15** simulator and **Autoware-Carla-Bridge ROS2** packages, and use \"***Jetson Orin***\" stands for the embedded system which will be used for deploying the **Autoware.universe** stack.\n\n------\n\n> Before you go through the following steps, I suggest you pull some necessary docker images using following commands, so after you finish reading this document, you can start deploying right away:\n\n```bash\n $ docker pull carlasim/carla:0.9.15\n $ docker pull tumgeka/carla-autoware-bridge:latest\n $ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n```\n\n\n\n## 3 Steps to deploy\n\n### STEP-1: Deploy the carla-0.9.15 simulator\n\nThis step is based on the carla official docker image, you have to pull the image and then run it on your Host machine.\n\nYou have to check the rpc-port is not used on your Host machine.\n\n```bash\n$ docker pull carlasim/carla:0.9.15\n$ docker run --privileged --gpus all --net=host -e DISPLAY=$DISPLAY carlasim/carla:0.9.15 /bin/bash ./CarlaUE4.sh -carla-rpc-port=1403\n```\n\n### STEP-2: Deploy the autoware-carla-bridge\n\nThis step is also based on the docker image issued by TUM team. Setting the timeout to a larger volume is necessary on my own laptop, the default value is 5000. \n\n```bash\n$ docker pull tumgeka/carla-autoware-bridge:latest\n$ docker run -it -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --network host tumgeka/carla-autoware-bridge:latest\n$ ros2 launch carla_autoware_bridge carla_aw_bridge.launch.py port:=1403 town:=Town10HD timeout:=10000\n```\n\n### STEP-3: Deploy autoware.universe stack on Jetson Orin\n\nCompile the autoware.universe is sometimes difficult, as there are various dependencies. So, I suggest that use our pre-built docker images is easier. These following commands need to run on the Jetson Orin terminals. As we want to use a Jetson Orin device to deploy the autoware.universe stack. \n\n***Note**:* *This docker image is a little bit large and only supported on the ARM64 platform, so Jetson series are all suitable.*\n\n```bash\n$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n$ sudo apt-get install python3-rocker\n$ rocker --user --nvidia --privileged --network host --x11 --volume $HOME/Documents  --volume $HOME/autoware -- 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1\n$ ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=carla_t2_vehicle sensor_model:=carla_t2_sensor_kit map_path:=/autoware1.0_ws/Town10/\n```\n\n> ***Note:*** When you run the rocker commands, and it may shows the error like the following messages: ***invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported***.\n>\n> You merely need to change the \"--gpus all\" to \"--runtime=nvidia\". like the example commands below: \"Your commands will be different with mine, you should copy the commands, after you run the ***rocker --user --nvidia xxxx** command*\"\n\n```bash\n$ docker run  --rm -it --network host   --runtime=nvidia --privileged  -e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   -e XAUTHORITY=/tmp/.dockerafc7hfmf.xauth -v /tmp/.dockerafc7hfmf.xauth:/tmp/.dockerafc7hfmf.xauth   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /etc/localtime:/etc/localtime:ro  8ea8cd5cadfe\n```\n\n\n\n### Optional\n\n#### Spawn NPCs\n\nIf you want to add some NPCs in the carla you can use the python script in the autoware-carla-brdige docker container. Must set the port to the same with the front steps. This ***generate_traffic.py*** should be used in the docker container's terminal of the STEP-2(tumgeka/carla-autoware-bridge:latest).\n\n```bash\n$ python3 src/carla_autoware_bridge/utils/generate_traffic.py -p 1403\n```\n\n#### Deploy autoware.universe stack on Host Machine\n\nCompile the autoware source codes on your host machine locally, if you do not want to deploy on the jetson embedded systems.\n\n```bash\n$ colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n```\n\nIf you want to compile the source codes on the Host Machine with a GPU card, you may need to solve some dependencies problems.\n\n\n\n\n\n\n\n## Local workflow\n\nLocal workflow will be added later, add a Dockerfile, released and revised source codes. \n\n\n\nTODO List\n\n- [ ] Add another shuttle-bus model in carla simulator to use\n- [ ] Simplified a Dockerfile for users to build the Autoware.universe docker image locally.\n- [ ] Uploads the revised autoware.universe codes for solving the problems of starting the perception module in the autoware.universe stack\n\n\n\n## Bugs Shooting\n\ndocker run error, the error info is like the followings:\n\n```bash\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'csv'\ninvoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported.Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.: unknown.\n```\n\nYou have to use the \"--runtime=nvidia \" flag to run using docker run commands.\n\n\n\n\n\n## Reference\n\n```\n@inproceedings{carla_aw_bridge24,\n\n  title = {CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research\n\nwith a Unified Framework for Simulation and Module Development,\n\n  author = {Kaljavesi, Gemb and Kerbl, Tobias and Betz, Tobias and Mitkovskii, Kirill and Diermeyer, Frank},\n\n  year = {2024}\n\n}\n```\n\nBlog link: https://www.bluenote.top/2024/03/12/008-AutonomousDriving/02-Autoware.universe/%E5%A6%82%E4%BD%953%E6%AD%A5%E5%AE%9E%E7%8E%B0%E5%9C%A8Jetson-Orin%E4%B8%8A%E8%81%94%E8%B0%83Autoware-Universe%E5%92%8CCarla-0-9-15%EF%BC%9F/index.html\n\n\n\n","slug":"008-AutonomousDriving/02-Autoware.universe/How to Joint testing between Autoware-Universe and Carla-0-9-15 on Jetson Orin?","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t6001hu9rlbxce2tsn","content":"<h1>3 Steps to Deploy Autoware.universe with Using Carla_0.9.15 on Jetson Orin</h1>\n<p>This repository is based on the github repository from TUM. <a href=\"https://github.com/TUMFTM/Carla-Autoware-Bridge.git\"><strong>LINK</strong></a></p>\n<p>Thanks a lot for their brilliant work, if you are interested in their paper, you can refer to this link [<a href=\"https://arxiv.org/abs/2402.11239\">PDF</a>]</p>\n<p>Autoware.universe source codes can be found from their github repository. <a href=\"https://github.com/autowarefoundation/autoware/tree/v1.0\"><strong>LINK</strong></a></p>\n<p>In this repository, we did some revisions to use <strong>REV-shuttle-bus</strong> in the <strong>Carla simulator</strong>, which has different sensors, also the <strong>autoware.universe</strong> version we used will be changed to our version later.</p>\n<h2 id=\"Requirements\"><strong>Requirements</strong></h2>\n<h3 id=\"Software\">Software</h3>\n<ol>\n<li>Carla 0.9.15 (the newest version)</li>\n<li>Autoware.universe (V1.0 Branch, will be changed to other version)</li>\n<li>Autoware-Carla-Bridge (Main Branch)</li>\n<li>ROS2 (Humble version)</li>\n<li>Ubuntu 22.04 (LTS Released version)</li>\n<li>Docker</li>\n</ol>\n<h3 id=\"Hardware\">Hardware</h3>\n<p>Jetson Orin Developer kit is an awesome platform for robots and Autonomous Vehicle.</p>\n<ol>\n<li>\n<p>One all 1 Gigabyte Switcher or Router (for communication between your host Ubuntu machine with Jetson Orin).</p>\n</li>\n<li>\n<p>One Jetson Orin Develop kit 32GB/64GB with nvme ssd 500G external storage.</p>\n</li>\n<li>\n<p>One Ubuntu 22.04 Host Machine.</p>\n</li>\n<li>\n<p>Two or more Cat 5e/6 or above level cables</p>\n</li>\n</ol>\n<p>All devices are required to connect with each other in a local network. All 1 Gigabyte specs are better.</p>\n<hr>\n<p><em><strong>Note:</strong></em> To illustrate the following steps easier, I use “<em><strong>Host Machine</strong></em>” stands for the machine which has a standard <strong>GPU</strong> device like <strong>RTX2070</strong> or 30/40 series and it 's also the one which will deploy a <strong>Carla 0.9.15</strong> simulator and <strong>Autoware-Carla-Bridge ROS2</strong> packages, and use “<em><strong>Jetson Orin</strong></em>” stands for the embedded system which will be used for deploying the <strong>Autoware.universe</strong> stack.</p>\n<hr>\n<blockquote>\n<p>Before you go through the following steps, I suggest you pull some necessary docker images using following commands, so after you finish reading this document, you can start deploying right away:</p>\n</blockquote>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull carlasim/carla:0.9.15</span><br><span class=\"line\">$ docker pull tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-Steps-to-deploy\">3 Steps to deploy</h2>\n<h3 id=\"STEP-1-Deploy-the-carla-0-9-15-simulator\">STEP-1: Deploy the carla-0.9.15 simulator</h3>\n<p>This step is based on the carla official docker image, you have to pull the image and then run it on your Host machine.</p>\n<p>You have to check the rpc-port is not used on your Host machine.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull carlasim/carla:0.9.15</span><br><span class=\"line\">$ docker run --privileged --gpus all --net=host -e DISPLAY=<span class=\"variable\">$DISPLAY</span> carlasim/carla:0.9.15 /bin/bash ./CarlaUE4.sh -carla-rpc-port=1403</span><br></pre></td></tr></table></figure>\n<h3 id=\"STEP-2-Deploy-the-autoware-carla-bridge\">STEP-2: Deploy the autoware-carla-bridge</h3>\n<p>This step is also based on the docker image issued by TUM team. Setting the timeout to a larger volume is necessary on my own laptop, the default value is 5000.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ docker run -it -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --network host tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ ros2 launch carla_autoware_bridge carla_aw_bridge.launch.py port:=1403 town:=Town10HD <span class=\"built_in\">timeout</span>:=10000</span><br></pre></td></tr></table></figure>\n<h3 id=\"STEP-3-Deploy-autoware-universe-stack-on-Jetson-Orin\">STEP-3: Deploy autoware.universe stack on Jetson Orin</h3>\n<p>Compile the autoware.universe is sometimes difficult, as there are various dependencies. So, I suggest that use our pre-built docker images is easier. These following commands need to run on the Jetson Orin terminals. As we want to use a Jetson Orin device to deploy the autoware.universe stack.</p>\n<p><em><strong>Note</strong>:</em> <em>This docker image is a little bit large and only supported on the ARM64 platform, so Jetson series are all suitable.</em></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br><span class=\"line\">$ sudo apt-get install python3-rocker</span><br><span class=\"line\">$ rocker --user --nvidia --privileged --network host --x11 --volume <span class=\"variable\">$HOME</span>/Documents  --volume <span class=\"variable\">$HOME</span>/autoware -- 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br><span class=\"line\">$ ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=carla_t2_vehicle sensor_model:=carla_t2_sensor_kit map_path:=/autoware1.0_ws/Town10/</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><em><strong>Note:</strong></em> When you run the rocker commands, and it may shows the error like the following messages: <em><strong>invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported</strong></em>.</p>\n<p>You merely need to change the “–gpus all” to “–runtime=nvidia”. like the example commands below: “Your commands will be different with mine, you should copy the commands, after you run the <em><strong>rocker --user --nvidia xxxx</strong> command</em>”</p>\n</blockquote>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run  --<span class=\"built_in\">rm</span> -it --network host   --runtime=nvidia --privileged  -e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   -e XAUTHORITY=/tmp/.dockerafc7hfmf.xauth -v /tmp/.dockerafc7hfmf.xauth:/tmp/.dockerafc7hfmf.xauth   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /etc/localtime:/etc/localtime:ro  8ea8cd5cadfe</span><br></pre></td></tr></table></figure>\n<h3 id=\"Optional\">Optional</h3>\n<h4 id=\"Spawn-NPCs\">Spawn NPCs</h4>\n<p>If you want to add some NPCs in the carla you can use the python script in the autoware-carla-brdige docker container. Must set the port to the same with the front steps. This <em><strong>generate_traffic.py</strong></em> should be used in the docker container’s terminal of the STEP-2(tumgeka/carla-autoware-bridge:latest).</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ python3 src/carla_autoware_bridge/utils/generate_traffic.py -p 1403</span><br></pre></td></tr></table></figure>\n<h4 id=\"Deploy-autoware-universe-stack-on-Host-Machine\">Deploy autoware.universe stack on Host Machine</h4>\n<p>Compile the autoware source codes on your host machine locally, if you do not want to deploy on the jetson embedded systems.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br></pre></td></tr></table></figure>\n<p>If you want to compile the source codes on the Host Machine with a GPU card, you may need to solve some dependencies problems.</p>\n<h2 id=\"Local-workflow\">Local workflow</h2>\n<p>Local workflow will be added later, add a Dockerfile, released and revised source codes.</p>\n<p>TODO List</p>\n<ul>\n<li>[ ] Add another shuttle-bus model in carla simulator to use</li>\n<li>[ ] Simplified a Dockerfile for users to build the Autoware.universe docker image locally.</li>\n<li>[ ] Uploads the revised autoware.universe codes for solving the problems of starting the perception module in the autoware.universe stack</li>\n</ul>\n<h2 id=\"Bugs-Shooting\">Bugs Shooting</h2>\n<p>docker run error, the error info is like the followings:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker: Error response from daemon: failed to create task <span class=\"keyword\">for</span> container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook <span class=\"comment\">#0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as &#x27;csv&#x27;</span></span><br><span class=\"line\">invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported.Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.: unknown.</span><br></pre></td></tr></table></figure>\n<p>You have to use the &quot;–runtime=nvidia &quot; flag to run using docker run commands.</p>\n<h2 id=\"Reference\">Reference</h2>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;carla_aw_bridge24,</span><br><span class=\"line\"></span><br><span class=\"line\">  title = &#123;CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research</span><br><span class=\"line\"></span><br><span class=\"line\">with a Unified Framework for Simulation and Module Development,</span><br><span class=\"line\"></span><br><span class=\"line\">  author = &#123;Kaljavesi, Gemb and Kerbl, Tobias and Betz, Tobias and Mitkovskii, Kirill and Diermeyer, Frank&#125;,</span><br><span class=\"line\"></span><br><span class=\"line\">  year = &#123;2024&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Blog link: <a href=\"https://www.bluenote.top/2024/03/12/008-AutonomousDriving/02-Autoware.universe/%E5%A6%82%E4%BD%953%E6%AD%A5%E5%AE%9E%E7%8E%B0%E5%9C%A8Jetson-Orin%E4%B8%8A%E8%81%94%E8%B0%83Autoware-Universe%E5%92%8CCarla-0-9-15%EF%BC%9F/index.html\">https://www.bluenote.top/2024/03/12/008-AutonomousDriving/02-Autoware.universe/如何3步实现在Jetson-Orin上联调Autoware-Universe和Carla-0-9-15？/index.html</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1>3 Steps to Deploy Autoware.universe with Using Carla_0.9.15 on Jetson Orin</h1>\n<p>This repository is based on the github repository from TUM. <a href=\"https://github.com/TUMFTM/Carla-Autoware-Bridge.git\"><strong>LINK</strong></a></p>\n<p>Thanks a lot for their brilliant work, if you are interested in their paper, you can refer to this link [<a href=\"https://arxiv.org/abs/2402.11239\">PDF</a>]</p>\n<p>Autoware.universe source codes can be found from their github repository. <a href=\"https://github.com/autowarefoundation/autoware/tree/v1.0\"><strong>LINK</strong></a></p>\n<p>In this repository, we did some revisions to use <strong>REV-shuttle-bus</strong> in the <strong>Carla simulator</strong>, which has different sensors, also the <strong>autoware.universe</strong> version we used will be changed to our version later.</p>\n<h2 id=\"Requirements\"><strong>Requirements</strong></h2>\n<h3 id=\"Software\">Software</h3>\n<ol>\n<li>Carla 0.9.15 (the newest version)</li>\n<li>Autoware.universe (V1.0 Branch, will be changed to other version)</li>\n<li>Autoware-Carla-Bridge (Main Branch)</li>\n<li>ROS2 (Humble version)</li>\n<li>Ubuntu 22.04 (LTS Released version)</li>\n<li>Docker</li>\n</ol>\n<h3 id=\"Hardware\">Hardware</h3>\n<p>Jetson Orin Developer kit is an awesome platform for robots and Autonomous Vehicle.</p>\n<ol>\n<li>\n<p>One all 1 Gigabyte Switcher or Router (for communication between your host Ubuntu machine with Jetson Orin).</p>\n</li>\n<li>\n<p>One Jetson Orin Develop kit 32GB/64GB with nvme ssd 500G external storage.</p>\n</li>\n<li>\n<p>One Ubuntu 22.04 Host Machine.</p>\n</li>\n<li>\n<p>Two or more Cat 5e/6 or above level cables</p>\n</li>\n</ol>\n<p>All devices are required to connect with each other in a local network. All 1 Gigabyte specs are better.</p>\n<hr>\n<p><em><strong>Note:</strong></em> To illustrate the following steps easier, I use “<em><strong>Host Machine</strong></em>” stands for the machine which has a standard <strong>GPU</strong> device like <strong>RTX2070</strong> or 30/40 series and it 's also the one which will deploy a <strong>Carla 0.9.15</strong> simulator and <strong>Autoware-Carla-Bridge ROS2</strong> packages, and use “<em><strong>Jetson Orin</strong></em>” stands for the embedded system which will be used for deploying the <strong>Autoware.universe</strong> stack.</p>\n<hr>\n<blockquote>\n<p>Before you go through the following steps, I suggest you pull some necessary docker images using following commands, so after you finish reading this document, you can start deploying right away:</p>\n</blockquote>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull carlasim/carla:0.9.15</span><br><span class=\"line\">$ docker pull tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-Steps-to-deploy\">3 Steps to deploy</h2>\n<h3 id=\"STEP-1-Deploy-the-carla-0-9-15-simulator\">STEP-1: Deploy the carla-0.9.15 simulator</h3>\n<p>This step is based on the carla official docker image, you have to pull the image and then run it on your Host machine.</p>\n<p>You have to check the rpc-port is not used on your Host machine.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull carlasim/carla:0.9.15</span><br><span class=\"line\">$ docker run --privileged --gpus all --net=host -e DISPLAY=<span class=\"variable\">$DISPLAY</span> carlasim/carla:0.9.15 /bin/bash ./CarlaUE4.sh -carla-rpc-port=1403</span><br></pre></td></tr></table></figure>\n<h3 id=\"STEP-2-Deploy-the-autoware-carla-bridge\">STEP-2: Deploy the autoware-carla-bridge</h3>\n<p>This step is also based on the docker image issued by TUM team. Setting the timeout to a larger volume is necessary on my own laptop, the default value is 5000.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ docker run -it -e RMW_IMPLEMENTATION=rmw_cyclonedds_cpp --network host tumgeka/carla-autoware-bridge:latest</span><br><span class=\"line\">$ ros2 launch carla_autoware_bridge carla_aw_bridge.launch.py port:=1403 town:=Town10HD <span class=\"built_in\">timeout</span>:=10000</span><br></pre></td></tr></table></figure>\n<h3 id=\"STEP-3-Deploy-autoware-universe-stack-on-Jetson-Orin\">STEP-3: Deploy autoware.universe stack on Jetson Orin</h3>\n<p>Compile the autoware.universe is sometimes difficult, as there are various dependencies. So, I suggest that use our pre-built docker images is easier. These following commands need to run on the Jetson Orin terminals. As we want to use a Jetson Orin device to deploy the autoware.universe stack.</p>\n<p><em><strong>Note</strong>:</em> <em>This docker image is a little bit large and only supported on the ARM64 platform, so Jetson series are all suitable.</em></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br><span class=\"line\">$ sudo apt-get install python3-rocker</span><br><span class=\"line\">$ rocker --user --nvidia --privileged --network host --x11 --volume <span class=\"variable\">$HOME</span>/Documents  --volume <span class=\"variable\">$HOME</span>/autoware -- 1429053840/autoware.universe-carla-0.9.15:humble-20240215-cuda-arm64-v0.1</span><br><span class=\"line\">$ ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=carla_t2_vehicle sensor_model:=carla_t2_sensor_kit map_path:=/autoware1.0_ws/Town10/</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><em><strong>Note:</strong></em> When you run the rocker commands, and it may shows the error like the following messages: <em><strong>invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported</strong></em>.</p>\n<p>You merely need to change the “–gpus all” to “–runtime=nvidia”. like the example commands below: “Your commands will be different with mine, you should copy the commands, after you run the <em><strong>rocker --user --nvidia xxxx</strong> command</em>”</p>\n</blockquote>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run  --<span class=\"built_in\">rm</span> -it --network host   --runtime=nvidia --privileged  -e DISPLAY -e TERM   -e QT_X11_NO_MITSHM=1   -e XAUTHORITY=/tmp/.dockerafc7hfmf.xauth -v /tmp/.dockerafc7hfmf.xauth:/tmp/.dockerafc7hfmf.xauth   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /etc/localtime:/etc/localtime:ro  8ea8cd5cadfe</span><br></pre></td></tr></table></figure>\n<h3 id=\"Optional\">Optional</h3>\n<h4 id=\"Spawn-NPCs\">Spawn NPCs</h4>\n<p>If you want to add some NPCs in the carla you can use the python script in the autoware-carla-brdige docker container. Must set the port to the same with the front steps. This <em><strong>generate_traffic.py</strong></em> should be used in the docker container’s terminal of the STEP-2(tumgeka/carla-autoware-bridge:latest).</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ python3 src/carla_autoware_bridge/utils/generate_traffic.py -p 1403</span><br></pre></td></tr></table></figure>\n<h4 id=\"Deploy-autoware-universe-stack-on-Host-Machine\">Deploy autoware.universe stack on Host Machine</h4>\n<p>Compile the autoware source codes on your host machine locally, if you do not want to deploy on the jetson embedded systems.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br></pre></td></tr></table></figure>\n<p>If you want to compile the source codes on the Host Machine with a GPU card, you may need to solve some dependencies problems.</p>\n<h2 id=\"Local-workflow\">Local workflow</h2>\n<p>Local workflow will be added later, add a Dockerfile, released and revised source codes.</p>\n<p>TODO List</p>\n<ul>\n<li>[ ] Add another shuttle-bus model in carla simulator to use</li>\n<li>[ ] Simplified a Dockerfile for users to build the Autoware.universe docker image locally.</li>\n<li>[ ] Uploads the revised autoware.universe codes for solving the problems of starting the perception module in the autoware.universe stack</li>\n</ul>\n<h2 id=\"Bugs-Shooting\">Bugs Shooting</h2>\n<p>docker run error, the error info is like the followings:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker: Error response from daemon: failed to create task <span class=\"keyword\">for</span> container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook <span class=\"comment\">#0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as &#x27;csv&#x27;</span></span><br><span class=\"line\">invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported.Please use the NVIDIA Container Runtime (e.g. specify the --runtime=nvidia flag) instead.: unknown.</span><br></pre></td></tr></table></figure>\n<p>You have to use the &quot;–runtime=nvidia &quot; flag to run using docker run commands.</p>\n<h2 id=\"Reference\">Reference</h2>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;carla_aw_bridge24,</span><br><span class=\"line\"></span><br><span class=\"line\">  title = &#123;CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research</span><br><span class=\"line\"></span><br><span class=\"line\">with a Unified Framework for Simulation and Module Development,</span><br><span class=\"line\"></span><br><span class=\"line\">  author = &#123;Kaljavesi, Gemb and Kerbl, Tobias and Betz, Tobias and Mitkovskii, Kirill and Diermeyer, Frank&#125;,</span><br><span class=\"line\"></span><br><span class=\"line\">  year = &#123;2024&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Blog link: <a href=\"https://www.bluenote.top/2024/03/12/008-AutonomousDriving/02-Autoware.universe/%E5%A6%82%E4%BD%953%E6%AD%A5%E5%AE%9E%E7%8E%B0%E5%9C%A8Jetson-Orin%E4%B8%8A%E8%81%94%E8%B0%83Autoware-Universe%E5%92%8CCarla-0-9-15%EF%BC%9F/index.html\">https://www.bluenote.top/2024/03/12/008-AutonomousDriving/02-Autoware.universe/如何3步实现在Jetson-Orin上联调Autoware-Universe和Carla-0-9-15？/index.html</a></p>\n"},{"title":"Ubuntu18安装AWSIM运行Autoware.Universe","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-06-07T16:08:53.000Z","updated":"2023-06-07T16:08:53.000Z","keywords":null,"description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/5f19587122afa_270_185.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***Ubuntu18安装AWSIM运行Autoware.Universe***\n\n主要介绍如何在Ubuntu18主机系统上快速配置好仿真环境以及使用autoware.universe在docker容器中进行车辆自动驾驶。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n# 效果图\n\n![Image_top](https://www.synotech.top:5523/uploads/2023/06/08/202306080127094.jpg)\n\n\n\n# 仿真配置情况\n\n![image-20230608012930509](https://www.synotech.top:5523/uploads/2023/06/08/202306080129751.jpg)\n\n# 环境要求\n\n## 主机系统\n\n如果打算在主机系统本地运行仿真，则需要参考官方推荐，使用20或者22的ubuntu系统。\n\n选择Ubuntu20需要选择**1.0.2**的版本\n\n[AWSIM/index.md at v1.0.2 · tier4/AWSIM](https://github.com/tier4/AWSIM/blob/v1.0.2/docs/GettingStarted/QuickStartDemo/index.md)\n\nubuntu20使用的是ROS2 Galactic版本。\n\n选择Ubuntu22可以选择**1.1.0**版本\n\n## 硬件要求\n\n![image-20230608013036319](https://www.synotech.top:5523/uploads/2023/06/08/202306080130564.jpg)\n\n经过测试内存如果在16GB，显存是8GB可能已经是最小能运行的配置了，因为我实际使用这个配置中，会出现仿真中画面卡顿，不流畅的问题。\n\n如果需要在本地把bashrc的配置更改一下，如下：\n\n```text\nexport ROS_LOCALHOST_ONLY=1\nexport RMW_IMPLEMENTATION=rmw_cyclonedds_cpp\n\nif [ ! -e /tmp/cycloneDDS_configured ]; then\n    sudo sysctl -w net.core.rmem_max=2147483647\n    sudo ip link set lo multicast on\n    touch /tmp/cycloneDDS_configured\nfi\n```\n\n## 安装显卡驱动和Vulkan Graphics Library\n\n```text\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt update\nsudo ubuntu-drivers autoinstall\n##如果你没有安装过显卡驱动，第一次安装驱动都需要重启下\nsudo reboot\n##使用nvidia-smi查看显卡驱动等信息\n安装Vulkan\nsudo apt install libvulkan1\n```\n\n## 下载仿真器的二进制文件\n\n以下链接下载：\n\n[__https://github.com/tier4/AWSIM/releases/download/v1.1.0/AWSIM_v1.1.0.zip__](https://github.com/tier4/AWSIM/releases/download/v1.1.0/AWSIM_v1.1.0.zip)\n\n解压文件到$HOME路径下，\n\n解压出文件，执行AWSIM_demo.x86_64，就可以启动仿真环境，***这里我们先不启动，因为我们后面要在容器内启动***。\n\n## 下载地图\n\n使用以下链接下载地图：\n\n[__https://github.com/tier4/AWSIM/releases/download/v1.1.0/nishishinjuku_autoware_map.zip__](https://github.com/tier4/AWSIM/releases/download/v1.1.0/nishishinjuku_autoware_map.zip)\n\n下载完后，创建一个文件夹，解压地图到里面\n\n```text\nmkdir ~/autoware_map\n解压的地图文件放在这个文件夹下\n```\n\n# 拉取源码\n\n```text\ngit clone https://github.com/autowarefoundation/autoware.git\ncd autoware\n需要切换到awsim-stable分支下\ngit checkout awsim-stable\n```\n\n# 安装主机依赖项\n\n1. CUDA\n\n1. Docker Engine\n\n1. NVIDIA Container Toolkit\n\n1. Rocker\n\n你可以选择自己手动以此安装好上面的依赖项，也可以使用脚本的方式安装，在容器内已经都有了相关的环境，所以不需要安装了。在主机系统上安装的话可以以下使用脚本安装以上依赖\n\n```text\n./setup-dev-env.sh\n```\n\n# 拉取镜像并启动\n\n```text\ndocker pull ghcr.io/autowarefoundation/autoware-universe:latest-cuda\nrocker --nvidia --x11 --user --volume $HOME/AWSIM_v1.1.0 --volume $HOME/autoware --volume $HOME/autoware_map -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda\n```\n\n创建src文件夹并克隆代码进来\n\n```text\nmkdir src\nvcs import src < autoware.repos\n```\n\n# 安装ROS依赖包\n\n```text\nsource /opt/ros/humble/setup.bash\nrosdep update\nrosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO\n```\n\n# 编译工作空间\n\n```text\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"-w\"\n```\n\n# 启动Autoware\n\n```text\nsource install/setup.bash\nros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=/home/xx/autoware_map/nishishinjuku_autoware_map\n```\n\n**BUGS**\n\n启动的时候，地图路径需要按照绝对路径填写，不能使用~/map_path这样的相对路径。\n\n上面的步骤都顺利的话，你将会看到rviz在你主机系统上出现以下窗口内容：\n\n![Image_2](https://www.synotech.top:5523/uploads/2023/06/08/202306080127254.jpg)\n\n如果你可以看到在地图上有车辆和电云等初始位置的信息，那么你成功完成了autoware和仿真环境的关联。\n\n接下来你给定一个目标点，然后给出一个启动的信号如下指令：\n\n```text\ncd autoware\nsource install/setup.bash\nros2 topic pub /autoware/engage autoware_auto_vehicle_msgs/msg/Engage '{engage: True}' -1\n```\n\n车辆将会自动运行到你设定的目标点位置。\n\n\n\n其中遇到的一些上述没有说的问题，可以参考以下链接去找解决方案，这里面包含了一些常见的问题。\n\n[__https://tier4.github.io/AWSIM/DeveloperGuide/TroubleShooting/__](https://tier4.github.io/AWSIM/DeveloperGuide/TroubleShooting/)\n\n\n\n正确运行起来之后，我在笔记本GPU是2070-MAXQ和Ubuntu18.04上使用官方提供的镜像和awsim-stable代码进行演示，效果如下：\n\n![image-20230608012831843](https://www.synotech.top:5523/uploads/2023/06/08/202306080128339.jpg)\n\n\n\n# 参考引用\n\n**参考官网链接**\n\n[Quick Start Demo - AWSIM document](https://tier4.github.io/AWSIM/GettingStarted/QuickStartDemo/)\n\n","source":"_posts/008-AutonomousDriving/02-Autoware.universe/Ubuntu18安装AWSIM运行Autoware-Universe.md","raw":"---\ntitle: Ubuntu18安装AWSIM运行Autoware.Universe\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-06-08 00:08:53\nupdated: 2023-06-08 00:08:53\ntags: [Autoware.Universe, Ubuntu18, AWSIM]\ncategories:\n- 自动驾驶\n- Autoware.Universe\nkeywords:\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***Ubuntu18安装AWSIM运行Autoware.Universe***\n\n主要介绍如何在Ubuntu18主机系统上快速配置好仿真环境以及使用autoware.universe在docker容器中进行车辆自动驾驶。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n# 效果图\n\n![Image_top](https://www.synotech.top:5523/uploads/2023/06/08/202306080127094.jpg)\n\n\n\n# 仿真配置情况\n\n![image-20230608012930509](https://www.synotech.top:5523/uploads/2023/06/08/202306080129751.jpg)\n\n# 环境要求\n\n## 主机系统\n\n如果打算在主机系统本地运行仿真，则需要参考官方推荐，使用20或者22的ubuntu系统。\n\n选择Ubuntu20需要选择**1.0.2**的版本\n\n[AWSIM/index.md at v1.0.2 · tier4/AWSIM](https://github.com/tier4/AWSIM/blob/v1.0.2/docs/GettingStarted/QuickStartDemo/index.md)\n\nubuntu20使用的是ROS2 Galactic版本。\n\n选择Ubuntu22可以选择**1.1.0**版本\n\n## 硬件要求\n\n![image-20230608013036319](https://www.synotech.top:5523/uploads/2023/06/08/202306080130564.jpg)\n\n经过测试内存如果在16GB，显存是8GB可能已经是最小能运行的配置了，因为我实际使用这个配置中，会出现仿真中画面卡顿，不流畅的问题。\n\n如果需要在本地把bashrc的配置更改一下，如下：\n\n```text\nexport ROS_LOCALHOST_ONLY=1\nexport RMW_IMPLEMENTATION=rmw_cyclonedds_cpp\n\nif [ ! -e /tmp/cycloneDDS_configured ]; then\n    sudo sysctl -w net.core.rmem_max=2147483647\n    sudo ip link set lo multicast on\n    touch /tmp/cycloneDDS_configured\nfi\n```\n\n## 安装显卡驱动和Vulkan Graphics Library\n\n```text\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt update\nsudo ubuntu-drivers autoinstall\n##如果你没有安装过显卡驱动，第一次安装驱动都需要重启下\nsudo reboot\n##使用nvidia-smi查看显卡驱动等信息\n安装Vulkan\nsudo apt install libvulkan1\n```\n\n## 下载仿真器的二进制文件\n\n以下链接下载：\n\n[__https://github.com/tier4/AWSIM/releases/download/v1.1.0/AWSIM_v1.1.0.zip__](https://github.com/tier4/AWSIM/releases/download/v1.1.0/AWSIM_v1.1.0.zip)\n\n解压文件到$HOME路径下，\n\n解压出文件，执行AWSIM_demo.x86_64，就可以启动仿真环境，***这里我们先不启动，因为我们后面要在容器内启动***。\n\n## 下载地图\n\n使用以下链接下载地图：\n\n[__https://github.com/tier4/AWSIM/releases/download/v1.1.0/nishishinjuku_autoware_map.zip__](https://github.com/tier4/AWSIM/releases/download/v1.1.0/nishishinjuku_autoware_map.zip)\n\n下载完后，创建一个文件夹，解压地图到里面\n\n```text\nmkdir ~/autoware_map\n解压的地图文件放在这个文件夹下\n```\n\n# 拉取源码\n\n```text\ngit clone https://github.com/autowarefoundation/autoware.git\ncd autoware\n需要切换到awsim-stable分支下\ngit checkout awsim-stable\n```\n\n# 安装主机依赖项\n\n1. CUDA\n\n1. Docker Engine\n\n1. NVIDIA Container Toolkit\n\n1. Rocker\n\n你可以选择自己手动以此安装好上面的依赖项，也可以使用脚本的方式安装，在容器内已经都有了相关的环境，所以不需要安装了。在主机系统上安装的话可以以下使用脚本安装以上依赖\n\n```text\n./setup-dev-env.sh\n```\n\n# 拉取镜像并启动\n\n```text\ndocker pull ghcr.io/autowarefoundation/autoware-universe:latest-cuda\nrocker --nvidia --x11 --user --volume $HOME/AWSIM_v1.1.0 --volume $HOME/autoware --volume $HOME/autoware_map -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda\n```\n\n创建src文件夹并克隆代码进来\n\n```text\nmkdir src\nvcs import src < autoware.repos\n```\n\n# 安装ROS依赖包\n\n```text\nsource /opt/ros/humble/setup.bash\nrosdep update\nrosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO\n```\n\n# 编译工作空间\n\n```text\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"-w\"\n```\n\n# 启动Autoware\n\n```text\nsource install/setup.bash\nros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=/home/xx/autoware_map/nishishinjuku_autoware_map\n```\n\n**BUGS**\n\n启动的时候，地图路径需要按照绝对路径填写，不能使用~/map_path这样的相对路径。\n\n上面的步骤都顺利的话，你将会看到rviz在你主机系统上出现以下窗口内容：\n\n![Image_2](https://www.synotech.top:5523/uploads/2023/06/08/202306080127254.jpg)\n\n如果你可以看到在地图上有车辆和电云等初始位置的信息，那么你成功完成了autoware和仿真环境的关联。\n\n接下来你给定一个目标点，然后给出一个启动的信号如下指令：\n\n```text\ncd autoware\nsource install/setup.bash\nros2 topic pub /autoware/engage autoware_auto_vehicle_msgs/msg/Engage '{engage: True}' -1\n```\n\n车辆将会自动运行到你设定的目标点位置。\n\n\n\n其中遇到的一些上述没有说的问题，可以参考以下链接去找解决方案，这里面包含了一些常见的问题。\n\n[__https://tier4.github.io/AWSIM/DeveloperGuide/TroubleShooting/__](https://tier4.github.io/AWSIM/DeveloperGuide/TroubleShooting/)\n\n\n\n正确运行起来之后，我在笔记本GPU是2070-MAXQ和Ubuntu18.04上使用官方提供的镜像和awsim-stable代码进行演示，效果如下：\n\n![image-20230608012831843](https://www.synotech.top:5523/uploads/2023/06/08/202306080128339.jpg)\n\n\n\n# 参考引用\n\n**参考官网链接**\n\n[Quick Start Demo - AWSIM document](https://tier4.github.io/AWSIM/GettingStarted/QuickStartDemo/)\n\n","slug":"008-AutonomousDriving/02-Autoware.universe/Ubuntu18安装AWSIM运行Autoware-Universe","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t8001lu9rl5jk3ga8t","content":"<h1><em><strong>Ubuntu18安装AWSIM运行Autoware.Universe</strong></em></h1>\n<p>主要介绍如何在Ubuntu18主机系统上快速配置好仿真环境以及使用autoware.universe在docker容器中进行车辆自动驾驶。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>效果图</h1>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080127094.jpg\" alt=\"Image_top\"></p>\n<h1>仿真配置情况</h1>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080129751.jpg\" alt=\"image-20230608012930509\"></p>\n<h1>环境要求</h1>\n<h2 id=\"主机系统\">主机系统</h2>\n<p>如果打算在主机系统本地运行仿真，则需要参考官方推荐，使用20或者22的ubuntu系统。</p>\n<p>选择Ubuntu20需要选择<strong>1.0.2</strong>的版本</p>\n<p><a href=\"https://github.com/tier4/AWSIM/blob/v1.0.2/docs/GettingStarted/QuickStartDemo/index.md\">AWSIM/index.md at v1.0.2 · tier4/AWSIM</a></p>\n<p>ubuntu20使用的是ROS2 Galactic版本。</p>\n<p>选择Ubuntu22可以选择<strong>1.1.0</strong>版本</p>\n<h2 id=\"硬件要求\">硬件要求</h2>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080130564.jpg\" alt=\"image-20230608013036319\"></p>\n<p>经过测试内存如果在16GB，显存是8GB可能已经是最小能运行的配置了，因为我实际使用这个配置中，会出现仿真中画面卡顿，不流畅的问题。</p>\n<p>如果需要在本地把bashrc的配置更改一下，如下：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export ROS_LOCALHOST_ONLY=1</span><br><span class=\"line\">export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp</span><br><span class=\"line\"></span><br><span class=\"line\">if [ ! -e /tmp/cycloneDDS_configured ]; then</span><br><span class=\"line\">    sudo sysctl -w net.core.rmem_max=2147483647</span><br><span class=\"line\">    sudo ip link set lo multicast on</span><br><span class=\"line\">    touch /tmp/cycloneDDS_configured</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<h2 id=\"安装显卡驱动和Vulkan-Graphics-Library\">安装显卡驱动和Vulkan Graphics Library</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class=\"line\">sudo apt update</span><br><span class=\"line\">sudo ubuntu-drivers autoinstall</span><br><span class=\"line\">##如果你没有安装过显卡驱动，第一次安装驱动都需要重启下</span><br><span class=\"line\">sudo reboot</span><br><span class=\"line\">##使用nvidia-smi查看显卡驱动等信息</span><br><span class=\"line\">安装Vulkan</span><br><span class=\"line\">sudo apt install libvulkan1</span><br></pre></td></tr></table></figure>\n<h2 id=\"下载仿真器的二进制文件\">下载仿真器的二进制文件</h2>\n<p>以下链接下载：</p>\n<p><a href=\"https://github.com/tier4/AWSIM/releases/download/v1.1.0/AWSIM_v1.1.0.zip\"><strong>https://github.com/tier4/AWSIM/releases/download/v1.1.0/AWSIM_v1.1.0.zip</strong></a></p>\n<p>解压文件到$HOME路径下，</p>\n<p>解压出文件，执行AWSIM_demo.x86_64，就可以启动仿真环境，<em><strong>这里我们先不启动，因为我们后面要在容器内启动</strong></em>。</p>\n<h2 id=\"下载地图\">下载地图</h2>\n<p>使用以下链接下载地图：</p>\n<p><a href=\"https://github.com/tier4/AWSIM/releases/download/v1.1.0/nishishinjuku_autoware_map.zip\"><strong>https://github.com/tier4/AWSIM/releases/download/v1.1.0/nishishinjuku_autoware_map.zip</strong></a></p>\n<p>下载完后，创建一个文件夹，解压地图到里面</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/autoware_map</span><br><span class=\"line\">解压的地图文件放在这个文件夹下</span><br></pre></td></tr></table></figure>\n<h1>拉取源码</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/autowarefoundation/autoware.git</span><br><span class=\"line\">cd autoware</span><br><span class=\"line\">需要切换到awsim-stable分支下</span><br><span class=\"line\">git checkout awsim-stable</span><br></pre></td></tr></table></figure>\n<h1>安装主机依赖项</h1>\n<ol>\n<li>\n<p>CUDA</p>\n</li>\n<li>\n<p>Docker Engine</p>\n</li>\n<li>\n<p>NVIDIA Container Toolkit</p>\n</li>\n<li>\n<p>Rocker</p>\n</li>\n</ol>\n<p>你可以选择自己手动以此安装好上面的依赖项，也可以使用脚本的方式安装，在容器内已经都有了相关的环境，所以不需要安装了。在主机系统上安装的话可以以下使用脚本安装以上依赖</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./setup-dev-env.sh</span><br></pre></td></tr></table></figure>\n<h1>拉取镜像并启动</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br><span class=\"line\">rocker --nvidia --x11 --user --volume $HOME/AWSIM_v1.1.0 --volume $HOME/autoware --volume $HOME/autoware_map -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br></pre></td></tr></table></figure>\n<p>创建src文件夹并克隆代码进来</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir src</span><br><span class=\"line\">vcs import src &lt; autoware.repos</span><br></pre></td></tr></table></figure>\n<h1>安装ROS依赖包</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source /opt/ros/humble/setup.bash</span><br><span class=\"line\">rosdep update</span><br><span class=\"line\">rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO</span><br></pre></td></tr></table></figure>\n<h1>编译工作空间</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=&quot;-w&quot;</span><br></pre></td></tr></table></figure>\n<h1>启动Autoware</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=/home/xx/autoware_map/nishishinjuku_autoware_map</span><br></pre></td></tr></table></figure>\n<p><strong>BUGS</strong></p>\n<p>启动的时候，地图路径需要按照绝对路径填写，不能使用~/map_path这样的相对路径。</p>\n<p>上面的步骤都顺利的话，你将会看到rviz在你主机系统上出现以下窗口内容：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080127254.jpg\" alt=\"Image_2\"></p>\n<p>如果你可以看到在地图上有车辆和电云等初始位置的信息，那么你成功完成了autoware和仿真环境的关联。</p>\n<p>接下来你给定一个目标点，然后给出一个启动的信号如下指令：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd autoware</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 topic pub /autoware/engage autoware_auto_vehicle_msgs/msg/Engage &#x27;&#123;engage: True&#125;&#x27; -1</span><br></pre></td></tr></table></figure>\n<p>车辆将会自动运行到你设定的目标点位置。</p>\n<p>其中遇到的一些上述没有说的问题，可以参考以下链接去找解决方案，这里面包含了一些常见的问题。</p>\n<p><a href=\"https://tier4.github.io/AWSIM/DeveloperGuide/TroubleShooting/\"><strong>https://tier4.github.io/AWSIM/DeveloperGuide/TroubleShooting/</strong></a></p>\n<p>正确运行起来之后，我在笔记本GPU是2070-MAXQ和Ubuntu18.04上使用官方提供的镜像和awsim-stable代码进行演示，效果如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080128339.jpg\" alt=\"image-20230608012831843\"></p>\n<h1>参考引用</h1>\n<p><strong>参考官网链接</strong></p>\n<p><a href=\"https://tier4.github.io/AWSIM/GettingStarted/QuickStartDemo/\">Quick Start Demo - AWSIM document</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>Ubuntu18安装AWSIM运行Autoware.Universe</strong></em></h1>\n<p>主要介绍如何在Ubuntu18主机系统上快速配置好仿真环境以及使用autoware.universe在docker容器中进行车辆自动驾驶。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>效果图</h1>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080127094.jpg\" alt=\"Image_top\"></p>\n<h1>仿真配置情况</h1>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080129751.jpg\" alt=\"image-20230608012930509\"></p>\n<h1>环境要求</h1>\n<h2 id=\"主机系统\">主机系统</h2>\n<p>如果打算在主机系统本地运行仿真，则需要参考官方推荐，使用20或者22的ubuntu系统。</p>\n<p>选择Ubuntu20需要选择<strong>1.0.2</strong>的版本</p>\n<p><a href=\"https://github.com/tier4/AWSIM/blob/v1.0.2/docs/GettingStarted/QuickStartDemo/index.md\">AWSIM/index.md at v1.0.2 · tier4/AWSIM</a></p>\n<p>ubuntu20使用的是ROS2 Galactic版本。</p>\n<p>选择Ubuntu22可以选择<strong>1.1.0</strong>版本</p>\n<h2 id=\"硬件要求\">硬件要求</h2>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080130564.jpg\" alt=\"image-20230608013036319\"></p>\n<p>经过测试内存如果在16GB，显存是8GB可能已经是最小能运行的配置了，因为我实际使用这个配置中，会出现仿真中画面卡顿，不流畅的问题。</p>\n<p>如果需要在本地把bashrc的配置更改一下，如下：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export ROS_LOCALHOST_ONLY=1</span><br><span class=\"line\">export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp</span><br><span class=\"line\"></span><br><span class=\"line\">if [ ! -e /tmp/cycloneDDS_configured ]; then</span><br><span class=\"line\">    sudo sysctl -w net.core.rmem_max=2147483647</span><br><span class=\"line\">    sudo ip link set lo multicast on</span><br><span class=\"line\">    touch /tmp/cycloneDDS_configured</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n<h2 id=\"安装显卡驱动和Vulkan-Graphics-Library\">安装显卡驱动和Vulkan Graphics Library</h2>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class=\"line\">sudo apt update</span><br><span class=\"line\">sudo ubuntu-drivers autoinstall</span><br><span class=\"line\">##如果你没有安装过显卡驱动，第一次安装驱动都需要重启下</span><br><span class=\"line\">sudo reboot</span><br><span class=\"line\">##使用nvidia-smi查看显卡驱动等信息</span><br><span class=\"line\">安装Vulkan</span><br><span class=\"line\">sudo apt install libvulkan1</span><br></pre></td></tr></table></figure>\n<h2 id=\"下载仿真器的二进制文件\">下载仿真器的二进制文件</h2>\n<p>以下链接下载：</p>\n<p><a href=\"https://github.com/tier4/AWSIM/releases/download/v1.1.0/AWSIM_v1.1.0.zip\"><strong>https://github.com/tier4/AWSIM/releases/download/v1.1.0/AWSIM_v1.1.0.zip</strong></a></p>\n<p>解压文件到$HOME路径下，</p>\n<p>解压出文件，执行AWSIM_demo.x86_64，就可以启动仿真环境，<em><strong>这里我们先不启动，因为我们后面要在容器内启动</strong></em>。</p>\n<h2 id=\"下载地图\">下载地图</h2>\n<p>使用以下链接下载地图：</p>\n<p><a href=\"https://github.com/tier4/AWSIM/releases/download/v1.1.0/nishishinjuku_autoware_map.zip\"><strong>https://github.com/tier4/AWSIM/releases/download/v1.1.0/nishishinjuku_autoware_map.zip</strong></a></p>\n<p>下载完后，创建一个文件夹，解压地图到里面</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/autoware_map</span><br><span class=\"line\">解压的地图文件放在这个文件夹下</span><br></pre></td></tr></table></figure>\n<h1>拉取源码</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/autowarefoundation/autoware.git</span><br><span class=\"line\">cd autoware</span><br><span class=\"line\">需要切换到awsim-stable分支下</span><br><span class=\"line\">git checkout awsim-stable</span><br></pre></td></tr></table></figure>\n<h1>安装主机依赖项</h1>\n<ol>\n<li>\n<p>CUDA</p>\n</li>\n<li>\n<p>Docker Engine</p>\n</li>\n<li>\n<p>NVIDIA Container Toolkit</p>\n</li>\n<li>\n<p>Rocker</p>\n</li>\n</ol>\n<p>你可以选择自己手动以此安装好上面的依赖项，也可以使用脚本的方式安装，在容器内已经都有了相关的环境，所以不需要安装了。在主机系统上安装的话可以以下使用脚本安装以上依赖</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./setup-dev-env.sh</span><br></pre></td></tr></table></figure>\n<h1>拉取镜像并启动</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br><span class=\"line\">rocker --nvidia --x11 --user --volume $HOME/AWSIM_v1.1.0 --volume $HOME/autoware --volume $HOME/autoware_map -- ghcr.io/autowarefoundation/autoware-universe:latest-cuda</span><br></pre></td></tr></table></figure>\n<p>创建src文件夹并克隆代码进来</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir src</span><br><span class=\"line\">vcs import src &lt; autoware.repos</span><br></pre></td></tr></table></figure>\n<h1>安装ROS依赖包</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source /opt/ros/humble/setup.bash</span><br><span class=\"line\">rosdep update</span><br><span class=\"line\">rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO</span><br></pre></td></tr></table></figure>\n<h1>编译工作空间</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=&quot;-w&quot;</span><br></pre></td></tr></table></figure>\n<h1>启动Autoware</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_sensor_kit map_path:=/home/xx/autoware_map/nishishinjuku_autoware_map</span><br></pre></td></tr></table></figure>\n<p><strong>BUGS</strong></p>\n<p>启动的时候，地图路径需要按照绝对路径填写，不能使用~/map_path这样的相对路径。</p>\n<p>上面的步骤都顺利的话，你将会看到rviz在你主机系统上出现以下窗口内容：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080127254.jpg\" alt=\"Image_2\"></p>\n<p>如果你可以看到在地图上有车辆和电云等初始位置的信息，那么你成功完成了autoware和仿真环境的关联。</p>\n<p>接下来你给定一个目标点，然后给出一个启动的信号如下指令：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd autoware</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 topic pub /autoware/engage autoware_auto_vehicle_msgs/msg/Engage &#x27;&#123;engage: True&#125;&#x27; -1</span><br></pre></td></tr></table></figure>\n<p>车辆将会自动运行到你设定的目标点位置。</p>\n<p>其中遇到的一些上述没有说的问题，可以参考以下链接去找解决方案，这里面包含了一些常见的问题。</p>\n<p><a href=\"https://tier4.github.io/AWSIM/DeveloperGuide/TroubleShooting/\"><strong>https://tier4.github.io/AWSIM/DeveloperGuide/TroubleShooting/</strong></a></p>\n<p>正确运行起来之后，我在笔记本GPU是2070-MAXQ和Ubuntu18.04上使用官方提供的镜像和awsim-stable代码进行演示，效果如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080128339.jpg\" alt=\"image-20230608012831843\"></p>\n<h1>参考引用</h1>\n<p><strong>参考官网链接</strong></p>\n<p><a href=\"https://tier4.github.io/AWSIM/GettingStarted/QuickStartDemo/\">Quick Start Demo - AWSIM document</a></p>\n"},{"title":"Docker网络代理配置","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-06-04T11:06:25.000Z","updated":"2023-06-04T11:06:25.000Z","keywords":"Docker, 网络代理","description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/walle.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***Docker网络代理配置***\n\n之前一直被Docker容器和镜像的科学上网代理所困惑，搞了好久没搞得很清楚，这次把使用经验记录下来，方便回看。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n------\n\n# Docker Pull时的代理\n\n这个代理也称为Dockerd 代理，\n\nDocker pull经常碰到拉取的镜像源在国外，导致要科学上网或者延迟高的问题，所以我们需要给docker配置网络代理，方便快速地拉取镜像到本地。（网速的快慢决定了程序员一天的幸福指数）执行如下指令，添加一个代理配置文件。\n\n```text\nsudo mkdir -p /etc/systemd/system/docker.service.d\nsudo touch /etc/systemd/system/docker.service.d/proxy.conf\n```\n\n然后需要修改proxy.conf文件中的代理服务器的内容\n\n```text\n[Service]\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:8080/\"\nEnvironment=\"HTTPS_PROXY=http://proxy.example.com:8080/\"\nEnvironment=\"NO_PROXY=localhost,127.0.0.1,.example.com\"\n```\n\n记住：这里需要配置的代理服务器可以使你本地的局域网内运行的代理服务器地址或者你本机上运行的代理服务器，比如Clash的本地服务端，并且是不需要密码的代理服务器。\n\n# Container 代理\n\n需要给容器运行时，内部有科学上网的需求的时候，配置容器内部的代理，这个代理将会在容器科学上网时起作用，是一种用户级别的，配置的文件是~/.docker/config.json，只在Docker17.07及以上版本生效，内容如下：\n\n```text\n{\n \"proxies\":\n {\n   \"default\":\n   {\n     \"httpProxy\": \"http://proxy.example.com:8080\",\n     \"httpsProxy\": \"http://proxy.example.com:8080\",\n     \"noProxy\": \"localhost,127.0.0.1,.example.com\"\n   }\n }\n}\n\n```\n\n`config.json` 非常方便，默认在所有配置修改后启动的容器生效，适合个人开发环境。但是在CI/CD的自动构建环境、或者实际上线运行的环境中，这种方法就不太合适，用 `-e` 注入这种显式配置会更好，减轻对构建、部署环境的依赖。\n\n# Docker Build 代理\n\n虽然 `docker build` 的本质，也是启动一个容器，但是环境会略有不同，用户级配置无效。在构建时，需要注入 `http_proxy` 等参数。\n\n```text\ndocker build . \\\n    --build-arg \"HTTP_PROXY=http://proxy.example.com:8080/\" \\\n    --build-arg \"HTTPS_PROXY=http://proxy.example.com:8080/\" \\\n    --build-arg \"NO_PROXY=localhost,127.0.0.1,.example.com\" \\\n    -t your/image:tag\n```\n\n需要注意的是：无论是 `docker run` 还是 `docker build`，默认是网络隔绝的。如果代理使用的是 `localhost:3128` 这类，则会无效。这类仅限本地的代理，必须加上 `--network host` 才能正常使用。而一般则需要配置代理的外部IP，而且代理本身要开启 Gateway 模式。\n\n# 重启生效配置\n\n`docker build` 代理是在执行前设置的，所以修改后，下次执行立即生效。Container 代理的修改也是立即生效的，但是**只针对以后启动的 Container，对已经启动的 Container 无效。**`dockerd` 代理的修改比较特殊，它实际上是改 `systemd` 的配置，因此需要重载 `systemd` 并重启 `dockerd` 才能生效。\n\n```text\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n```\n\n# 参考引用\n\n[如何优雅的给 Docker 配置网络代理_运维之美的博客-CSDN博客](https://blog.csdn.net/easylife206/article/details/114826425)\n\n","source":"_posts/009-Devops/01-Docker/Docker网络代理配置.md","raw":"---\ntitle: Docker网络代理配置\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-06-04 19:06:25\nupdated: 2023-06-04 19:06:25\ntags: Docker\ncategories: \n- Devops\n- Docker\nkeywords: Docker, 网络代理\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***Docker网络代理配置***\n\n之前一直被Docker容器和镜像的科学上网代理所困惑，搞了好久没搞得很清楚，这次把使用经验记录下来，方便回看。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n------\n\n# Docker Pull时的代理\n\n这个代理也称为Dockerd 代理，\n\nDocker pull经常碰到拉取的镜像源在国外，导致要科学上网或者延迟高的问题，所以我们需要给docker配置网络代理，方便快速地拉取镜像到本地。（网速的快慢决定了程序员一天的幸福指数）执行如下指令，添加一个代理配置文件。\n\n```text\nsudo mkdir -p /etc/systemd/system/docker.service.d\nsudo touch /etc/systemd/system/docker.service.d/proxy.conf\n```\n\n然后需要修改proxy.conf文件中的代理服务器的内容\n\n```text\n[Service]\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:8080/\"\nEnvironment=\"HTTPS_PROXY=http://proxy.example.com:8080/\"\nEnvironment=\"NO_PROXY=localhost,127.0.0.1,.example.com\"\n```\n\n记住：这里需要配置的代理服务器可以使你本地的局域网内运行的代理服务器地址或者你本机上运行的代理服务器，比如Clash的本地服务端，并且是不需要密码的代理服务器。\n\n# Container 代理\n\n需要给容器运行时，内部有科学上网的需求的时候，配置容器内部的代理，这个代理将会在容器科学上网时起作用，是一种用户级别的，配置的文件是~/.docker/config.json，只在Docker17.07及以上版本生效，内容如下：\n\n```text\n{\n \"proxies\":\n {\n   \"default\":\n   {\n     \"httpProxy\": \"http://proxy.example.com:8080\",\n     \"httpsProxy\": \"http://proxy.example.com:8080\",\n     \"noProxy\": \"localhost,127.0.0.1,.example.com\"\n   }\n }\n}\n\n```\n\n`config.json` 非常方便，默认在所有配置修改后启动的容器生效，适合个人开发环境。但是在CI/CD的自动构建环境、或者实际上线运行的环境中，这种方法就不太合适，用 `-e` 注入这种显式配置会更好，减轻对构建、部署环境的依赖。\n\n# Docker Build 代理\n\n虽然 `docker build` 的本质，也是启动一个容器，但是环境会略有不同，用户级配置无效。在构建时，需要注入 `http_proxy` 等参数。\n\n```text\ndocker build . \\\n    --build-arg \"HTTP_PROXY=http://proxy.example.com:8080/\" \\\n    --build-arg \"HTTPS_PROXY=http://proxy.example.com:8080/\" \\\n    --build-arg \"NO_PROXY=localhost,127.0.0.1,.example.com\" \\\n    -t your/image:tag\n```\n\n需要注意的是：无论是 `docker run` 还是 `docker build`，默认是网络隔绝的。如果代理使用的是 `localhost:3128` 这类，则会无效。这类仅限本地的代理，必须加上 `--network host` 才能正常使用。而一般则需要配置代理的外部IP，而且代理本身要开启 Gateway 模式。\n\n# 重启生效配置\n\n`docker build` 代理是在执行前设置的，所以修改后，下次执行立即生效。Container 代理的修改也是立即生效的，但是**只针对以后启动的 Container，对已经启动的 Container 无效。**`dockerd` 代理的修改比较特殊，它实际上是改 `systemd` 的配置，因此需要重载 `systemd` 并重启 `dockerd` 才能生效。\n\n```text\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n```\n\n# 参考引用\n\n[如何优雅的给 Docker 配置网络代理_运维之美的博客-CSDN博客](https://blog.csdn.net/easylife206/article/details/114826425)\n\n","slug":"009-Devops/01-Docker/Docker网络代理配置","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483t9001nu9rlhmolecum","content":"<h1><em><strong>Docker网络代理配置</strong></em></h1>\n<p>之前一直被Docker容器和镜像的科学上网代理所困惑，搞了好久没搞得很清楚，这次把使用经验记录下来，方便回看。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h1>Docker Pull时的代理</h1>\n<p>这个代理也称为Dockerd 代理，</p>\n<p>Docker pull经常碰到拉取的镜像源在国外，导致要科学上网或者延迟高的问题，所以我们需要给docker配置网络代理，方便快速地拉取镜像到本地。（网速的快慢决定了程序员一天的幸福指数）执行如下指令，添加一个代理配置文件。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mkdir -p /etc/systemd/system/docker.service.d</span><br><span class=\"line\">sudo touch /etc/systemd/system/docker.service.d/proxy.conf</span><br></pre></td></tr></table></figure>\n<p>然后需要修改proxy.conf文件中的代理服务器的内容</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Service]</span><br><span class=\"line\">Environment=&quot;HTTP_PROXY=http://proxy.example.com:8080/&quot;</span><br><span class=\"line\">Environment=&quot;HTTPS_PROXY=http://proxy.example.com:8080/&quot;</span><br><span class=\"line\">Environment=&quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot;</span><br></pre></td></tr></table></figure>\n<p>记住：这里需要配置的代理服务器可以使你本地的局域网内运行的代理服务器地址或者你本机上运行的代理服务器，比如Clash的本地服务端，并且是不需要密码的代理服务器。</p>\n<h1>Container 代理</h1>\n<p>需要给容器运行时，内部有科学上网的需求的时候，配置容器内部的代理，这个代理将会在容器科学上网时起作用，是一种用户级别的，配置的文件是~/.docker/config.json，只在Docker17.07及以上版本生效，内容如下：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\"> &quot;proxies&quot;:</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   &quot;default&quot;:</span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">     &quot;httpProxy&quot;: &quot;http://proxy.example.com:8080&quot;,</span><br><span class=\"line\">     &quot;httpsProxy&quot;: &quot;http://proxy.example.com:8080&quot;,</span><br><span class=\"line\">     &quot;noProxy&quot;: &quot;localhost,127.0.0.1,.example.com&quot;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><code>config.json</code> 非常方便，默认在所有配置修改后启动的容器生效，适合个人开发环境。但是在CI/CD的自动构建环境、或者实际上线运行的环境中，这种方法就不太合适，用 <code>-e</code> 注入这种显式配置会更好，减轻对构建、部署环境的依赖。</p>\n<h1>Docker Build 代理</h1>\n<p>虽然 <code>docker build</code> 的本质，也是启动一个容器，但是环境会略有不同，用户级配置无效。在构建时，需要注入 <code>http_proxy</code> 等参数。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker build . \\</span><br><span class=\"line\">    --build-arg &quot;HTTP_PROXY=http://proxy.example.com:8080/&quot; \\</span><br><span class=\"line\">    --build-arg &quot;HTTPS_PROXY=http://proxy.example.com:8080/&quot; \\</span><br><span class=\"line\">    --build-arg &quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot; \\</span><br><span class=\"line\">    -t your/image:tag</span><br></pre></td></tr></table></figure>\n<p>需要注意的是：无论是 <code>docker run</code> 还是 <code>docker build</code>，默认是网络隔绝的。如果代理使用的是 <code>localhost:3128</code> 这类，则会无效。这类仅限本地的代理，必须加上 <code>--network host</code> 才能正常使用。而一般则需要配置代理的外部IP，而且代理本身要开启 Gateway 模式。</p>\n<h1>重启生效配置</h1>\n<p><code>docker build</code> 代理是在执行前设置的，所以修改后，下次执行立即生效。Container 代理的修改也是立即生效的，但是<strong>只针对以后启动的 Container，对已经启动的 Container 无效。</strong><code>dockerd</code> 代理的修改比较特殊，它实际上是改 <code>systemd</code> 的配置，因此需要重载 <code>systemd</code> 并重启 <code>dockerd</code> 才能生效。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl daemon-reload</span><br><span class=\"line\">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>\n<h1>参考引用</h1>\n<p><a href=\"https://blog.csdn.net/easylife206/article/details/114826425\">如何优雅的给 Docker 配置网络代理_运维之美的博客-CSDN博客</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>Docker网络代理配置</strong></em></h1>\n<p>之前一直被Docker容器和镜像的科学上网代理所困惑，搞了好久没搞得很清楚，这次把使用经验记录下来，方便回看。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h1>Docker Pull时的代理</h1>\n<p>这个代理也称为Dockerd 代理，</p>\n<p>Docker pull经常碰到拉取的镜像源在国外，导致要科学上网或者延迟高的问题，所以我们需要给docker配置网络代理，方便快速地拉取镜像到本地。（网速的快慢决定了程序员一天的幸福指数）执行如下指令，添加一个代理配置文件。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mkdir -p /etc/systemd/system/docker.service.d</span><br><span class=\"line\">sudo touch /etc/systemd/system/docker.service.d/proxy.conf</span><br></pre></td></tr></table></figure>\n<p>然后需要修改proxy.conf文件中的代理服务器的内容</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Service]</span><br><span class=\"line\">Environment=&quot;HTTP_PROXY=http://proxy.example.com:8080/&quot;</span><br><span class=\"line\">Environment=&quot;HTTPS_PROXY=http://proxy.example.com:8080/&quot;</span><br><span class=\"line\">Environment=&quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot;</span><br></pre></td></tr></table></figure>\n<p>记住：这里需要配置的代理服务器可以使你本地的局域网内运行的代理服务器地址或者你本机上运行的代理服务器，比如Clash的本地服务端，并且是不需要密码的代理服务器。</p>\n<h1>Container 代理</h1>\n<p>需要给容器运行时，内部有科学上网的需求的时候，配置容器内部的代理，这个代理将会在容器科学上网时起作用，是一种用户级别的，配置的文件是~/.docker/config.json，只在Docker17.07及以上版本生效，内容如下：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\"> &quot;proxies&quot;:</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   &quot;default&quot;:</span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">     &quot;httpProxy&quot;: &quot;http://proxy.example.com:8080&quot;,</span><br><span class=\"line\">     &quot;httpsProxy&quot;: &quot;http://proxy.example.com:8080&quot;,</span><br><span class=\"line\">     &quot;noProxy&quot;: &quot;localhost,127.0.0.1,.example.com&quot;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><code>config.json</code> 非常方便，默认在所有配置修改后启动的容器生效，适合个人开发环境。但是在CI/CD的自动构建环境、或者实际上线运行的环境中，这种方法就不太合适，用 <code>-e</code> 注入这种显式配置会更好，减轻对构建、部署环境的依赖。</p>\n<h1>Docker Build 代理</h1>\n<p>虽然 <code>docker build</code> 的本质，也是启动一个容器，但是环境会略有不同，用户级配置无效。在构建时，需要注入 <code>http_proxy</code> 等参数。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker build . \\</span><br><span class=\"line\">    --build-arg &quot;HTTP_PROXY=http://proxy.example.com:8080/&quot; \\</span><br><span class=\"line\">    --build-arg &quot;HTTPS_PROXY=http://proxy.example.com:8080/&quot; \\</span><br><span class=\"line\">    --build-arg &quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot; \\</span><br><span class=\"line\">    -t your/image:tag</span><br></pre></td></tr></table></figure>\n<p>需要注意的是：无论是 <code>docker run</code> 还是 <code>docker build</code>，默认是网络隔绝的。如果代理使用的是 <code>localhost:3128</code> 这类，则会无效。这类仅限本地的代理，必须加上 <code>--network host</code> 才能正常使用。而一般则需要配置代理的外部IP，而且代理本身要开启 Gateway 模式。</p>\n<h1>重启生效配置</h1>\n<p><code>docker build</code> 代理是在执行前设置的，所以修改后，下次执行立即生效。Container 代理的修改也是立即生效的，但是<strong>只针对以后启动的 Container，对已经启动的 Container 无效。</strong><code>dockerd</code> 代理的修改比较特殊，它实际上是改 <code>systemd</code> 的配置，因此需要重载 <code>systemd</code> 并重启 <code>dockerd</code> 才能生效。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl daemon-reload</span><br><span class=\"line\">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>\n<h1>参考引用</h1>\n<p><a href=\"https://blog.csdn.net/easylife206/article/details/114826425\">如何优雅的给 Docker 配置网络代理_运维之美的博客-CSDN博客</a></p>\n"},{"title":"ARM平台安装Clash","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-06-08T15:29:42.000Z","updated":"2023-06-08T15:29:42.000Z","keywords":"Clash, ARM","description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/5f363d07a24be_270_185.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***ARM平台安装Clash***\n\n本文介绍如何在ARM平台上安装Clash，比如Jetson开发板、树莓派开发板等\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n# 背景介绍\n\n需要在ARM架构的主机上安装Clash，实现科学上网，因为经常需要访问些海外的资源，比如拉取docker镜像等等操作。\n\n\n\n# 安装步骤\n\n1. 下载Clash的二进制安装包\n\n安装包链接：[__传送门__](https://github.com/Dreamacro/clash/releases) 注意根据自己的操作系统的情况选择合适的二进制包。\n\n下载好后，将clash文件移动到/usr/local/bin下，给予权限。\n\n```text\n$sudo mv ./clash /usr/local/bin\n$sudo chmod a+x /usr/local/bin/clash\n```\n\n2. 编写自己的代理配置文件\n\n根据自己的平台选择对应的clash二进制安装包，比如armv7、armv8、amd64等等，代理的配置文件可以参考我的如下内容：\n\n```text\n# port of HTTP\nport: 7890\n\n## port of SOCKS5\nsocks-port: 7891\n\n# `allow-lan` must be true in your config.yml\nallow-lan: true\n\n# set log level to stdout (default is info)\n# info / warning / error / debug / silent\nlog-level: info\n\n# A RESTful API for clash\n#使用0.0.0.0可以使用局域网设备访问\nexternal-controller: 0.0.0.0:8080\n\nmode: Rule\n\nProxy:\n#以下省略，由梯子的服务商提供\n\n```\n\nConfig文件默认放在~/.config/clash这里，将你自己的机场提供的clash配置文件复制到这里就可以\n\n```text\nmkdir ~/.config/clash\nmv your/clash/config/file config.yaml\nmv config.yaml ~/.config/clash\n```\n\n除了配置文件，还需要一个全球IP库，Country.mmdb文件，可以实现各个国家的 IP 信息解析和地理定位，没有这个文件 clash 无法正常启动，会报错找不到，这个配置文件也放在默认路径下：~/.config/clash。\n\n3. 运行clash\n\n```text\n$ clash\n```\n\n\n\n我这里希望在Jetson Xavier NX上Clash，因为它的架构是ARMv8的，所以必须下载ARMv8的版本的clash安装包。\n\nARMv8: [__https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv8-v1.11.0.gz__](https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv8-v1.11.0.gz)\n\nARMv7: [__https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv7-v1.11.0.gz__](https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv7-v1.11.0.gz)\n\n\n\n\n\n# 参考引用\n\n[在树莓派上配置Clash-linux](https://zhuanlan.zhihu.com/p/56050058)\n\n[如何在树莓派上使用Clash](https://mraddict.top/posts/clash-on-rpi/)\n","source":"_posts/010-EdgeComputing/01-RaspberryPi/ARM平台安装Clash.md","raw":"---\ntitle: ARM平台安装Clash\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-06-08 23:29:42\nupdated: 2023-06-08 23:29:42\ntags: [ARM, RaspberryPi, Jetson, Clash]\ncategories:\n- EdgeComupting\n- RaspberryPi\nkeywords: Clash, ARM\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***ARM平台安装Clash***\n\n本文介绍如何在ARM平台上安装Clash，比如Jetson开发板、树莓派开发板等\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n# 背景介绍\n\n需要在ARM架构的主机上安装Clash，实现科学上网，因为经常需要访问些海外的资源，比如拉取docker镜像等等操作。\n\n\n\n# 安装步骤\n\n1. 下载Clash的二进制安装包\n\n安装包链接：[__传送门__](https://github.com/Dreamacro/clash/releases) 注意根据自己的操作系统的情况选择合适的二进制包。\n\n下载好后，将clash文件移动到/usr/local/bin下，给予权限。\n\n```text\n$sudo mv ./clash /usr/local/bin\n$sudo chmod a+x /usr/local/bin/clash\n```\n\n2. 编写自己的代理配置文件\n\n根据自己的平台选择对应的clash二进制安装包，比如armv7、armv8、amd64等等，代理的配置文件可以参考我的如下内容：\n\n```text\n# port of HTTP\nport: 7890\n\n## port of SOCKS5\nsocks-port: 7891\n\n# `allow-lan` must be true in your config.yml\nallow-lan: true\n\n# set log level to stdout (default is info)\n# info / warning / error / debug / silent\nlog-level: info\n\n# A RESTful API for clash\n#使用0.0.0.0可以使用局域网设备访问\nexternal-controller: 0.0.0.0:8080\n\nmode: Rule\n\nProxy:\n#以下省略，由梯子的服务商提供\n\n```\n\nConfig文件默认放在~/.config/clash这里，将你自己的机场提供的clash配置文件复制到这里就可以\n\n```text\nmkdir ~/.config/clash\nmv your/clash/config/file config.yaml\nmv config.yaml ~/.config/clash\n```\n\n除了配置文件，还需要一个全球IP库，Country.mmdb文件，可以实现各个国家的 IP 信息解析和地理定位，没有这个文件 clash 无法正常启动，会报错找不到，这个配置文件也放在默认路径下：~/.config/clash。\n\n3. 运行clash\n\n```text\n$ clash\n```\n\n\n\n我这里希望在Jetson Xavier NX上Clash，因为它的架构是ARMv8的，所以必须下载ARMv8的版本的clash安装包。\n\nARMv8: [__https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv8-v1.11.0.gz__](https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv8-v1.11.0.gz)\n\nARMv7: [__https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv7-v1.11.0.gz__](https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv7-v1.11.0.gz)\n\n\n\n\n\n# 参考引用\n\n[在树莓派上配置Clash-linux](https://zhuanlan.zhihu.com/p/56050058)\n\n[如何在树莓派上使用Clash](https://mraddict.top/posts/clash-on-rpi/)\n","slug":"010-EdgeComputing/01-RaspberryPi/ARM平台安装Clash","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483ta001pu9rl1ihs6dqj","content":"<h1><em><strong>ARM平台安装Clash</strong></em></h1>\n<p>本文介绍如何在ARM平台上安装Clash，比如Jetson开发板、树莓派开发板等</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>背景介绍</h1>\n<p>需要在ARM架构的主机上安装Clash，实现科学上网，因为经常需要访问些海外的资源，比如拉取docker镜像等等操作。</p>\n<h1>安装步骤</h1>\n<ol>\n<li>下载Clash的二进制安装包</li>\n</ol>\n<p>安装包链接：<a href=\"https://github.com/Dreamacro/clash/releases\"><strong>传送门</strong></a> 注意根据自己的操作系统的情况选择合适的二进制包。</p>\n<p>下载好后，将clash文件移动到/usr/local/bin下，给予权限。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$sudo mv ./clash /usr/local/bin</span><br><span class=\"line\">$sudo chmod a+x /usr/local/bin/clash</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>编写自己的代理配置文件</li>\n</ol>\n<p>根据自己的平台选择对应的clash二进制安装包，比如armv7、armv8、amd64等等，代理的配置文件可以参考我的如下内容：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># port of HTTP</span><br><span class=\"line\">port: 7890</span><br><span class=\"line\"></span><br><span class=\"line\">## port of SOCKS5</span><br><span class=\"line\">socks-port: 7891</span><br><span class=\"line\"></span><br><span class=\"line\"># `allow-lan` must be true in your config.yml</span><br><span class=\"line\">allow-lan: true</span><br><span class=\"line\"></span><br><span class=\"line\"># set log level to stdout (default is info)</span><br><span class=\"line\"># info / warning / error / debug / silent</span><br><span class=\"line\">log-level: info</span><br><span class=\"line\"></span><br><span class=\"line\"># A RESTful API for clash</span><br><span class=\"line\">#使用0.0.0.0可以使用局域网设备访问</span><br><span class=\"line\">external-controller: 0.0.0.0:8080</span><br><span class=\"line\"></span><br><span class=\"line\">mode: Rule</span><br><span class=\"line\"></span><br><span class=\"line\">Proxy:</span><br><span class=\"line\">#以下省略，由梯子的服务商提供</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Config文件默认放在~/.config/clash这里，将你自己的机场提供的clash配置文件复制到这里就可以</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/.config/clash</span><br><span class=\"line\">mv your/clash/config/file config.yaml</span><br><span class=\"line\">mv config.yaml ~/.config/clash</span><br></pre></td></tr></table></figure>\n<p>除了配置文件，还需要一个全球IP库，Country.mmdb文件，可以实现各个国家的 IP 信息解析和地理定位，没有这个文件 clash 无法正常启动，会报错找不到，这个配置文件也放在默认路径下：~/.config/clash。</p>\n<ol start=\"3\">\n<li>运行clash</li>\n</ol>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ clash</span><br></pre></td></tr></table></figure>\n<p>我这里希望在Jetson Xavier NX上Clash，因为它的架构是ARMv8的，所以必须下载ARMv8的版本的clash安装包。</p>\n<p>ARMv8: <a href=\"https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv8-v1.11.0.gz\"><strong>https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv8-v1.11.0.gz</strong></a></p>\n<p>ARMv7: <a href=\"https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv7-v1.11.0.gz\"><strong>https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv7-v1.11.0.gz</strong></a></p>\n<h1>参考引用</h1>\n<p><a href=\"https://zhuanlan.zhihu.com/p/56050058\">在树莓派上配置Clash-linux</a></p>\n<p><a href=\"https://mraddict.top/posts/clash-on-rpi/\">如何在树莓派上使用Clash</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>ARM平台安装Clash</strong></em></h1>\n<p>本文介绍如何在ARM平台上安装Clash，比如Jetson开发板、树莓派开发板等</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>背景介绍</h1>\n<p>需要在ARM架构的主机上安装Clash，实现科学上网，因为经常需要访问些海外的资源，比如拉取docker镜像等等操作。</p>\n<h1>安装步骤</h1>\n<ol>\n<li>下载Clash的二进制安装包</li>\n</ol>\n<p>安装包链接：<a href=\"https://github.com/Dreamacro/clash/releases\"><strong>传送门</strong></a> 注意根据自己的操作系统的情况选择合适的二进制包。</p>\n<p>下载好后，将clash文件移动到/usr/local/bin下，给予权限。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$sudo mv ./clash /usr/local/bin</span><br><span class=\"line\">$sudo chmod a+x /usr/local/bin/clash</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>编写自己的代理配置文件</li>\n</ol>\n<p>根据自己的平台选择对应的clash二进制安装包，比如armv7、armv8、amd64等等，代理的配置文件可以参考我的如下内容：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># port of HTTP</span><br><span class=\"line\">port: 7890</span><br><span class=\"line\"></span><br><span class=\"line\">## port of SOCKS5</span><br><span class=\"line\">socks-port: 7891</span><br><span class=\"line\"></span><br><span class=\"line\"># `allow-lan` must be true in your config.yml</span><br><span class=\"line\">allow-lan: true</span><br><span class=\"line\"></span><br><span class=\"line\"># set log level to stdout (default is info)</span><br><span class=\"line\"># info / warning / error / debug / silent</span><br><span class=\"line\">log-level: info</span><br><span class=\"line\"></span><br><span class=\"line\"># A RESTful API for clash</span><br><span class=\"line\">#使用0.0.0.0可以使用局域网设备访问</span><br><span class=\"line\">external-controller: 0.0.0.0:8080</span><br><span class=\"line\"></span><br><span class=\"line\">mode: Rule</span><br><span class=\"line\"></span><br><span class=\"line\">Proxy:</span><br><span class=\"line\">#以下省略，由梯子的服务商提供</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Config文件默认放在~/.config/clash这里，将你自己的机场提供的clash配置文件复制到这里就可以</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/.config/clash</span><br><span class=\"line\">mv your/clash/config/file config.yaml</span><br><span class=\"line\">mv config.yaml ~/.config/clash</span><br></pre></td></tr></table></figure>\n<p>除了配置文件，还需要一个全球IP库，Country.mmdb文件，可以实现各个国家的 IP 信息解析和地理定位，没有这个文件 clash 无法正常启动，会报错找不到，这个配置文件也放在默认路径下：~/.config/clash。</p>\n<ol start=\"3\">\n<li>运行clash</li>\n</ol>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ clash</span><br></pre></td></tr></table></figure>\n<p>我这里希望在Jetson Xavier NX上Clash，因为它的架构是ARMv8的，所以必须下载ARMv8的版本的clash安装包。</p>\n<p>ARMv8: <a href=\"https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv8-v1.11.0.gz\"><strong>https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv8-v1.11.0.gz</strong></a></p>\n<p>ARMv7: <a href=\"https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv7-v1.11.0.gz\"><strong>https://github.com/Dreamacro/clash/releases/download/v1.11.0/clash-linux-armv7-v1.11.0.gz</strong></a></p>\n<h1>参考引用</h1>\n<p><a href=\"https://zhuanlan.zhihu.com/p/56050058\">在树莓派上配置Clash-linux</a></p>\n<p><a href=\"https://mraddict.top/posts/clash-on-rpi/\">如何在树莓派上使用Clash</a></p>\n"},{"title":"TuringPi2安装Jetson-Xavier-NX模组","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-06-06T02:44:50.000Z","updated":"2023-06-06T02:44:50.000Z","keywords":"TuringPi2 Jetson-Xavier","description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/gkihqEjXxJ5UZ1C.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***TuringPi2安装Jetson-Xavier-NX模组***\n\nTuringPi2安装Xavier-NX模组的个人经验描述，不具有官方的正确性，但是可以完成目标。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请注明出处，感谢！\n\n---\n\n# 安装注意事项：\n\n1. 目前在我尝试了多次直接在板载上刷写系统的情况下，并无法直接通过Turing Pi2上的USB2.0的口成功完成系统刷写后的**启动操作**。按我的理解，因为Xavier刷写系统完成之后，启动的时候，USB2.0就不能一直是Device模式，需要切换成Host模式。\n\n1. 目前我将Xavier NX模组安装在Node1的位置，因为只有Node1可以连接HDMI的输出接口，因为刷写系统后，我需要可视化界面安装和操作一些指令。\n\n1. 由于USB2.0的接口只能用在系统刷写，我们无法再Node1上使用无线键鼠，所以我们需要提前将一些必要的软件装上，比如Nomachine\\Todesk之类的远程桌面软件，这样可以忽略显示器输出和键鼠的问题。\n\n\n\n# 安装前的工作\n\n1. 安装M.2硬盘\n\n如果需要使用M.2的硬盘，在刷写系统之前，需要安装好硬盘，然后将系统刷写在NVME的硬盘上，再SDKManager上有nvme的选项，这样系统将会刷写在固态硬盘上，启动的时候从固态硬盘启动。由于Xavier NX开发者套件的模组是没有EMMC的，必须要插上一个SD卡才能正常刷写系统，所以需要安装一个SD卡。同时由于Xavier NX在SD卡上刷写系统后，性能以及存储空间不够的问题，我们优先选择将系统刷写在NVME的固态硬盘上，这样系统性能以及存储空间都满足后期的要求。\n\n**开发者套件底板**\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061532390.jpg\" alt=\"42c00ea160cf31f1475bc223de92796\" style=\"zoom:50%;\" />\n\n\n\n2. 刷写Jetpack系统\n\n由于以上解释的问题和原因，我们需要提前将Xavier NX模组进行系统刷写和必要软件安装，我这里采用Deveploer Kit的底板安装好模组之后，直接通过SDKManager进行系统刷写和Jetson相关的软件安装。\n\n**SDKManager刷系统界面**\n\n![1685984312(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061540638.jpg)\n\n# 安装Xavier-NX模组\n\n## 选择合适的Node位置\n\n在Turing Pi2上安装Xavier模组，需要提前考虑后期的使用场景中，需要用到哪些常用的接口，比如USB3.0、M.2、HDMI等，这样根据不同的Node功能接口特点，选择你需要安装的位置。\n\n我这里选择放在Node1的位置，主要是我需要Xavier-NX后期做一些显示输出的工作，比如仿真模型、UI界面等，Node1接口包含了一个HDMI接口。\n\n### **安装所选的Node位置示意图**\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061540863.jpg\" alt=\"bdecb376f540886ba8e5e9cd4c08fc6\" style=\"zoom:50%;\" />\n\n## 安装硬件及硬盘\n\n因为我们使用了NVME固态硬盘，我们需要根据选择Node位置，安装在对应的背面上SLOT的位置，Node1对应Slot1，以此类推，所以我这里安装在Slot1上，这样我们启动之后，Node1位置上安装的Xavier模组才能够从M.2固态硬盘启动。\n\n### **硬盘对应Node安装示意图**\n\n![803586cd0d24d4c59c14d926f528e7e](https://www.synotech.top:5523/uploads/2023/06/06/202306061541481.jpg)\n\n## Xavier模组的散热排线\n\n一定要将Xavier模组的散热排线接口接在Turing Pi2上的对应接口，这个供电接口在Node的插槽位置旁边，4个针脚的白色方形口。排线延长线默认发货的时候提供了，因为Xavier模组自带的风扇供电线长度比较短，一般需要使用到这个供电延长线。\n\n### **散热延长排线安装示意图**\n\n![336458f3eb3b94829121d391352a306](https://www.synotech.top:5523/uploads/2023/06/06/202306061541510.jpg)\n\n***\n\n### 引用文献：\n","source":"_posts/011-ClusterSuperComputing/01-TuringPi/TuringPi2安装Jetson-Xavier-NX模组.md","raw":"---\ntitle: TuringPi2安装Jetson-Xavier-NX模组\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-06-06 10:44:50\nupdated: 2023-06-06 10:44:50\ntags: [TuringPi2, Jetson]\ncategories: \n- 集群超算\n- TuringPi\nkeywords: TuringPi2 Jetson-Xavier\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***TuringPi2安装Jetson-Xavier-NX模组***\n\nTuringPi2安装Xavier-NX模组的个人经验描述，不具有官方的正确性，但是可以完成目标。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请注明出处，感谢！\n\n---\n\n# 安装注意事项：\n\n1. 目前在我尝试了多次直接在板载上刷写系统的情况下，并无法直接通过Turing Pi2上的USB2.0的口成功完成系统刷写后的**启动操作**。按我的理解，因为Xavier刷写系统完成之后，启动的时候，USB2.0就不能一直是Device模式，需要切换成Host模式。\n\n1. 目前我将Xavier NX模组安装在Node1的位置，因为只有Node1可以连接HDMI的输出接口，因为刷写系统后，我需要可视化界面安装和操作一些指令。\n\n1. 由于USB2.0的接口只能用在系统刷写，我们无法再Node1上使用无线键鼠，所以我们需要提前将一些必要的软件装上，比如Nomachine\\Todesk之类的远程桌面软件，这样可以忽略显示器输出和键鼠的问题。\n\n\n\n# 安装前的工作\n\n1. 安装M.2硬盘\n\n如果需要使用M.2的硬盘，在刷写系统之前，需要安装好硬盘，然后将系统刷写在NVME的硬盘上，再SDKManager上有nvme的选项，这样系统将会刷写在固态硬盘上，启动的时候从固态硬盘启动。由于Xavier NX开发者套件的模组是没有EMMC的，必须要插上一个SD卡才能正常刷写系统，所以需要安装一个SD卡。同时由于Xavier NX在SD卡上刷写系统后，性能以及存储空间不够的问题，我们优先选择将系统刷写在NVME的固态硬盘上，这样系统性能以及存储空间都满足后期的要求。\n\n**开发者套件底板**\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061532390.jpg\" alt=\"42c00ea160cf31f1475bc223de92796\" style=\"zoom:50%;\" />\n\n\n\n2. 刷写Jetpack系统\n\n由于以上解释的问题和原因，我们需要提前将Xavier NX模组进行系统刷写和必要软件安装，我这里采用Deveploer Kit的底板安装好模组之后，直接通过SDKManager进行系统刷写和Jetson相关的软件安装。\n\n**SDKManager刷系统界面**\n\n![1685984312(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061540638.jpg)\n\n# 安装Xavier-NX模组\n\n## 选择合适的Node位置\n\n在Turing Pi2上安装Xavier模组，需要提前考虑后期的使用场景中，需要用到哪些常用的接口，比如USB3.0、M.2、HDMI等，这样根据不同的Node功能接口特点，选择你需要安装的位置。\n\n我这里选择放在Node1的位置，主要是我需要Xavier-NX后期做一些显示输出的工作，比如仿真模型、UI界面等，Node1接口包含了一个HDMI接口。\n\n### **安装所选的Node位置示意图**\n\n<img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061540863.jpg\" alt=\"bdecb376f540886ba8e5e9cd4c08fc6\" style=\"zoom:50%;\" />\n\n## 安装硬件及硬盘\n\n因为我们使用了NVME固态硬盘，我们需要根据选择Node位置，安装在对应的背面上SLOT的位置，Node1对应Slot1，以此类推，所以我这里安装在Slot1上，这样我们启动之后，Node1位置上安装的Xavier模组才能够从M.2固态硬盘启动。\n\n### **硬盘对应Node安装示意图**\n\n![803586cd0d24d4c59c14d926f528e7e](https://www.synotech.top:5523/uploads/2023/06/06/202306061541481.jpg)\n\n## Xavier模组的散热排线\n\n一定要将Xavier模组的散热排线接口接在Turing Pi2上的对应接口，这个供电接口在Node的插槽位置旁边，4个针脚的白色方形口。排线延长线默认发货的时候提供了，因为Xavier模组自带的风扇供电线长度比较短，一般需要使用到这个供电延长线。\n\n### **散热延长排线安装示意图**\n\n![336458f3eb3b94829121d391352a306](https://www.synotech.top:5523/uploads/2023/06/06/202306061541510.jpg)\n\n***\n\n### 引用文献：\n","slug":"011-ClusterSuperComputing/01-TuringPi/TuringPi2安装Jetson-Xavier-NX模组","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483tb001su9rl0n2iapug","content":"<h1><em><strong>TuringPi2安装Jetson-Xavier-NX模组</strong></em></h1>\n<p>TuringPi2安装Xavier-NX模组的个人经验描述，不具有官方的正确性，但是可以完成目标。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请注明出处，感谢！</p>\n<hr>\n<h1>安装注意事项：</h1>\n<ol>\n<li>\n<p>目前在我尝试了多次直接在板载上刷写系统的情况下，并无法直接通过Turing Pi2上的USB2.0的口成功完成系统刷写后的<strong>启动操作</strong>。按我的理解，因为Xavier刷写系统完成之后，启动的时候，USB2.0就不能一直是Device模式，需要切换成Host模式。</p>\n</li>\n<li>\n<p>目前我将Xavier NX模组安装在Node1的位置，因为只有Node1可以连接HDMI的输出接口，因为刷写系统后，我需要可视化界面安装和操作一些指令。</p>\n</li>\n<li>\n<p>由于USB2.0的接口只能用在系统刷写，我们无法再Node1上使用无线键鼠，所以我们需要提前将一些必要的软件装上，比如Nomachine\\Todesk之类的远程桌面软件，这样可以忽略显示器输出和键鼠的问题。</p>\n</li>\n</ol>\n<h1>安装前的工作</h1>\n<ol>\n<li>安装M.2硬盘</li>\n</ol>\n<p>如果需要使用M.2的硬盘，在刷写系统之前，需要安装好硬盘，然后将系统刷写在NVME的硬盘上，再SDKManager上有nvme的选项，这样系统将会刷写在固态硬盘上，启动的时候从固态硬盘启动。由于Xavier NX开发者套件的模组是没有EMMC的，必须要插上一个SD卡才能正常刷写系统，所以需要安装一个SD卡。同时由于Xavier NX在SD卡上刷写系统后，性能以及存储空间不够的问题，我们优先选择将系统刷写在NVME的固态硬盘上，这样系统性能以及存储空间都满足后期的要求。</p>\n<p><strong>开发者套件底板</strong></p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061532390.jpg\" alt=\"42c00ea160cf31f1475bc223de92796\" style=\"zoom:50%;\">\n<ol start=\"2\">\n<li>刷写Jetpack系统</li>\n</ol>\n<p>由于以上解释的问题和原因，我们需要提前将Xavier NX模组进行系统刷写和必要软件安装，我这里采用Deveploer Kit的底板安装好模组之后，直接通过SDKManager进行系统刷写和Jetson相关的软件安装。</p>\n<p><strong>SDKManager刷系统界面</strong></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061540638.jpg\" alt=\"1685984312(1)\"></p>\n<h1>安装Xavier-NX模组</h1>\n<h2 id=\"选择合适的Node位置\">选择合适的Node位置</h2>\n<p>在Turing Pi2上安装Xavier模组，需要提前考虑后期的使用场景中，需要用到哪些常用的接口，比如USB3.0、M.2、HDMI等，这样根据不同的Node功能接口特点，选择你需要安装的位置。</p>\n<p>我这里选择放在Node1的位置，主要是我需要Xavier-NX后期做一些显示输出的工作，比如仿真模型、UI界面等，Node1接口包含了一个HDMI接口。</p>\n<h3 id=\"安装所选的Node位置示意图\"><strong>安装所选的Node位置示意图</strong></h3>\n<img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061540863.jpg\" alt=\"bdecb376f540886ba8e5e9cd4c08fc6\" style=\"zoom:50%;\">\n<h2 id=\"安装硬件及硬盘\">安装硬件及硬盘</h2>\n<p>因为我们使用了NVME固态硬盘，我们需要根据选择Node位置，安装在对应的背面上SLOT的位置，Node1对应Slot1，以此类推，所以我这里安装在Slot1上，这样我们启动之后，Node1位置上安装的Xavier模组才能够从M.2固态硬盘启动。</p>\n<h3 id=\"硬盘对应Node安装示意图\"><strong>硬盘对应Node安装示意图</strong></h3>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061541481.jpg\" alt=\"803586cd0d24d4c59c14d926f528e7e\"></p>\n<h2 id=\"Xavier模组的散热排线\">Xavier模组的散热排线</h2>\n<p>一定要将Xavier模组的散热排线接口接在Turing Pi2上的对应接口，这个供电接口在Node的插槽位置旁边，4个针脚的白色方形口。排线延长线默认发货的时候提供了，因为Xavier模组自带的风扇供电线长度比较短，一般需要使用到这个供电延长线。</p>\n<h3 id=\"散热延长排线安装示意图\"><strong>散热延长排线安装示意图</strong></h3>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061541510.jpg\" alt=\"336458f3eb3b94829121d391352a306\"></p>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>TuringPi2安装Jetson-Xavier-NX模组</strong></em></h1>\n<p>TuringPi2安装Xavier-NX模组的个人经验描述，不具有官方的正确性，但是可以完成目标。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请注明出处，感谢！</p>\n<hr>\n<h1>安装注意事项：</h1>\n<ol>\n<li>\n<p>目前在我尝试了多次直接在板载上刷写系统的情况下，并无法直接通过Turing Pi2上的USB2.0的口成功完成系统刷写后的<strong>启动操作</strong>。按我的理解，因为Xavier刷写系统完成之后，启动的时候，USB2.0就不能一直是Device模式，需要切换成Host模式。</p>\n</li>\n<li>\n<p>目前我将Xavier NX模组安装在Node1的位置，因为只有Node1可以连接HDMI的输出接口，因为刷写系统后，我需要可视化界面安装和操作一些指令。</p>\n</li>\n<li>\n<p>由于USB2.0的接口只能用在系统刷写，我们无法再Node1上使用无线键鼠，所以我们需要提前将一些必要的软件装上，比如Nomachine\\Todesk之类的远程桌面软件，这样可以忽略显示器输出和键鼠的问题。</p>\n</li>\n</ol>\n<h1>安装前的工作</h1>\n<ol>\n<li>安装M.2硬盘</li>\n</ol>\n<p>如果需要使用M.2的硬盘，在刷写系统之前，需要安装好硬盘，然后将系统刷写在NVME的硬盘上，再SDKManager上有nvme的选项，这样系统将会刷写在固态硬盘上，启动的时候从固态硬盘启动。由于Xavier NX开发者套件的模组是没有EMMC的，必须要插上一个SD卡才能正常刷写系统，所以需要安装一个SD卡。同时由于Xavier NX在SD卡上刷写系统后，性能以及存储空间不够的问题，我们优先选择将系统刷写在NVME的固态硬盘上，这样系统性能以及存储空间都满足后期的要求。</p>\n<p><strong>开发者套件底板</strong></p>\n<img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061532390.jpg\" alt=\"42c00ea160cf31f1475bc223de92796\" style=\"zoom:50%;\">\n<ol start=\"2\">\n<li>刷写Jetpack系统</li>\n</ol>\n<p>由于以上解释的问题和原因，我们需要提前将Xavier NX模组进行系统刷写和必要软件安装，我这里采用Deveploer Kit的底板安装好模组之后，直接通过SDKManager进行系统刷写和Jetson相关的软件安装。</p>\n<p><strong>SDKManager刷系统界面</strong></p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061540638.jpg\" alt=\"1685984312(1)\"></p>\n<h1>安装Xavier-NX模组</h1>\n<h2 id=\"选择合适的Node位置\">选择合适的Node位置</h2>\n<p>在Turing Pi2上安装Xavier模组，需要提前考虑后期的使用场景中，需要用到哪些常用的接口，比如USB3.0、M.2、HDMI等，这样根据不同的Node功能接口特点，选择你需要安装的位置。</p>\n<p>我这里选择放在Node1的位置，主要是我需要Xavier-NX后期做一些显示输出的工作，比如仿真模型、UI界面等，Node1接口包含了一个HDMI接口。</p>\n<h3 id=\"安装所选的Node位置示意图\"><strong>安装所选的Node位置示意图</strong></h3>\n<img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061540863.jpg\" alt=\"bdecb376f540886ba8e5e9cd4c08fc6\" style=\"zoom:50%;\">\n<h2 id=\"安装硬件及硬盘\">安装硬件及硬盘</h2>\n<p>因为我们使用了NVME固态硬盘，我们需要根据选择Node位置，安装在对应的背面上SLOT的位置，Node1对应Slot1，以此类推，所以我这里安装在Slot1上，这样我们启动之后，Node1位置上安装的Xavier模组才能够从M.2固态硬盘启动。</p>\n<h3 id=\"硬盘对应Node安装示意图\"><strong>硬盘对应Node安装示意图</strong></h3>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061541481.jpg\" alt=\"803586cd0d24d4c59c14d926f528e7e\"></p>\n<h2 id=\"Xavier模组的散热排线\">Xavier模组的散热排线</h2>\n<p>一定要将Xavier模组的散热排线接口接在Turing Pi2上的对应接口，这个供电接口在Node的插槽位置旁边，4个针脚的白色方形口。排线延长线默认发货的时候提供了，因为Xavier模组自带的风扇供电线长度比较短，一般需要使用到这个供电延长线。</p>\n<h3 id=\"散热延长排线安装示意图\"><strong>散热延长排线安装示意图</strong></h3>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061541510.jpg\" alt=\"336458f3eb3b94829121d391352a306\"></p>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n"},{"title":"Vector机器人运行SDK-Examples","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-06-07T15:39:52.000Z","updated":"2023-06-07T15:39:52.000Z","keywords":"Vector","description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/SNN.jpeg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***Vector机器人运行SDK-Examples***\n\n介绍并演示使用Vector机器人提供的一些SDK的案例代码，执行之后的效果。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n# 启动3d_viewer.py脚本\n\n![3d_view](https://www.synotech.top:5523/uploads/2023/06/08/202306080007457.jpg)\n\n这个脚本将会创建一个图像窗口和3D OpenGL的窗口，显示了当前机器人通过一线Laser激光扫描创建的地图，同时在图像窗口，如果看到CUBE的话，会显示检测的CUBE信息，并画出矩形框。\n\n# 17_create_wall.py\n\n运行Demo17, 将会在机器人的正前方100mm处创建一堵墙，然后下指令让机器人穿过墙，前往机器人正前方200mm处。\n\n代码如下：\n\n![image](https://www.synotech.top:5523/uploads/2023/06/07/202306072349464.jpg)\n\n3d_viewer创建可视化情况如下：\n\n![3d_viewer](https://www.synotech.top:5523/uploads/2023/06/07/202306072350142.jpg)\n\n机器人绕过前方虚拟的墙，然后到达距离机器人正前方200mm处。\n\n***\n\n### 引用文献：\n","source":"_posts/012-IntelligentAIRobots/01-VectorRobots/Vector机器人运行SDK-Examples.md","raw":"---\ntitle: Vector机器人运行SDK-Examples\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-06-07 23:39:52\nupdated: 2023-06-07 23:39:52\ntags: Vector\ncategories:\n- 智能AI机器人\n- VectorRobots\nkeywords: Vector\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***Vector机器人运行SDK-Examples***\n\n介绍并演示使用Vector机器人提供的一些SDK的案例代码，执行之后的效果。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！\n\n---\n\n# 启动3d_viewer.py脚本\n\n![3d_view](https://www.synotech.top:5523/uploads/2023/06/08/202306080007457.jpg)\n\n这个脚本将会创建一个图像窗口和3D OpenGL的窗口，显示了当前机器人通过一线Laser激光扫描创建的地图，同时在图像窗口，如果看到CUBE的话，会显示检测的CUBE信息，并画出矩形框。\n\n# 17_create_wall.py\n\n运行Demo17, 将会在机器人的正前方100mm处创建一堵墙，然后下指令让机器人穿过墙，前往机器人正前方200mm处。\n\n代码如下：\n\n![image](https://www.synotech.top:5523/uploads/2023/06/07/202306072349464.jpg)\n\n3d_viewer创建可视化情况如下：\n\n![3d_viewer](https://www.synotech.top:5523/uploads/2023/06/07/202306072350142.jpg)\n\n机器人绕过前方虚拟的墙，然后到达距离机器人正前方200mm处。\n\n***\n\n### 引用文献：\n","slug":"012-IntelligentAIRobots/01-VectorRobots/Vector机器人运行SDK-Examples","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483tc001wu9rl2mu3fqmv","content":"<h1><em><strong>Vector机器人运行SDK-Examples</strong></em></h1>\n<p>介绍并演示使用Vector机器人提供的一些SDK的案例代码，执行之后的效果。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>启动3d_viewer.py脚本</h1>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080007457.jpg\" alt=\"3d_view\"></p>\n<p>这个脚本将会创建一个图像窗口和3D OpenGL的窗口，显示了当前机器人通过一线Laser激光扫描创建的地图，同时在图像窗口，如果看到CUBE的话，会显示检测的CUBE信息，并画出矩形框。</p>\n<h1>17_create_wall.py</h1>\n<p>运行Demo17, 将会在机器人的正前方100mm处创建一堵墙，然后下指令让机器人穿过墙，前往机器人正前方200mm处。</p>\n<p>代码如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/07/202306072349464.jpg\" alt=\"image\"></p>\n<p>3d_viewer创建可视化情况如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/07/202306072350142.jpg\" alt=\"3d_viewer\"></p>\n<p>机器人绕过前方虚拟的墙，然后到达距离机器人正前方200mm处。</p>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>Vector机器人运行SDK-Examples</strong></em></h1>\n<p>介绍并演示使用Vector机器人提供的一些SDK的案例代码，执行之后的效果。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，转载请说明出处引用，感谢！</p>\n<hr>\n<h1>启动3d_viewer.py脚本</h1>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/08/202306080007457.jpg\" alt=\"3d_view\"></p>\n<p>这个脚本将会创建一个图像窗口和3D OpenGL的窗口，显示了当前机器人通过一线Laser激光扫描创建的地图，同时在图像窗口，如果看到CUBE的话，会显示检测的CUBE信息，并画出矩形框。</p>\n<h1>17_create_wall.py</h1>\n<p>运行Demo17, 将会在机器人的正前方100mm处创建一堵墙，然后下指令让机器人穿过墙，前往机器人正前方200mm处。</p>\n<p>代码如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/07/202306072349464.jpg\" alt=\"image\"></p>\n<p>3d_viewer创建可视化情况如下：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/07/202306072350142.jpg\" alt=\"3d_viewer\"></p>\n<p>机器人绕过前方虚拟的墙，然后到达距离机器人正前方200mm处。</p>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n"},{"title":"如何在Ubuntu18.04系统上操作Vector机器人","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-06-06T03:15:32.000Z","updated":"2023-06-06T03:15:32.000Z","keywords":"Vector","description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/SNN.jpeg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***如何在Ubuntu18.04系统上操 作Vector机器人***\n\n本篇主要介绍如何在Ubuntu18系统上安装Vector SDK并在局域网内成功连接上机器人，读取传感器信息以及下发指令操作机器人运动。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n# 安装SDK \n\n```text\n#如果已经安装了Python3，则跳过这里\nsudo apt-get update\nsudo apt-get install python3\nsudo apt install python3-pip\n\n#\nsudo apt-get install python3-pil.imagetk\npython3 -m pip install --user anki_vector\n\n#如果已经安装过SDK，则可以直接升级\npython3 -m pip install --user --upgrade anki_vector\n\n#这一步完成Vector的认证\npython3 -m anki_vector.configure\n```\n\n# 配置Vector信息\n\n窗口提示配置信息和隐私策略等\n\n![image](https://www.synotech.top:5523/uploads/2023/06/06/202306061548371.jpg)\n\n然后输入你的robot name, 比如：Vector-A1B2\n\n![1681308582(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061549087.jpg)\n\n输入你的机器人在局域网内wifi分配的IP地址，比如：192.168.42.42\n\n![1681288567(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061549237.jpg)\n\n输入你的机器人的序列号，比如：00e20100\n\n![1681288477(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061549830.jpg)\n\n根据官网信息，配置好了自己的Vector账号之后，可以看到Linux终端输出了SUCCESS！的信息，则配置成功。\n\n![1681288650(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061550564.jpg)\n\n细心观察，可以看到这里连接到的机器人主机是通过443端口的。并且对你的认证秘钥以及sdk的配置都保存在你本地的`.anki_vector/`的路径下了。\n\n如果你成功运行到这里，看到输出，那么你已经完成了机器人和你的Ubuntu主机在局域网内连通的配置。\n\n***\n\n### 引用文献：\n\n[Installation - Linux — Vector SDK 0.6.0 documentation](https://developer.anki.com/vector/docs/install-linux.html)\n","source":"_posts/012-IntelligentAIRobots/01-VectorRobots/如何在Ubuntu18-04系统上操作Vector机器人.md","raw":"---\ntitle: 如何在Ubuntu18.04系统上操作Vector机器人\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-06-06 11:15:32\nupdated: 2023-06-06 11:15:32\ntags: Vector\ncategories: \n- 智能AI机器人\n- VectorRobots\nkeywords: Vector\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***如何在Ubuntu18.04系统上操 作Vector机器人***\n\n本篇主要介绍如何在Ubuntu18系统上安装Vector SDK并在局域网内成功连接上机器人，读取传感器信息以及下发指令操作机器人运动。\n\n## ***写在前面：***\n\n本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！\n\n---\n\n# 安装SDK \n\n```text\n#如果已经安装了Python3，则跳过这里\nsudo apt-get update\nsudo apt-get install python3\nsudo apt install python3-pip\n\n#\nsudo apt-get install python3-pil.imagetk\npython3 -m pip install --user anki_vector\n\n#如果已经安装过SDK，则可以直接升级\npython3 -m pip install --user --upgrade anki_vector\n\n#这一步完成Vector的认证\npython3 -m anki_vector.configure\n```\n\n# 配置Vector信息\n\n窗口提示配置信息和隐私策略等\n\n![image](https://www.synotech.top:5523/uploads/2023/06/06/202306061548371.jpg)\n\n然后输入你的robot name, 比如：Vector-A1B2\n\n![1681308582(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061549087.jpg)\n\n输入你的机器人在局域网内wifi分配的IP地址，比如：192.168.42.42\n\n![1681288567(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061549237.jpg)\n\n输入你的机器人的序列号，比如：00e20100\n\n![1681288477(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061549830.jpg)\n\n根据官网信息，配置好了自己的Vector账号之后，可以看到Linux终端输出了SUCCESS！的信息，则配置成功。\n\n![1681288650(1)](https://www.synotech.top:5523/uploads/2023/06/06/202306061550564.jpg)\n\n细心观察，可以看到这里连接到的机器人主机是通过443端口的。并且对你的认证秘钥以及sdk的配置都保存在你本地的`.anki_vector/`的路径下了。\n\n如果你成功运行到这里，看到输出，那么你已经完成了机器人和你的Ubuntu主机在局域网内连通的配置。\n\n***\n\n### 引用文献：\n\n[Installation - Linux — Vector SDK 0.6.0 documentation](https://developer.anki.com/vector/docs/install-linux.html)\n","slug":"012-IntelligentAIRobots/01-VectorRobots/如何在Ubuntu18-04系统上操作Vector机器人","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483td0020u9rldo1ag5b1","content":"<h1><em><strong>如何在Ubuntu18.04系统上操 作Vector机器人</strong></em></h1>\n<p>本篇主要介绍如何在Ubuntu18系统上安装Vector SDK并在局域网内成功连接上机器人，读取传感器信息以及下发指令操作机器人运动。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h1>安装SDK</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#如果已经安装了Python3，则跳过这里</span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get install python3</span><br><span class=\"line\">sudo apt install python3-pip</span><br><span class=\"line\"></span><br><span class=\"line\">#</span><br><span class=\"line\">sudo apt-get install python3-pil.imagetk</span><br><span class=\"line\">python3 -m pip install --user anki_vector</span><br><span class=\"line\"></span><br><span class=\"line\">#如果已经安装过SDK，则可以直接升级</span><br><span class=\"line\">python3 -m pip install --user --upgrade anki_vector</span><br><span class=\"line\"></span><br><span class=\"line\">#这一步完成Vector的认证</span><br><span class=\"line\">python3 -m anki_vector.configure</span><br></pre></td></tr></table></figure>\n<h1>配置Vector信息</h1>\n<p>窗口提示配置信息和隐私策略等</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061548371.jpg\" alt=\"image\"></p>\n<p>然后输入你的robot name, 比如：Vector-A1B2</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061549087.jpg\" alt=\"1681308582(1)\"></p>\n<p>输入你的机器人在局域网内wifi分配的IP地址，比如：192.168.42.42</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061549237.jpg\" alt=\"1681288567(1)\"></p>\n<p>输入你的机器人的序列号，比如：00e20100</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061549830.jpg\" alt=\"1681288477(1)\"></p>\n<p>根据官网信息，配置好了自己的Vector账号之后，可以看到Linux终端输出了SUCCESS！的信息，则配置成功。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061550564.jpg\" alt=\"1681288650(1)\"></p>\n<p>细心观察，可以看到这里连接到的机器人主机是通过443端口的。并且对你的认证秘钥以及sdk的配置都保存在你本地的<code>.anki_vector/</code>的路径下了。</p>\n<p>如果你成功运行到这里，看到输出，那么你已经完成了机器人和你的Ubuntu主机在局域网内连通的配置。</p>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p><a href=\"https://developer.anki.com/vector/docs/install-linux.html\">Installation - Linux — Vector SDK 0.6.0 documentation</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>如何在Ubuntu18.04系统上操 作Vector机器人</strong></em></h1>\n<p>本篇主要介绍如何在Ubuntu18系统上安装Vector SDK并在局域网内成功连接上机器人，读取传感器信息以及下发指令操作机器人运动。</p>\n<h2 id=\"写在前面：\"><em><strong>写在前面：</strong></em></h2>\n<p>本博客属于个人学习笔记，希望通过文章记录，规范整理自己的学习内容，方便对知识复习和分享。如果有错误的地方，还请指出，同时转载请说明出处，感谢！</p>\n<hr>\n<h1>安装SDK</h1>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#如果已经安装了Python3，则跳过这里</span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get install python3</span><br><span class=\"line\">sudo apt install python3-pip</span><br><span class=\"line\"></span><br><span class=\"line\">#</span><br><span class=\"line\">sudo apt-get install python3-pil.imagetk</span><br><span class=\"line\">python3 -m pip install --user anki_vector</span><br><span class=\"line\"></span><br><span class=\"line\">#如果已经安装过SDK，则可以直接升级</span><br><span class=\"line\">python3 -m pip install --user --upgrade anki_vector</span><br><span class=\"line\"></span><br><span class=\"line\">#这一步完成Vector的认证</span><br><span class=\"line\">python3 -m anki_vector.configure</span><br></pre></td></tr></table></figure>\n<h1>配置Vector信息</h1>\n<p>窗口提示配置信息和隐私策略等</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061548371.jpg\" alt=\"image\"></p>\n<p>然后输入你的robot name, 比如：Vector-A1B2</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061549087.jpg\" alt=\"1681308582(1)\"></p>\n<p>输入你的机器人在局域网内wifi分配的IP地址，比如：192.168.42.42</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061549237.jpg\" alt=\"1681288567(1)\"></p>\n<p>输入你的机器人的序列号，比如：00e20100</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061549830.jpg\" alt=\"1681288477(1)\"></p>\n<p>根据官网信息，配置好了自己的Vector账号之后，可以看到Linux终端输出了SUCCESS！的信息，则配置成功。</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/06/06/202306061550564.jpg\" alt=\"1681288650(1)\"></p>\n<p>细心观察，可以看到这里连接到的机器人主机是通过443端口的。并且对你的认证秘钥以及sdk的配置都保存在你本地的<code>.anki_vector/</code>的路径下了。</p>\n<p>如果你成功运行到这里，看到输出，那么你已经完成了机器人和你的Ubuntu主机在局域网内连通的配置。</p>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p><a href=\"https://developer.anki.com/vector/docs/install-linux.html\">Installation - Linux — Vector SDK 0.6.0 documentation</a></p>\n"},{"title":"MiRo-E入门","comments":1,"toc":true,"toc_number":true,"copyright_author":"Bluet","mathjax":true,"date":"2023-10-11T06:29:19.000Z","updated":"2023-10-11T06:29:19.000Z","keywords":"MiRo","description":null,"top_img":null,"cover":"https://www.synotech.top:5523/wallpaper/walle.jpg","copyright_author_href":null,"copyright_url":null,"copyright_info":null,"_content":"\n# ***MiRo-E入门***\n\n# 背景\n\nMiRo-E机器人是谢菲尔德大学机器人实验室的一块陪伴型仿生机器人，它具有仿生的行为和语音，能够根据和人类的交互产生不同的反馈，具有适应性的行为表现。MiRo-E目前已经适配到了Ubuntu20的版本，对应的是ROS-Noetic的版本，熟悉ROS的同学可以很快上手。机器人本体具有多种类型的传感器：双目相机、声呐传感器、光传感器、麦克风声音传感器、触感传感器、跌落检测传感器以及轮式编码器集成在步进电机内。需要更多传感器信息的同学可以参考：\n\n[MIRO-E: Sensors](http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Technical_Sensors)\n\n实验室拍摄的机器人：\n\n![IMG_20190213_151708](https://www.synotech.top:5523/uploads/2023/10/11/202310111650895.jpg)\n\n\n\n安装步骤清单：\n\n1. 安装ROS1/2\n\n1. 安装MDK环境\n\n1. 测试MDK安装情况\n\n\n\n# 安装ROS1\n\nROS的安装步骤，根据自己的主机系统版本进行选择，官网有很详细的安装步骤，也可以参考我之前的博客进行安装，链接：\n\n\n\n也可以参考MiRo官方的指导：\n\n[MIRO-E: Install ROS](http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer_Install_Steps_Install_ROS)\n\n这里我演示MiRo-E官网安装ubuntu20上的ros-noetic版本。官网也有一行命令脚本快速安装：\n\n```text\nwget -c https://raw.githubusercontent.com/qboticslabs/ros_install_noetic/master/ros_install_noetic.sh && chmod +x ./ros_install_noetic.sh && ./ros_install_noetic.sh\n```\n\n如果安装成功，source下环境就可以启动了roscore。\n\n\n\n# 安装MDK\n\n安装MDK的依赖如：\n\n```text\n$ sudo apt install build-essential python3-pip\n$ pip install apriltag\n$ sudo apt install python3-matplotlib python3-tk ffmpeg\n\n```\n\n下载MDK，根据自己的系统架构来选择版本，下载完成解压到自己定义的路径下，一般放在~/路径就可以，解压后执行下面的操作：\n\n```text\n$ cd ~/mdk-190211/bin/deb64\n$ ./install_mdk.sh\n```\n\n下载包的时候需要你提交下自己的信息，才会开始下载，如图填好信息提交：\n\n![1696731223](https://www.synotech.top:5523/uploads/2023/10/11/202310111720406.jpg)\n\n笔者现在看到的版本是mdk_230105的版本了，之前是20190211。\n\n\n\n# 测试MDK\n\n测试下安装MDK的情况，如果有类似如下输出就是安装好了。其中MDK release是对应你自己安装的版本号，MIRO的edition也是。\n\n```text\nSourcing mdk/setup.bash...\n\nMIRO edition: 2\nMDK path: ~/mdk\nMDK release: R190518\nUser setup: ~/.miro2/config/user_setup.bash\n\nLocal network address: 192.168.1.100 (set from miro_get_dynamic_address())\nRobot network address:  (not set)\nROS master address: http://localhost:11311 (not set, assumed running locally)\n\nType \"miro_info\" to see your environment\n________________________________________________________________\n```\n\n\n\n# 配置MDK\n\n这个配置一般前面正常安装输出了信息，在上面的测试MDK安装情况的时候，那就不需要额外配置。这里配置的内容可以参考官网，我举例说明：\n\n官网也做了提示，在大多数情况下，都不需要这步骤的额外配置，\n\nIn most cases, no configuration changes will be required at all—read [Install MDK](http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer_Install_Steps_Install_MDK) carefully, and determine if you need to make changes before continuing with the instructions on this page.\n\n## 配置的文件\n\n可以配置的文件路径： ~/.miro2/config/user_setup.bash. 这里面提到的配置参数都可以修改，但是你修改之前必须知道它的含义。\n\n安装的时候会创建一个~/mdk的超链接。这个超链接会链接到你下载的包放置的文件路径。\n\n安装MDK的过程中，会把source ~/mdk/setup.bash 配置到~/.bashrc中。\n\n这个source的环境配置大多数人都会删除掉，你可以删除之后，自己在每次使用MDK之前把环境重新source一遍。\n\n\n\n# 网络的配置\n\n如果你是在本地电脑上进行仿真和MDK一起配置使用，那么网络的配置就不需要额外的设置，MDK中已经在配置文件里进行自动网络Local network address的配置，如果是使用机器人本体进行测试，就需要把机器人的网络地址配置到~/.miro2/config/user_setup.bash中。\n\n\n\n关于我的硕士答辩项目，可以参考我的MiRo答辩项目-2019的博客：\n\n\n\nDemo视频如下：\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/O_p8CYiN4_s?si=kBjI4p2nZnI149sP\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SpyKzYiWuG8?si=E-nNVPwEV19MeNzw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n\n\n\n\n***\n\n### 引用文献：\n\n[MIRO-E: Developer](http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer)\n","source":"_posts/012-IntelligentAIRobots/02-MiRoRobots/MiRo-E入门.md","raw":"---\ntitle: MiRo-E入门\ncomments: true\ntoc: true\ntoc_number: true\ncopyright_author: Bluet\nmathjax: true\ndate: 2023-10-11 14:29:19\nupdated: 2023-10-11 14:29:19\ntags: MiRo\ncategories:\n- 智能AI机器人\n- MiRoRobots\nkeywords: MiRo\ndescription:\ntop_img:\ncover:\ncopyright_author_href:\ncopyright_url:\ncopyright_info:\n---\n\n# ***MiRo-E入门***\n\n# 背景\n\nMiRo-E机器人是谢菲尔德大学机器人实验室的一块陪伴型仿生机器人，它具有仿生的行为和语音，能够根据和人类的交互产生不同的反馈，具有适应性的行为表现。MiRo-E目前已经适配到了Ubuntu20的版本，对应的是ROS-Noetic的版本，熟悉ROS的同学可以很快上手。机器人本体具有多种类型的传感器：双目相机、声呐传感器、光传感器、麦克风声音传感器、触感传感器、跌落检测传感器以及轮式编码器集成在步进电机内。需要更多传感器信息的同学可以参考：\n\n[MIRO-E: Sensors](http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Technical_Sensors)\n\n实验室拍摄的机器人：\n\n![IMG_20190213_151708](https://www.synotech.top:5523/uploads/2023/10/11/202310111650895.jpg)\n\n\n\n安装步骤清单：\n\n1. 安装ROS1/2\n\n1. 安装MDK环境\n\n1. 测试MDK安装情况\n\n\n\n# 安装ROS1\n\nROS的安装步骤，根据自己的主机系统版本进行选择，官网有很详细的安装步骤，也可以参考我之前的博客进行安装，链接：\n\n\n\n也可以参考MiRo官方的指导：\n\n[MIRO-E: Install ROS](http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer_Install_Steps_Install_ROS)\n\n这里我演示MiRo-E官网安装ubuntu20上的ros-noetic版本。官网也有一行命令脚本快速安装：\n\n```text\nwget -c https://raw.githubusercontent.com/qboticslabs/ros_install_noetic/master/ros_install_noetic.sh && chmod +x ./ros_install_noetic.sh && ./ros_install_noetic.sh\n```\n\n如果安装成功，source下环境就可以启动了roscore。\n\n\n\n# 安装MDK\n\n安装MDK的依赖如：\n\n```text\n$ sudo apt install build-essential python3-pip\n$ pip install apriltag\n$ sudo apt install python3-matplotlib python3-tk ffmpeg\n\n```\n\n下载MDK，根据自己的系统架构来选择版本，下载完成解压到自己定义的路径下，一般放在~/路径就可以，解压后执行下面的操作：\n\n```text\n$ cd ~/mdk-190211/bin/deb64\n$ ./install_mdk.sh\n```\n\n下载包的时候需要你提交下自己的信息，才会开始下载，如图填好信息提交：\n\n![1696731223](https://www.synotech.top:5523/uploads/2023/10/11/202310111720406.jpg)\n\n笔者现在看到的版本是mdk_230105的版本了，之前是20190211。\n\n\n\n# 测试MDK\n\n测试下安装MDK的情况，如果有类似如下输出就是安装好了。其中MDK release是对应你自己安装的版本号，MIRO的edition也是。\n\n```text\nSourcing mdk/setup.bash...\n\nMIRO edition: 2\nMDK path: ~/mdk\nMDK release: R190518\nUser setup: ~/.miro2/config/user_setup.bash\n\nLocal network address: 192.168.1.100 (set from miro_get_dynamic_address())\nRobot network address:  (not set)\nROS master address: http://localhost:11311 (not set, assumed running locally)\n\nType \"miro_info\" to see your environment\n________________________________________________________________\n```\n\n\n\n# 配置MDK\n\n这个配置一般前面正常安装输出了信息，在上面的测试MDK安装情况的时候，那就不需要额外配置。这里配置的内容可以参考官网，我举例说明：\n\n官网也做了提示，在大多数情况下，都不需要这步骤的额外配置，\n\nIn most cases, no configuration changes will be required at all—read [Install MDK](http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer_Install_Steps_Install_MDK) carefully, and determine if you need to make changes before continuing with the instructions on this page.\n\n## 配置的文件\n\n可以配置的文件路径： ~/.miro2/config/user_setup.bash. 这里面提到的配置参数都可以修改，但是你修改之前必须知道它的含义。\n\n安装的时候会创建一个~/mdk的超链接。这个超链接会链接到你下载的包放置的文件路径。\n\n安装MDK的过程中，会把source ~/mdk/setup.bash 配置到~/.bashrc中。\n\n这个source的环境配置大多数人都会删除掉，你可以删除之后，自己在每次使用MDK之前把环境重新source一遍。\n\n\n\n# 网络的配置\n\n如果你是在本地电脑上进行仿真和MDK一起配置使用，那么网络的配置就不需要额外的设置，MDK中已经在配置文件里进行自动网络Local network address的配置，如果是使用机器人本体进行测试，就需要把机器人的网络地址配置到~/.miro2/config/user_setup.bash中。\n\n\n\n关于我的硕士答辩项目，可以参考我的MiRo答辩项目-2019的博客：\n\n\n\nDemo视频如下：\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/O_p8CYiN4_s?si=kBjI4p2nZnI149sP\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SpyKzYiWuG8?si=E-nNVPwEV19MeNzw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n\n\n\n\n***\n\n### 引用文献：\n\n[MIRO-E: Developer](http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer)\n","slug":"012-IntelligentAIRobots/02-MiRoRobots/MiRo-E入门","published":1,"layout":"post","photos":[],"link":"","_id":"clu8483te0022u9rldlju3t7l","content":"<h1><em><strong>MiRo-E入门</strong></em></h1>\n<h1>背景</h1>\n<p>MiRo-E机器人是谢菲尔德大学机器人实验室的一块陪伴型仿生机器人，它具有仿生的行为和语音，能够根据和人类的交互产生不同的反馈，具有适应性的行为表现。MiRo-E目前已经适配到了Ubuntu20的版本，对应的是ROS-Noetic的版本，熟悉ROS的同学可以很快上手。机器人本体具有多种类型的传感器：双目相机、声呐传感器、光传感器、麦克风声音传感器、触感传感器、跌落检测传感器以及轮式编码器集成在步进电机内。需要更多传感器信息的同学可以参考：</p>\n<p><a href=\"http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Technical_Sensors\">MIRO-E: Sensors</a></p>\n<p>实验室拍摄的机器人：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/10/11/202310111650895.jpg\" alt=\"IMG_20190213_151708\"></p>\n<p>安装步骤清单：</p>\n<ol>\n<li>\n<p>安装ROS1/2</p>\n</li>\n<li>\n<p>安装MDK环境</p>\n</li>\n<li>\n<p>测试MDK安装情况</p>\n</li>\n</ol>\n<h1>安装ROS1</h1>\n<p>ROS的安装步骤，根据自己的主机系统版本进行选择，官网有很详细的安装步骤，也可以参考我之前的博客进行安装，链接：</p>\n<p>也可以参考MiRo官方的指导：</p>\n<p><a href=\"http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer_Install_Steps_Install_ROS\">MIRO-E: Install ROS</a></p>\n<p>这里我演示MiRo-E官网安装ubuntu20上的ros-noetic版本。官网也有一行命令脚本快速安装：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget -c https://raw.githubusercontent.com/qboticslabs/ros_install_noetic/master/ros_install_noetic.sh &amp;&amp; chmod +x ./ros_install_noetic.sh &amp;&amp; ./ros_install_noetic.sh</span><br></pre></td></tr></table></figure>\n<p>如果安装成功，source下环境就可以启动了roscore。</p>\n<h1>安装MDK</h1>\n<p>安装MDK的依赖如：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo apt install build-essential python3-pip</span><br><span class=\"line\">$ pip install apriltag</span><br><span class=\"line\">$ sudo apt install python3-matplotlib python3-tk ffmpeg</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>下载MDK，根据自己的系统架构来选择版本，下载完成解压到自己定义的路径下，一般放在~/路径就可以，解压后执行下面的操作：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd ~/mdk-190211/bin/deb64</span><br><span class=\"line\">$ ./install_mdk.sh</span><br></pre></td></tr></table></figure>\n<p>下载包的时候需要你提交下自己的信息，才会开始下载，如图填好信息提交：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/10/11/202310111720406.jpg\" alt=\"1696731223\"></p>\n<p>笔者现在看到的版本是mdk_230105的版本了，之前是20190211。</p>\n<h1>测试MDK</h1>\n<p>测试下安装MDK的情况，如果有类似如下输出就是安装好了。其中MDK release是对应你自己安装的版本号，MIRO的edition也是。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Sourcing mdk/setup.bash...</span><br><span class=\"line\"></span><br><span class=\"line\">MIRO edition: 2</span><br><span class=\"line\">MDK path: ~/mdk</span><br><span class=\"line\">MDK release: R190518</span><br><span class=\"line\">User setup: ~/.miro2/config/user_setup.bash</span><br><span class=\"line\"></span><br><span class=\"line\">Local network address: 192.168.1.100 (set from miro_get_dynamic_address())</span><br><span class=\"line\">Robot network address:  (not set)</span><br><span class=\"line\">ROS master address: http://localhost:11311 (not set, assumed running locally)</span><br><span class=\"line\"></span><br><span class=\"line\">Type &quot;miro_info&quot; to see your environment</span><br><span class=\"line\">________________________________________________________________</span><br></pre></td></tr></table></figure>\n<h1>配置MDK</h1>\n<p>这个配置一般前面正常安装输出了信息，在上面的测试MDK安装情况的时候，那就不需要额外配置。这里配置的内容可以参考官网，我举例说明：</p>\n<p>官网也做了提示，在大多数情况下，都不需要这步骤的额外配置，</p>\n<p>In most cases, no configuration changes will be required at all—read <a href=\"http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer_Install_Steps_Install_MDK\">Install MDK</a> carefully, and determine if you need to make changes before continuing with the instructions on this page.</p>\n<h2 id=\"配置的文件\">配置的文件</h2>\n<p>可以配置的文件路径： ~/.miro2/config/user_setup.bash. 这里面提到的配置参数都可以修改，但是你修改之前必须知道它的含义。</p>\n<p>安装的时候会创建一个~/mdk的超链接。这个超链接会链接到你下载的包放置的文件路径。</p>\n<p>安装MDK的过程中，会把source ~/mdk/setup.bash 配置到~/.bashrc中。</p>\n<p>这个source的环境配置大多数人都会删除掉，你可以删除之后，自己在每次使用MDK之前把环境重新source一遍。</p>\n<h1>网络的配置</h1>\n<p>如果你是在本地电脑上进行仿真和MDK一起配置使用，那么网络的配置就不需要额外的设置，MDK中已经在配置文件里进行自动网络Local network address的配置，如果是使用机器人本体进行测试，就需要把机器人的网络地址配置到~/.miro2/config/user_setup.bash中。</p>\n<p>关于我的硕士答辩项目，可以参考我的MiRo答辩项目-2019的博客：</p>\n<p>Demo视频如下：</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/O_p8CYiN4_s?si=kBjI4p2nZnI149sP\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SpyKzYiWuG8?si=E-nNVPwEV19MeNzw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p><a href=\"http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer\">MIRO-E: Developer</a></p>\n","site":{"data":{"link":[{"class_name":"友情链接","class_desc":"那些人，那些事","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-tw/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"简单快速的网络框架"}]},{"class_name":"网站","class_desc":"值得推荐的网站","link_list":[{"name":"Youtube","link":"https://www.youtube.com/","avatar":"https://i.loli.net/2020/05/14/9ZkGg8v3azHJfM1.png","descr":"视频网站"},{"name":"Weibo","link":"https://www.weibo.com/","avatar":"https://i.loli.net/2020/05/14/TLJBum386vcnI1P.png","descr":"中国最大社交分享平台"},{"name":"Twitter","link":"https://twitter.com/","avatar":"https://i.loli.net/2020/05/14/5VyHPQqR6LWF39a.png","descr":"社交分享平台"}]}]}},"cover_type":"img","excerpt":"","more":"<h1><em><strong>MiRo-E入门</strong></em></h1>\n<h1>背景</h1>\n<p>MiRo-E机器人是谢菲尔德大学机器人实验室的一块陪伴型仿生机器人，它具有仿生的行为和语音，能够根据和人类的交互产生不同的反馈，具有适应性的行为表现。MiRo-E目前已经适配到了Ubuntu20的版本，对应的是ROS-Noetic的版本，熟悉ROS的同学可以很快上手。机器人本体具有多种类型的传感器：双目相机、声呐传感器、光传感器、麦克风声音传感器、触感传感器、跌落检测传感器以及轮式编码器集成在步进电机内。需要更多传感器信息的同学可以参考：</p>\n<p><a href=\"http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Technical_Sensors\">MIRO-E: Sensors</a></p>\n<p>实验室拍摄的机器人：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/10/11/202310111650895.jpg\" alt=\"IMG_20190213_151708\"></p>\n<p>安装步骤清单：</p>\n<ol>\n<li>\n<p>安装ROS1/2</p>\n</li>\n<li>\n<p>安装MDK环境</p>\n</li>\n<li>\n<p>测试MDK安装情况</p>\n</li>\n</ol>\n<h1>安装ROS1</h1>\n<p>ROS的安装步骤，根据自己的主机系统版本进行选择，官网有很详细的安装步骤，也可以参考我之前的博客进行安装，链接：</p>\n<p>也可以参考MiRo官方的指导：</p>\n<p><a href=\"http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer_Install_Steps_Install_ROS\">MIRO-E: Install ROS</a></p>\n<p>这里我演示MiRo-E官网安装ubuntu20上的ros-noetic版本。官网也有一行命令脚本快速安装：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget -c https://raw.githubusercontent.com/qboticslabs/ros_install_noetic/master/ros_install_noetic.sh &amp;&amp; chmod +x ./ros_install_noetic.sh &amp;&amp; ./ros_install_noetic.sh</span><br></pre></td></tr></table></figure>\n<p>如果安装成功，source下环境就可以启动了roscore。</p>\n<h1>安装MDK</h1>\n<p>安装MDK的依赖如：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo apt install build-essential python3-pip</span><br><span class=\"line\">$ pip install apriltag</span><br><span class=\"line\">$ sudo apt install python3-matplotlib python3-tk ffmpeg</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>下载MDK，根据自己的系统架构来选择版本，下载完成解压到自己定义的路径下，一般放在~/路径就可以，解压后执行下面的操作：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd ~/mdk-190211/bin/deb64</span><br><span class=\"line\">$ ./install_mdk.sh</span><br></pre></td></tr></table></figure>\n<p>下载包的时候需要你提交下自己的信息，才会开始下载，如图填好信息提交：</p>\n<p><img src=\"https://www.synotech.top:5523/uploads/2023/10/11/202310111720406.jpg\" alt=\"1696731223\"></p>\n<p>笔者现在看到的版本是mdk_230105的版本了，之前是20190211。</p>\n<h1>测试MDK</h1>\n<p>测试下安装MDK的情况，如果有类似如下输出就是安装好了。其中MDK release是对应你自己安装的版本号，MIRO的edition也是。</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Sourcing mdk/setup.bash...</span><br><span class=\"line\"></span><br><span class=\"line\">MIRO edition: 2</span><br><span class=\"line\">MDK path: ~/mdk</span><br><span class=\"line\">MDK release: R190518</span><br><span class=\"line\">User setup: ~/.miro2/config/user_setup.bash</span><br><span class=\"line\"></span><br><span class=\"line\">Local network address: 192.168.1.100 (set from miro_get_dynamic_address())</span><br><span class=\"line\">Robot network address:  (not set)</span><br><span class=\"line\">ROS master address: http://localhost:11311 (not set, assumed running locally)</span><br><span class=\"line\"></span><br><span class=\"line\">Type &quot;miro_info&quot; to see your environment</span><br><span class=\"line\">________________________________________________________________</span><br></pre></td></tr></table></figure>\n<h1>配置MDK</h1>\n<p>这个配置一般前面正常安装输出了信息，在上面的测试MDK安装情况的时候，那就不需要额外配置。这里配置的内容可以参考官网，我举例说明：</p>\n<p>官网也做了提示，在大多数情况下，都不需要这步骤的额外配置，</p>\n<p>In most cases, no configuration changes will be required at all—read <a href=\"http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer_Install_Steps_Install_MDK\">Install MDK</a> carefully, and determine if you need to make changes before continuing with the instructions on this page.</p>\n<h2 id=\"配置的文件\">配置的文件</h2>\n<p>可以配置的文件路径： ~/.miro2/config/user_setup.bash. 这里面提到的配置参数都可以修改，但是你修改之前必须知道它的含义。</p>\n<p>安装的时候会创建一个~/mdk的超链接。这个超链接会链接到你下载的包放置的文件路径。</p>\n<p>安装MDK的过程中，会把source ~/mdk/setup.bash 配置到~/.bashrc中。</p>\n<p>这个source的环境配置大多数人都会删除掉，你可以删除之后，自己在每次使用MDK之前把环境重新source一遍。</p>\n<h1>网络的配置</h1>\n<p>如果你是在本地电脑上进行仿真和MDK一起配置使用，那么网络的配置就不需要额外的设置，MDK中已经在配置文件里进行自动网络Local network address的配置，如果是使用机器人本体进行测试，就需要把机器人的网络地址配置到~/.miro2/config/user_setup.bash中。</p>\n<p>关于我的硕士答辩项目，可以参考我的MiRo答辩项目-2019的博客：</p>\n<p>Demo视频如下：</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/O_p8CYiN4_s?si=kBjI4p2nZnI149sP\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SpyKzYiWuG8?si=E-nNVPwEV19MeNzw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n<hr>\n<h3 id=\"引用文献：\">引用文献：</h3>\n<p><a href=\"http://labs.consequentialrobotics.com/miro-e/docs/index.php?page=Developer\">MIRO-E: Developer</a></p>\n"}],"PostAsset":[{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220404-06906ed3.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220404-06906ed3.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220404-c90356a3.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220404-c90356a3.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-040bd4c6.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-040bd4c6.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-09163d0e.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-09163d0e.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-0abae8b4.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-0abae8b4.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-34745253.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-34745253.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-40ed843a.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-40ed843a.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-410a67fb.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-410a67fb.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-41245f4f.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-41245f4f.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4225951f.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-4225951f.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-457455e9.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-457455e9.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-48750e2d.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-48750e2d.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4a0db26b.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-4a0db26b.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-4b653a5e.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-4b653a5e.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-7d662ac4.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-7d662ac4.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-7e9d0b45.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-7e9d0b45.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-83dab511.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-83dab511.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-998602da.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-998602da.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-9b2895c7.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-9b2895c7.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a067c6cf.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-a067c6cf.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a5b9d88a.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-a5b9d88a.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-a6a42af7.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-a6a42af7.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-af372d35.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-af372d35.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-c075dee1.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-c075dee1.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-cd1519fb.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-cd1519fb.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-dfeec3f1.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-dfeec3f1.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-dff73fa3.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-dff73fa3.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-e2ff73cf.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-e2ff73cf.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-e582528a.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-e582528a.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-eab99959.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-eab99959.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-ed4f3265.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-ed4f3265.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f1a06055.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-f1a06055.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f843c893.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-f843c893.png","modified":1,"renderable":1},{"_id":"source/_posts/001-MachineLearning/MachineLeaning-ChapterOne/20220405-f9890acf.png","post":"clu8483si0006u9rlh9bia7u4","slug":"20220405-f9890acf.png","modified":1,"renderable":1},{"_id":"source/_posts/005-LifeMemories/人生感悟-开篇/677558eb37581273-166153976382210.jpg","post":"clu8483sl000bu9rl5ckvc1ss","slug":"677558eb37581273-166153976382210.jpg","modified":1,"renderable":1},{"_id":"source/_posts/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit/5b208976-b632-11e5-8406-38d379ec46aa.png","post":"clu8483sr000ju9rlfmgccau7","slug":"5b208976-b632-11e5-8406-38d379ec46aa.png","modified":1,"renderable":1},{"_id":"source/_posts/002-NVIDIA/01-Container-Toolkit/NVIDIA-Container-Toolkit/nvidia-docker-arch-new.png","post":"clu8483sr000ju9rlfmgccau7","slug":"nvidia-docker-arch-new.png","modified":1,"renderable":1},{"_id":"source/_posts/002-NVIDIA/03-TensorRT/TensorRT安装指南/02-TensorRT安装指南-6ff62ded.png","post":"clu8483su000pu9rlf9auaj16","slug":"02-TensorRT安装指南-6ff62ded.png","modified":1,"renderable":1},{"_id":"source/_posts/002-NVIDIA/03-TensorRT/TensorRT安装指南/02-TensorRT安装指南-a2f8136f.png","post":"clu8483su000pu9rlf9auaj16","slug":"02-TensorRT安装指南-a2f8136f.png","modified":1,"renderable":1},{"_id":"source/_posts/002-NVIDIA/03-TensorRT/TensorRT概述/01-TensorRT概述-b0cc304a.png","post":"clu8483sv000ru9rla8xddl48","slug":"01-TensorRT概述-b0cc304a.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220112-4b9a80b5.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220112-4b9a80b5.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220112-76ab38e2.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220112-76ab38e2.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-0da8fa36.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220114-0da8fa36.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-159d3456.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220114-159d3456.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-8bd94d2e.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220114-8bd94d2e.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-923e402c.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220114-923e402c.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-cddd0cdf.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220114-cddd0cdf.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220114-e0917cd4.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220114-e0917cd4.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-3b5f0f20.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220116-3b5f0f20.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-67901f3c.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220116-67901f3c.png","modified":1,"renderable":1},{"_id":"source/_posts/003-Projects/01-视觉跟踪/跟踪项目说明书/20220116-76d60e91.png","post":"clu8483sz0010u9rlet7307bp","slug":"20220116-76d60e91.png","modified":1,"renderable":1},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu/Cmake.png","post":"clu8483t00012u9rl503ucax8","slug":"Cmake.png","modified":1,"renderable":1},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830004817114.png","post":"clu8483t00012u9rl503ucax8","slug":"image-20220830004817114.png","modified":1,"renderable":1},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830010646685.png","post":"clu8483t00012u9rl503ucax8","slug":"image-20220830010646685.png","modified":1,"renderable":1},{"_id":"source/_posts/004-Linux/01-Cmake/Cmake升级-Ubuntu/image-20220830010853110.png","post":"clu8483t00012u9rl503ucax8","slug":"image-20220830010853110.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"clu8483si0006u9rlh9bia7u4","category_id":"clu8483sk0009u9rl3h1ca1sq","_id":"clu8483st000nu9rlbpaa5mkl"},{"post_id":"clu8483sj0008u9rlbdjo92la","category_id":"clu8483sk0009u9rl3h1ca1sq","_id":"clu8483sw000su9rl557836y0"},{"post_id":"clu8483sl000bu9rl5ckvc1ss","category_id":"clu8483st000mu9rlbkrxgxip","_id":"clu8483tb001uu9rl4y4jcue6"},{"post_id":"clu8483sl000bu9rl5ckvc1ss","category_id":"clu8483t7001iu9rlgjwcbl5p","_id":"clu8483td001yu9rl0r5bb3zj"},{"post_id":"clu8483sn000fu9rl1btrcb6n","category_id":"clu8483sw000tu9rlfxh8evey","_id":"clu8483tg0028u9rlb8mt4oki"},{"post_id":"clu8483sn000fu9rl1btrcb6n","category_id":"clu8483te0021u9rlhu60gh45","_id":"clu8483tg002au9rl8y0542zl"},{"post_id":"clu8483so000gu9rl33z43oqx","category_id":"clu8483sy000yu9rlfkpbgb6o","_id":"clu8483ti002mu9rl8esw9s5o"},{"post_id":"clu8483so000gu9rl33z43oqx","category_id":"clu8483th002gu9rler7n35t9","_id":"clu8483tj002pu9rlh8io70et"},{"post_id":"clu8483sr000ju9rlfmgccau7","category_id":"clu8483t00013u9rlftc2abaw","_id":"clu8483tl002xu9rl8fun5mp4"},{"post_id":"clu8483sr000ju9rlfmgccau7","category_id":"clu8483tj002ru9rlejin9f94","_id":"clu8483tl002yu9rl7zij6tgv"},{"post_id":"clu8483ss000lu9rl24xu1gdu","category_id":"clu8483t00013u9rlftc2abaw","_id":"clu8483tn0038u9rldkbxgfjs"},{"post_id":"clu8483ss000lu9rl24xu1gdu","category_id":"clu8483tl0030u9rla3i55x46","_id":"clu8483tn003bu9rl54l2bhde"},{"post_id":"clu8483su000pu9rlf9auaj16","category_id":"clu8483t00013u9rlftc2abaw","_id":"clu8483tp003ju9rlg8wz1x1r"},{"post_id":"clu8483su000pu9rlf9auaj16","category_id":"clu8483tn003du9rla7oxgsgb","_id":"clu8483tp003mu9rlhlkhc4n7"},{"post_id":"clu8483sv000ru9rla8xddl48","category_id":"clu8483t00013u9rlftc2abaw","_id":"clu8483ts003zu9rlai9oemzc"},{"post_id":"clu8483sv000ru9rla8xddl48","category_id":"clu8483tn003du9rla7oxgsgb","_id":"clu8483ts0041u9rlfclv46xg"},{"post_id":"clu8483sx000vu9rl32g25n40","category_id":"clu8483t00013u9rlftc2abaw","_id":"clu8483tu004bu9rl4l2shrho"},{"post_id":"clu8483sx000vu9rl32g25n40","category_id":"clu8483ts0043u9rlg95xbc6u","_id":"clu8483tu004du9rlbxtg2490"},{"post_id":"clu8483sy000xu9rl135v6mjw","category_id":"clu8483tf0025u9rl6kp152da","_id":"clu8483tw004ou9rlg6q0dr9e"},{"post_id":"clu8483sy000xu9rl135v6mjw","category_id":"clu8483tu004gu9rlh6hsfz69","_id":"clu8483tw004pu9rl8fpt8hgr"},{"post_id":"clu8483sz0010u9rlet7307bp","category_id":"clu8483tf0025u9rl6kp152da","_id":"clu8483tw004qu9rl76703lh9"},{"post_id":"clu8483sz0010u9rlet7307bp","category_id":"clu8483tu004gu9rlh6hsfz69","_id":"clu8483tx004tu9rl6xwj8nzv"},{"post_id":"clu8483t00012u9rl503ucax8","category_id":"clu8483tg002du9rlbhzd0mts","_id":"clu8483tx004uu9rl71tzc5uw"},{"post_id":"clu8483t00012u9rl503ucax8","category_id":"clu8483tw004nu9rlhfd2bwxy","_id":"clu8483tx004wu9rlck3f6pyr"},{"post_id":"clu8483t10014u9rl4njyfmw3","category_id":"clu8483ti002ju9rl4avc6jlk","_id":"clu8483tx004yu9rl6zib547q"},{"post_id":"clu8483t10014u9rl4njyfmw3","category_id":"clu8483tw004su9rl42vkeyl0","_id":"clu8483ty004zu9rl0r2v3gqi"},{"post_id":"clu8483t20017u9rl4a8ebfdg","category_id":"clu8483ti002ju9rl4avc6jlk","_id":"clu8483ty0051u9rlfkue3ys5"},{"post_id":"clu8483t20017u9rl4a8ebfdg","category_id":"clu8483tw004su9rl42vkeyl0","_id":"clu8483ty0052u9rlgurx39oz"},{"post_id":"clu8483t20018u9rlhuivh769","category_id":"clu8483ti002ju9rl4avc6jlk","_id":"clu8483ty0053u9rl2n0kgmal"},{"post_id":"clu8483t20018u9rlhuivh769","category_id":"clu8483tw004su9rl42vkeyl0","_id":"clu8483ty0055u9rle43abrap"},{"post_id":"clu8483t3001bu9rl95628i0x","category_id":"clu8483tl002wu9rlf7g78ko9","_id":"clu8483tz0056u9rlbc5khq7h"},{"post_id":"clu8483t3001bu9rl95628i0x","category_id":"clu8483ty0050u9rl3vh357ue","_id":"clu8483tz0058u9rl1ynt3ctr"},{"post_id":"clu8483t4001cu9rl1ed91mm3","category_id":"clu8483tl002wu9rlf7g78ko9","_id":"clu8483tz005au9rlav9g7oek"},{"post_id":"clu8483t4001cu9rl1ed91mm3","category_id":"clu8483ty0054u9rl70qj0yl8","_id":"clu8483tz005bu9rl375taegz"},{"post_id":"clu8483t5001fu9rl7pbre1kt","category_id":"clu8483tl002wu9rlf7g78ko9","_id":"clu8483u0005du9rlf6awb6p5"},{"post_id":"clu8483t5001fu9rl7pbre1kt","category_id":"clu8483ty0054u9rl70qj0yl8","_id":"clu8483u0005eu9rld5stgf5b"},{"post_id":"clu8483t6001hu9rlbxce2tsn","category_id":"clu8483tl002wu9rlf7g78ko9","_id":"clu8483u0005gu9rlg6n94a1v"},{"post_id":"clu8483t6001hu9rlbxce2tsn","category_id":"clu8483ty0054u9rl70qj0yl8","_id":"clu8483u0005hu9rlbsp18l07"},{"post_id":"clu8483t8001lu9rl5jk3ga8t","category_id":"clu8483tl002wu9rlf7g78ko9","_id":"clu8483u0005iu9rlfxul27cf"},{"post_id":"clu8483t8001lu9rl5jk3ga8t","category_id":"clu8483ty0054u9rl70qj0yl8","_id":"clu8483u1005ku9rl0syog84y"},{"post_id":"clu8483t9001nu9rlhmolecum","category_id":"clu8483tp003ou9rl5yf13yh3","_id":"clu8483u1005lu9rl09nuc201"},{"post_id":"clu8483t9001nu9rlhmolecum","category_id":"clu8483u0005fu9rl1w1tfctq","_id":"clu8483u1005nu9rl15k78g5r"},{"post_id":"clu8483ta001pu9rl1ihs6dqj","category_id":"clu8483tq003qu9rl27pg43xk","_id":"clu8483u1005ou9rldablb3jm"},{"post_id":"clu8483ta001pu9rl1ihs6dqj","category_id":"clu8483u0005ju9rl9wiq47ii","_id":"clu8483u1005qu9rlcn1283e7"},{"post_id":"clu8483tb001su9rl0n2iapug","category_id":"clu8483tr003xu9rl32ard4bv","_id":"clu8483u2005ru9rl2pjneryn"},{"post_id":"clu8483tb001su9rl0n2iapug","category_id":"clu8483u1005mu9rl4sbr8syl","_id":"clu8483u2005tu9rl2tn2fn7t"},{"post_id":"clu8483tc001wu9rl2mu3fqmv","category_id":"clu8483ts0040u9rl28xf9rhi","_id":"clu8483u2005vu9rl851a9lo1"},{"post_id":"clu8483tc001wu9rl2mu3fqmv","category_id":"clu8483u1005pu9rl892u6zny","_id":"clu8483u2005wu9rl7s775cjb"},{"post_id":"clu8483td0020u9rldo1ag5b1","category_id":"clu8483ts0040u9rl28xf9rhi","_id":"clu8483u2005xu9rlez9gd9hl"},{"post_id":"clu8483td0020u9rldo1ag5b1","category_id":"clu8483u1005pu9rl892u6zny","_id":"clu8483u2005yu9rl68db1nk4"},{"post_id":"clu8483te0022u9rldlju3t7l","category_id":"clu8483ts0040u9rl28xf9rhi","_id":"clu8483u3005zu9rl2maf0mij"},{"post_id":"clu8483te0022u9rldlju3t7l","category_id":"clu8483u2005uu9rl8rtkdzlv","_id":"clu8483u30060u9rl96h43jmn"}],"PostTag":[{"post_id":"clu8483sa0001u9rle4hu33zu","tag_id":"clu8483sg0004u9rl31ukdrjh","_id":"clu8483sn000du9rl3ppcg3kd"},{"post_id":"clu8483si0006u9rlh9bia7u4","tag_id":"clu8483sm000cu9rlaaer32op","_id":"clu8483sr000ku9rl9fq5bm27"},{"post_id":"clu8483sj0008u9rlbdjo92la","tag_id":"clu8483sm000cu9rlaaer32op","_id":"clu8483sv000qu9rl1sj82231"},{"post_id":"clu8483sl000bu9rl5ckvc1ss","tag_id":"clu8483st000ou9rl2zkw524z","_id":"clu8483sx000wu9rl6hj6ffj3"},{"post_id":"clu8483sn000fu9rl1btrcb6n","tag_id":"clu8483sw000uu9rl318o2z55","_id":"clu8483t00011u9rl3th84hwk"},{"post_id":"clu8483so000gu9rl33z43oqx","tag_id":"clu8483sz000zu9rl8riu5wed","_id":"clu8483t20016u9rlhwoqfwcu"},{"post_id":"clu8483sr000ju9rlfmgccau7","tag_id":"clu8483t10015u9rl5l7vhy3y","_id":"clu8483t6001gu9rlck00byub"},{"post_id":"clu8483sr000ju9rlfmgccau7","tag_id":"clu8483t3001au9rlg1a75xbf","_id":"clu8483t7001ju9rl3drz11dx"},{"post_id":"clu8483ss000lu9rl24xu1gdu","tag_id":"clu8483t5001eu9rl9asd2n28","_id":"clu8483t9001mu9rl3ecpa555"},{"post_id":"clu8483t9001nu9rlhmolecum","tag_id":"clu8483t3001au9rlg1a75xbf","_id":"clu8483tb001ru9rl7fonc5cm"},{"post_id":"clu8483su000pu9rlf9auaj16","tag_id":"clu8483t8001ku9rl3mgz4nqr","_id":"clu8483tc001vu9rlbjokchvc"},{"post_id":"clu8483sv000ru9rla8xddl48","tag_id":"clu8483t8001ku9rl3mgz4nqr","_id":"clu8483td001zu9rl7q5z5zf6"},{"post_id":"clu8483sx000vu9rl32g25n40","tag_id":"clu8483tc001xu9rl4p2oegru","_id":"clu8483tf0024u9rlfjkzhnkq"},{"post_id":"clu8483sy000xu9rl135v6mjw","tag_id":"clu8483tf0023u9rl4sxibzg9","_id":"clu8483tg0027u9rlb8474ta3"},{"post_id":"clu8483sz0010u9rlet7307bp","tag_id":"clu8483tf0023u9rl4sxibzg9","_id":"clu8483tg002cu9rl2xx29sdr"},{"post_id":"clu8483t00012u9rl503ucax8","tag_id":"clu8483tg002bu9rl9dyv8bzz","_id":"clu8483th002fu9rlb91kdsx2"},{"post_id":"clu8483t10014u9rl4njyfmw3","tag_id":"clu8483th002eu9rl15ih2cha","_id":"clu8483ti002iu9rl355t483z"},{"post_id":"clu8483t20017u9rl4a8ebfdg","tag_id":"clu8483th002eu9rl15ih2cha","_id":"clu8483ti002lu9rl5xtcdlvi"},{"post_id":"clu8483t20018u9rlhuivh769","tag_id":"clu8483th002eu9rl15ih2cha","_id":"clu8483tj002qu9rl30224pzl"},{"post_id":"clu8483t3001bu9rl95628i0x","tag_id":"clu8483tj002ou9rlgex3hgw1","_id":"clu8483tk002tu9rlg9lv9mr3"},{"post_id":"clu8483t4001cu9rl1ed91mm3","tag_id":"clu8483tk002su9rl838jg74x","_id":"clu8483tm0032u9rl6pp2h1c4"},{"post_id":"clu8483t4001cu9rl1ed91mm3","tag_id":"clu8483tk002vu9rl3vs1avgq","_id":"clu8483tm0034u9rlgqrg4atk"},{"post_id":"clu8483t4001cu9rl1ed91mm3","tag_id":"clu8483tl002zu9rl6jup5au0","_id":"clu8483tm0036u9rl3w8nhr2z"},{"post_id":"clu8483t5001fu9rl7pbre1kt","tag_id":"clu8483tk002su9rl838jg74x","_id":"clu8483tn0039u9rla6wy7bvt"},{"post_id":"clu8483t5001fu9rl7pbre1kt","tag_id":"clu8483t3001au9rlg1a75xbf","_id":"clu8483tn003cu9rl1iq1evzn"},{"post_id":"clu8483t6001hu9rlbxce2tsn","tag_id":"clu8483tk002su9rl838jg74x","_id":"clu8483to003hu9rlezwq17ea"},{"post_id":"clu8483t6001hu9rlbxce2tsn","tag_id":"clu8483tk002vu9rl3vs1avgq","_id":"clu8483tp003ku9rlhvxi0qd6"},{"post_id":"clu8483t6001hu9rlbxce2tsn","tag_id":"clu8483tl002zu9rl6jup5au0","_id":"clu8483tp003nu9rldap3e67c"},{"post_id":"clu8483t8001lu9rl5jk3ga8t","tag_id":"clu8483tk002su9rl838jg74x","_id":"clu8483tq003su9rl0vbx73fv"},{"post_id":"clu8483t8001lu9rl5jk3ga8t","tag_id":"clu8483tp003lu9rl13wn4kdx","_id":"clu8483tr003uu9rlgln07hpm"},{"post_id":"clu8483t8001lu9rl5jk3ga8t","tag_id":"clu8483tq003pu9rl3wqi1ro9","_id":"clu8483tr003wu9rldw1fgus7"},{"post_id":"clu8483ta001pu9rl1ihs6dqj","tag_id":"clu8483tq003ru9rlfkpb8yde","_id":"clu8483tt0045u9rl0i6v4g8j"},{"post_id":"clu8483ta001pu9rl1ihs6dqj","tag_id":"clu8483tr003vu9rlegi1180l","_id":"clu8483tt0047u9rl58jg582q"},{"post_id":"clu8483ta001pu9rl1ihs6dqj","tag_id":"clu8483tr003yu9rl5hbldiim","_id":"clu8483tt0049u9rl1k6f5dew"},{"post_id":"clu8483ta001pu9rl1ihs6dqj","tag_id":"clu8483ts0042u9rl8z2lcm9u","_id":"clu8483tu004cu9rlfniw6e1o"},{"post_id":"clu8483tb001su9rl0n2iapug","tag_id":"clu8483ts0044u9rl5utzcf14","_id":"clu8483tu004fu9rl9mk3d2wg"},{"post_id":"clu8483tb001su9rl0n2iapug","tag_id":"clu8483tr003yu9rl5hbldiim","_id":"clu8483tv004hu9rl4vka4t40"},{"post_id":"clu8483tc001wu9rl2mu3fqmv","tag_id":"clu8483tu004eu9rlh2mbaj8s","_id":"clu8483tv004ju9rl5ql66tfh"},{"post_id":"clu8483td0020u9rldo1ag5b1","tag_id":"clu8483tu004eu9rlh2mbaj8s","_id":"clu8483tw004mu9rl18lm0jpt"},{"post_id":"clu8483te0022u9rldlju3t7l","tag_id":"clu8483tv004lu9rl4z5xfnj1","_id":"clu8483tw004ru9rl8a504ns6"}],"Tag":[{"name":"Projects","_id":"clu8483sg0004u9rl31ukdrjh"},{"name":"A First Course in ML","_id":"clu8483sm000cu9rlaaer32op"},{"name":"感悟","_id":"clu8483st000ou9rl2zkw524z"},{"name":"上海工作","_id":"clu8483sw000uu9rl318o2z55"},{"name":"Pytorch","_id":"clu8483sz000zu9rl8riu5wed"},{"name":"Container-Toolkit","_id":"clu8483t10015u9rl5l7vhy3y"},{"name":"Docker","_id":"clu8483t3001au9rlg1a75xbf"},{"name":"ONNX","_id":"clu8483t5001eu9rl9asd2n28"},{"name":"TensorRT","_id":"clu8483t8001ku9rl3mgz4nqr"},{"name":"cuBLAS","_id":"clu8483tc001xu9rl4p2oegru"},{"name":"Gimbal Camera Tracking","_id":"clu8483tf0023u9rl4sxibzg9"},{"name":"CMake","_id":"clu8483tg002bu9rl9dyv8bzz"},{"name":"OpenCV","_id":"clu8483th002eu9rl15ih2cha"},{"name":"后融合","_id":"clu8483tj002ou9rlgex3hgw1"},{"name":"Autoware.Universe","_id":"clu8483tk002su9rl838jg74x"},{"name":"Ubuntu22","_id":"clu8483tk002vu9rl3vs1avgq"},{"name":"Carla","_id":"clu8483tl002zu9rl6jup5au0"},{"name":"Ubuntu18","_id":"clu8483tp003lu9rl13wn4kdx"},{"name":"AWSIM","_id":"clu8483tq003pu9rl3wqi1ro9"},{"name":"ARM","_id":"clu8483tq003ru9rlfkpb8yde"},{"name":"RaspberryPi","_id":"clu8483tr003vu9rlegi1180l"},{"name":"Jetson","_id":"clu8483tr003yu9rl5hbldiim"},{"name":"Clash","_id":"clu8483ts0042u9rl8z2lcm9u"},{"name":"TuringPi2","_id":"clu8483ts0044u9rl5utzcf14"},{"name":"Vector","_id":"clu8483tu004eu9rlh2mbaj8s"},{"name":"MiRo","_id":"clu8483tv004lu9rl4z5xfnj1"}]}}